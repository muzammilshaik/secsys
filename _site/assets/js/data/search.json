[
  
  {
    "title": "sed cheat sheet",
    "url": "/posts/sed/",
    "categories": "Cheatsheet",
    "tags": "cheatsheet",
    "date": "2024-12-20 00:00:00 +0530",
    





    
    "snippet": "Regular expressions.  Replace the first occurrence of a string in a file, and print the result:    sed 's/find/replace/' filename        Replace only on lines matching the line pattern:    sed '/li...",
    "content": "Regular expressions.  Replace the first occurrence of a string in a file, and print the result:    sed 's/find/replace/' filename        Replace only on lines matching the line pattern:    sed '/line_pattern/s/find/replace/'        Replace all occurrences of a string in a file, overwriting the file (i.e. in-place):    sed -i 's/find/replace/g' filename        Replace all occurrences of an extended regular expression in a file:    sed -r 's/regex/replace/g' filename        Apply multiple find-replace expressions to a file:    sed -e 's/find/replace/' -e 's/find/replace/' filename      File spacing  double space a file    sed G        double space a file which already has blank lines in it. Output file should contain no more than one blank line between lines of text.    sed '/^$/d;G'        triple space a file    sed 'G;G'        undo double-spacing (assumes even-numbered lines are always blank)    sed 'n;d'        insert a blank line above every line which matches “regex”    sed '/regex/{x;p;x;}'        insert a blank line below every line which matches “regex”    sed '/regex/G'        insert a blank line above and below every line which matches “regex”    sed '/regex/{x;p;x;G;}'      Userful  List out the second column in the table.    cat text/table.txt | sed 1d | awk '{ print $2 }'        Sum the columns in the table.    cat text/table.txt | sed 1d | awk '{ sum += $2 } END { print sum }'        Kills all processes by name.    ps aux | grep chrome | awk '{ print $2 }' | kill (or) pkill chrome        Deletes trailing whitespace.    sed 's/\\s\\+$//g' filename        Deletes all blank lines from file.    sed '/^$/d' filename        Insert ‘use strict’ to the top of every js file.    sed \"1i 'use strict';\" *.js        Append a new line at the end of every file.    sed '1a \\n' *        Generate random numbers and then sort.    for i in {1..20}; do echo $(($RANDOM * 777 * $i)); done | sort -n        Commatize numbers.    sed -r ':loop; s/(.*[0-9])([0-9]{3})/\\1,\\2/; t loop' text/numbers.txt      Sed Print  Print contents of a file.    sed -n '/fox/p' text/* (or) sed -n '/Sysadmin/p' text/geek.txt        Print lines starting with 3 and skipping by 2.    sed -n '3~2p' text/geek.txt        Print the last line.    sed -n '$p' text/geek.txt        Prints the lines matching the between the two patterns.    sed -n '/Hardware/,/Website/p' text/geek.txt    Sed Print Number    Prints the line number for all lines in the file.    sed -n '=' filename        Prints the line number that matches the pattern.    sed -n '/Linux/=' filename        Prints the line number in range of two patterns (inclusive).    sed -n '/Linux/,/Hardware/=' filename        Prints the total number of lines.    sed -n '$=' filename        number each line of a file (simple left alignment). Using a tab (see note on ‘\\t’ at end of file) instead of space will preserve margins.    sed = filename | sed 'N;s/\\n/\\t/'        number each line of a file (number on left, right-aligned)    sed = filename | sed 'N; s/^/     /; s/ *\\(.\\{6,\\}\\)\\n/\\1  /'        number each line of file, but only print numbers if line is not blank    sed '/./=' filename | sed '/./N; s/\\n/ /'        count lines (emulates “wc -l”)    sed -n '$='      Sed DeleteThe d command performs a deletion.  Deletes the 3rd line from beginning of file.    sed '3d' text/geek.txt        Delete every lines starting from 3 and skipping by 2.    sed '3~2d' text/geek.txt        Delete lines from 3 to 5.    sed '3,5d' text/geek.txt        Delete the last line.    sed '$d' text/geek.txt        Delete lines matching the pattern.    sed '/Sysadmin/d' text/geek.txt        delete lines matching pattern     sed '/pattern/d'        delete ALL blank lines from a file (same as “grep ‘.’ “)     sed '/^$/d'                           # method 1 sed '/./!d'                           # method 2        Sed Substitute    The s command performs a substitution.    Simple substituion for the first result.    sed 's/Linux/Unix/' text/geek.txt        Simple substituion for global instances.    sed 's/Linux/Unix/g' text/geek.txt        Replace nth instance.    sed 's/Linux/Unix/2' text/geek.txt        Write matched lines to output.    sed -n 's/Linux/Unix/gp' text/geek.txt &gt; text/geek-sub.txt        Use regex group for capturing additional patterns (up to 9).    sed 's/\\(Linux\\).\\+/\\1/g' text/geek.txt        sed -r 's/(Linux).+/\\1/g' text/geek.txt        Remove the last word.    sed -r 's/\\d$//g' text/geek.txt        Remove all letters.    sed -r 's/[a-zA-Z]//g' text/geek.txt        Remove html tags (WIP).    sed -r 's|(&lt;/?[a-z]+&gt;)||g' text/html.txt        Commatize any number.    sed ':a;s/\\B[0-9]\\{3\\}\\&gt;/,&amp;/;ta' text/numbers.txt        sed -r ':loop; s/\\B[0-9]{3}\\&gt;/,&amp;/; t loop' text/numbers.txt      Sed TransformThe y command performs a transformation.  Converts all lowercase chars to uppercase.    sed 'y/abcdefghijklmnopqrstuvwxyz/ABCDEFGHIJKLMNOPQRSTUVWXYZ/' text/geek.txt        Converts all uppercase chars to lowercase.    sed 'y/ABCDEFGHIJKLMNOPQRSTUVWXYZ/abcdefghijklmnopqrstuvwxyz/' text/geek.txt        Perform a two character shift.    sed 'y/abcdefghijklmnopqrstuvwxyz/cdefghijklmnopqrstuvwxyzab/' text/geek.txt      Special appplicatoins  get Usenet/e-mail message header    sed '/^$/q'                # deletes everything after first blank line        get Usenet/e-mail message body    sed '1,/^$/d'              # deletes everything up to first blank line        get Subject header, but remove initial “Subject: “ portion    sed '/^Subject: */!d; s///;q'        get return address header    sed '/^Reply-To:/q; /^From:/h; /./d;g;q'      Sed Multiple Commands  The -e flag allows for multiple commands.    sed -r -e 's/etc\\.*//g' -e 's/(\\s+)(\\))/\\2/g' text/geek.tx      "
  },
  
  {
    "title": "lxd cheat sheet",
    "url": "/posts/lxdcs/",
    "categories": "Cheatsheet",
    "tags": "cheatsheet",
    "date": "2024-12-20 00:00:00 +0530",
    





    
    "snippet": "Useful LXD commandsList available containerslxc image list ubuntu:        # ubuntu: is officially supported image sourcelxc image list images:        # images: is an unsupported sourcelxc image ali...",
    "content": "Useful LXD commandsList available containerslxc image list ubuntu:        # ubuntu: is officially supported image sourcelxc image list images:        # images: is an unsupported sourcelxc image alias list images:  # lists user-friendly namesLaunch a containerThis creates and starts a container.lxc launch ubuntu:14.04 CONTAINERNAME   # image and container names are optional lxc launch ubuntu:14.04/armhf armcont   # specific architecturelxc launch images:alpine/3.3/amd64      # unsupported images: sourceCreate containerWithout starting it.lxc init images:alpine/3.3/amd64 alpinecontlxc copy CONTAINER1 CONTAINER2        # clonelxc delete alpinecont [--force]       # --force if it is runningStart/stop after creating itlxc start alpinecontlxc stop alpinecont [--force]         # --force if it doesn't want to stoplxc restart alpinecont [--force]lxc pause alpinecont                  # SIGSTOP to all container processesList local containerslxc list lxc list --columns \"nsapt\"            # name, status, arch, PID of init, typelxc list security.privileged=true     # filter for propertieslxc info CONTAINERNAME                # detailed info about one containerAvailable columns for the list command    4 - IPv4 address    6 - IPv6 address    a - Architecture    c - Creation date    n - Name    p - PID of the container's init process    P - Profiles    s - State    S - Number of snapshots    t - Type (persistent or ephemeral)Renamelxc move CONTAINERNAME NEWNAMEConfigurationConfig changes are  effective immediately, even if container is running.export VISUAL=/usr/bin/vimlxc config edit CONTAINERNAME           # launches editorlxc config set CONTAINERNAME KEY VALUE  # change a single config itemlxc config device add CONTAINERNAME DEVICE TYPE KEY=VALUElxc config show [--expanded] CONTAINERNAMEConfiguration settings can be saved as **profiles**.Enter the containerlxc exec CONTAINERNAME -- PROGRAM OPTIONSlxc exec CONTAINERNAME shlxc exec CONATINERNAME --env KEY=VALUE PROGRAM   # environment variableThis command runs the program in all the namespaces and cgroups of the container. The program must exist inside the container.Access container fileslxc file pull CONTAINERNAME/etc/passwd /tmp/mypasswdlxc file push /tmp/mypasswd CONTAINERNAME/etc/passwd lxc file edit CONTAINERNAME/etc/passwd Snapshotslxc snapshot CONTAINERNAME SNAPNAME                        # SNAPNAME is optional; default name snap*X*lxc restore CONTAINERNAME SNAPNAME                         # resets the container to snapshotlxc copy CONTAINERNAME/SNAPNAME NEWCONTAINER               # new container from snapshotlxc delete CONTAINERNAME/SNAPNAME                              lxc info CONTAINERNAME                                     # lists snapshots among other infolxc move CONTAINERNAME/SNAPNAME CONTAINERNAME/NEWSNAPNAME  # rename snapshot"
  },
  
  {
    "title": "lxconsole",
    "url": "/posts/lxconsole/",
    "categories": "Virtualization",
    "tags": "",
    "date": "2024-12-20 00:00:00 +0530",
    





    
    "snippet": "LXConsole is an open-source web-based application designed to simplify the management of LXD servers. It offers a convenient graphical user interface that allows users to manage multiple Incus (LXD...",
    "content": "LXConsole is an open-source web-based application designed to simplify the management of LXD servers. It offers a convenient graphical user interface that allows users to manage multiple Incus (LXD’s fork) and LXD containers. In this blog, we will guide you through the entire process of setting up and using LXConsole, from launching a Debian instance to managing LXD containers seamlessly through a web interface.Stpe1: Launching the Debian instanceTo begin with, you need a Debian instance. This can be easily achieved using Proxmox helper scripts. These scripts automate the setup process, saving you time and effort. To launch the Debian instance, execute the following command:bash -c \"$(wget -qLO - https://github.com/community-scripts/ProxmoxVE/raw/main/ct/debian.sh)\"This command will download and execute a script that sets up a Debian container on your Proxmox environment. After running the script, you should have a Debian instance ready for further configuration.    ____       __    _   / __ \\___  / /_  (_)___ _____  / / / / _ \\/ __ \\/ / __ `/ __ \\ / /_/ /  __/ /_/ / / /_/ / / / //_____/\\___/_.___/_/\\__,_/_/ /_/  🧩  Using Advanced Settings  🖥️   Operating System: debian  🌟  Version: 12  📦  Container Type: Privileged  🔐  Root Password: ********  🆔  Container ID: XXX  🏠  Hostname: lxconsole  💾  Disk Size: 2  🧠  CPU Cores: 1  🛠️   RAM Size: 512  🌉  Bridge: vmbr0  📡  IP Address: XXXXXXXXXX  🌐  Gateway IP Address: XXXXXXXXXX  📡  APT-Cacher IP Address: Default  🚫  Disable IPv6: no  ⚙️   Interface MTU Size: Default  🔍  DNS Search Domain: Host  📡  DNS Server IP Address: Host  🏷️   Vlan: Default  🔑  Root SSH Access: yes  🔍  Verbose Mode: yes  🚀  Creating a Debian LXC using the above advanced settings  ✔️   Using XXXXXXXXXX for Template Storage.  ✔️   Using XXXXXXXXXX for Container Storage.  ✔️   Updated LXC Template List  ✔️   LXC Container XXX was successfully created.  ✔️   Started LXC Container  ✔️   Set up Container OS  ✔️   Network Connected: XXXXXXXXXX  ✔️   IPv4 Internet Connected  ✖️   IPv6 Internet Not Connected  ✔️   DNS Resolved github.com to XXXX:XXXX:XXXX::XXXX:XXXX  ✔️   Updated Container OS  ✔️   Installed Dependencies  ✔️   Cleaned  ✔️   Completed Successfully!  🚀  Debian setup has been successfully initialized!Step2: Configuring the LXconsoleOnce your server has booted and is running, you can connect to the server and start configuring LXConsole. To do this, use the LXC console to access your Debian server.❯ lxc-console XXXConnected to tty 1Type &lt;Ctrl+a q&gt; to exit the console, &lt;Ctrl+a Ctrl+a&gt; to enter Ctrl+a itselflxconsole login: lxconsole login: rootPassword: Debian LXC Container    🖥️    OS: Debian GNU/Linux - Version: 12    🏠   Hostname: lxconsole    💡   IP Address: XXXXXXXXXXroot@lxconsole:~# Next, install essential packages such as Git, Python 3, and Pip to prepare the environment for LXConsole:sudo apt install git python3 python3-pipOnce these packages are installed, clone the LXConsole repository from GitHub and install the necessary requirements:root@lxconsole:~# git clone https://github.com/PenningLabs/lxconsole.gitCloning into 'lxconsole'...remote: Enumerating objects: 3258, done.remote: Counting objects: 100% (797/797), done.remote: Compressing objects: 100% (287/287), done.remote: Total 3258 (delta 573), reused 719 (delta 500), pack-reused 2461 (from 1)Receiving objects: 100% (3258/3258), 22.76 MiB | 4.03 MiB/s, done.Resolving deltas: 100% (1143/1143), done.root@lxconsole:~# cd lxconsole &amp;&amp; pip3 install -r requirements.txtOnce the installation is complete, run LXConsole by executing the run.py file:python3 run.pyStep3: Creating the systemd fileTo ensure LXConsole starts automatically when the system boots up, you need to create a systemd service file. This will configure LXConsole to launch as a service during system startup.First, create the service file with the following command:touch /etc/systemd/system/lxconsole.serviceNext, add the following content to the file using you favourite editor or choice (eg: nano, vim) making sure to update the paths according to your environment:[Unit]Description=LXConsoleAfter=network.target[Service]Type=simpleUser=$USERWorkingDirectory=/location/to/gitclone/lxconsole/ExecStart=/usr/bin/python3 /location/to/gitclone/lxconsole/run.pyRestart=on-failure[Install]WantedBy=multi-user.targetAfter saving the file, reload the systemd daemon to apply the changes and start the LXConsole service:systemctl daemon-reloadsystemctl restart lxconsole.serviceThis ensures that LXConsole will run on startup and be ready to manage your LXD containers.Step4: Installing the lxdIf you dont have lxd server preconfigured you can install the lxd server by using the following stepsAs we are using the debian operating system debain has the apt package manager we use the apt package manager to install the lxd and initilaze itsudo apt-get install lxdlxd initDuring the initialization process, you will be prompted with a series of questions to configure LXD. Here’s a sample of what the configuration might look like:root@lxconsole:~/lxd# lxd initWould you like to use LXD clustering? (yes/no) [default=no]: Do you want to configure a new storage pool? (yes/no) [default=yes]: Name of the new storage pool [default=default]: lxdpoolWould you like to connect to a MAAS server? (yes/no) [default=no]: Would you like to create a new local network bridge? (yes/no) [default=yes]: What should the new bridge be called? [default=lxdbr0]: What IPv4 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]: What IPv6 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]: Would you like the LXD server to be available over the network? (yes/no) [default=no]: yesAddress to bind LXD to (not including port) [default=all]: Port to bind LXD to [default=8443]: Trust password for new clients: Again: Would you like stale cached images to be updated automatically? (yes/no) [default=yes]: Would you like a YAML \"lxd init\" preseed to be printed? (yes/no) [default=no]: root@lxconsole:~/lxd#Once you complete the configuration, LXD will be set up and ready to use.Step5: Accessing the lxconsoleNow that LXConsole is installed and running, you can access the web-based user interface. Open your browser and navigate to:http://$SERVER_IP:5000During the first boot of LXConsole, you will be prompted to register and log in with your credentials.Step6: Adding the lxd server to the lxconsoleOnce you loged in add the existing lxd container to the lxconsole to connect and manage the lxd conatiners                               Before adding the server to the lxconsole we need to add the client trusted certicateroot@lxconsole:~/lxd# nano lxconsole.crtroot@lxconsole:~/lxd# lxc config trust add lxconsole.crtroot@lxconsole:~/lxd# lxc config set core.https_address [::]Once the server is added successfully, you can begin managing your LXD containers and virtual machines via LXConsole.step7: Launching the lxd container (alpine)As we have configured lxd and lxconsole lets launch the alpine lxd container. Start by searching for available LXD images Then, launch a container using a specific image by executing the following command for example, to launch an Alpine Linux container, you can use:lxc image list imageslxc launch images:{distro}/{version}/{arch} {container-name-here}lxc info {distro}root@lxconsole:~/lxd# lxc image list images: | grep alpine | grep x86_64 | grep -i container| alpine/3.17 (3 more)                     | 25716e82c54e | yes    | Alpine 3.17 amd64 (20241218_0021)         | x86_64       | CONTAINER       | 2.94MB    | Dec 18, 2024 at 12:00am (UTC) || alpine/3.17/cloud (1 more)               | 0ffc7b828a14 | yes    | Alpine 3.17 amd64 (20241218_0021)         | x86_64       | CONTAINER       | 19.49MB   | Dec 18, 2024 at 12:00am (UTC) || alpine/3.18 (3 more)                     | 2dac80bf43a5 | yes    | Alpine 3.18 amd64 (20241218_0022)         | x86_64       | CONTAINER       | 2.95MB    | Dec 18, 2024 at 12:00am (UTC) || alpine/3.18/cloud (1 more)               | 959a3da69418 | yes    | Alpine 3.18 amd64 (20241218_0021)         | x86_64       | CONTAINER       | 20.79MB   | Dec 18, 2024 at 12:00am (UTC) || alpine/3.19 (3 more)                     | 52a2cf969e89 | yes    | Alpine 3.19 amd64 (20241218_0022)         | x86_64       | CONTAINER       | 2.93MB    | Dec 18, 2024 at 12:00am (UTC) || alpine/3.19/cloud (1 more)               | 305be1b7f6cf | yes    | Alpine 3.19 amd64 (20241218_0021)         | x86_64       | CONTAINER       | 20.98MB   | Dec 18, 2024 at 12:00am (UTC) || alpine/3.20 (3 more)                     | 9eef9c97c1a5 | yes    | Alpine 3.20 amd64 (20241218_0021)         | x86_64       | CONTAINER       | 3.08MB    | Dec 18, 2024 at 12:00am (UTC) || alpine/3.20/cloud (1 more)               | d75bd38b9454 | yes    | Alpine 3.20 amd64 (20241218_0021)         | x86_64       | CONTAINER       | 19.67MB   | Dec 18, 2024 at 12:00am (UTC) || alpine/edge (3 more)                     | 2e72675086fe | yes    | Alpine edge amd64 (20241218_0021)         | x86_64       | CONTAINER       | 3.09MB    | Dec 18, 2024 at 12:00am (UTC) || alpine/edge/cloud (1 more)               | 6426f253d902 | yes    | Alpine edge amd64 (20241218_0021)         | x86_64       | CONTAINER       | 20.13MB   | Dec 18, 2024 at 12:00am (UTC) |root@lxconsole:~/lxd# lxc launch images:alpine/3.20/amd64 alpineCreating alpineStarting alpine                           root@lxconsole:~/lxd# lxc list+--------+---------+----------------------+-----------------------------------------------+-----------+-----------+|  NAME  |  STATE  |         IPV4         |                     IPV6                      |   TYPE    | SNAPSHOTS |+--------+---------+----------------------+-----------------------------------------------+-----------+-----------+| alpine | RUNNING | XX.XX.XX.XX (eth0) | XXXX:XXXX:XXXX:XXXX:XXXX:XXXX:XXXX:XXXX (eth0) | CONTAINER | 0         |+--------+---------+----------------------+-----------------------------------------------+-----------+-----------+root@lxconsole:~/lxd# lxc info alpineName: alpineStatus: RUNNINGType: containerArchitecture: x86_64PID: 42834Created: 2024/12/18 16:34 ISTLast Used: 2024/12/18 16:34 ISTResources:  Processes: 5  CPU usage:    CPU usage (in seconds): 0  Memory usage:    Memory (current): 1.08MiB  Network usage:    eth0:      Type: broadcast      State: UP      Host interface: vethf8073443      MAC address: XX:XX:XX:XX:XX:XX      MTU: 1500      Bytes received: 48.06kB      Bytes sent: 1.73kB      Packets received: 1099      Packets sent: 15      IP addresses:        inet:  XX.XX.XX.XX (global)        inet6: XXXX:XXXX:XXXX:XXXX:XXXX:XXXX:XXXX:XXXX/64 (global)        inet6: XXXX::XXXX:XXXX:XXXX:XXXX/64 (link)    lo:      Type: loopback      State: UP      MTU: 65536      Bytes received: 0B      Bytes sent: 0B      Packets received: 0      Packets sent: 0      IP addresses:        inet:  127.0.0.1/8 (local)        inet6: ::1/128 (local)As we have launched the lxc container to connect to the newly created container, execute:lxc exec {containername} {command}root@lxconsole:~# lxc exec alpine sh~ # cat /etc/os-release NAME=\"Alpine Linux\"ID=alpineVERSION_ID=3.20.3PRETTY_NAME=\"Alpine Linux v3.20\"HOME_URL=\"https://alpinelinux.org/\"BUG_REPORT_URL=\"https://gitlab.alpinelinux.org/alpine/aports/-/issues\"~ # launching the Debian containerHere’s an elaboration of the section:Let’s launch the Debian container in the same way we did with the Alpine container. To do this, we first need to filter out the specific image we intend to use from the list of available images. In this case, we’re opting for the “debian/12/cloud” image, which is a cloud-specific version of Debian 12. This image is optimized for cloud environments, making it ideal for container deployment.To select the image, we can run the lxc image list command, which will display a list of available images from various sources. We can use the grep command to filter through the output and narrow down the list to only the Debian 12 cloud images.Once we have the correct image identified, we can proceed to launch the container. The command we’ll use is lxc launch, followed by the image reference (images:debian/12/cloud). This will create and start the container, naming it “debian” in the process. The container will then be accessible for further management or use.&gt; root@lxconsole:~# lxc image list images: | grep debian/12| debian/12 (7 more)                       | 13829fee748c | yes    | Debian bookworm amd64 (20250102_0002)     | x86_64       | CONTAINER       | 96.04MB   | Jan 2, 2025 at 12:00am (UTC)  || debian/12 (7 more)                       | ec5f73713766 | yes    | Debian bookworm amd64 (20250102_0002)     | x86_64       | VIRTUAL-MACHINE | 351.38MB  | Jan 2, 2025 at 12:00am (UTC)  || debian/12/arm64 (3 more)                 | 7f5fbadc7060 | yes    | Debian bookworm arm64 (20241230_0004)     | aarch64      | CONTAINER       | 93.10MB   | Dec 30, 2024 at 12:00am (UTC) || debian/12/cloud (3 more)                 | 9e3c8ec9bab8 | yes    | Debian bookworm amd64 (20250102_0002)     | x86_64       | VIRTUAL-MACHINE | 394.74MB  | Jan 2, 2025 at 12:00am (UTC)  || debian/12/cloud (3 more)                 | b7ef0541b199 | yes    | Debian bookworm amd64 (20250102_0002)     | x86_64       | CONTAINER       | 122.26MB  | Jan 2, 2025 at 12:00am (UTC)  || debian/12/cloud/arm64 (1 more)           | 29ec992650ff | yes    | Debian bookworm arm64 (20241230_0004)     | aarch64      | CONTAINER       | 118.60MB  | Dec 30, 2024 at 12:00am (UTC) |&gt; root@lxconsole:~# lxc launch images:debian/12/cloud debianCreating debianStarting debian                           &gt; root@lxconsole:~# lxc list+--------+---------+----------------------+-----------------------------------------------+-----------+-----------+|  NAME  |  STATE  |         IPV4         |                     IPV6                      |   TYPE    | SNAPSHOTS |+--------+---------+----------------------+-----------------------------------------------+-----------+-----------+| debian | RUNNING | 10.150.239.33 (eth0) | fd42:f569:921f:766a:216:3eff:fe0f:6cc6 (eth0) | CONTAINER | 0         |+--------+---------+----------------------+-----------------------------------------------+-----------+-----------+&gt; root@lxconsole:~# lxc exec debian /bin/bash&gt; root@debian:~#LXConsole allows you to manage the containers, view logs, and perform other administrative tasks from a central web interface, making it a powerful tool for container management.With LXConsole up and running, you now have a comprehensive web-based solution for managing your LXD containers. Whether you’re deploying new containers, configuring existing ones, or monitoring your containerized environment, LXConsole offers an intuitive and efficient interface to simplify the process.For those looking to deepen their understanding of LXD commands and management, we’ve ceated LXC Cheat Sheet, which serves as a handy reference for common LXC commands and their usage."
  },
  
  {
    "title": "Disk Management on proxmoxVM",
    "url": "/posts/Diskmanagement/",
    "categories": "Virtualization",
    "tags": "",
    "date": "2024-12-09 00:00:00 +0530",
    





    
    "snippet": "In this blog post, we will explore the process of increasing disk space for a Debian virtual machine (VM) running on Proxmox. This guide will detail the commands used and the output received during...",
    "content": "In this blog post, we will explore the process of increasing disk space for a Debian virtual machine (VM) running on Proxmox. This guide will detail the commands used and the output received during the resizing operation, providing a comprehensive overview for users looking to expand their VM’s storage capacity.Step 1: Resize Disk in ProxmoxFirst, you need to increase the disk size from the Proxmox GUI. This can be done by selecting your VM, navigating to the “Hardware” tab, and adjusting the disk size accordingly. After resizing the disk in Proxmox, you can verify the new size by using the parted command.root@pbs:~# parted                                                                                                                                                                            GNU Parted 3.5                                                                                                                                                                                Using /dev/sda                                                                                                                                                                                Welcome to GNU Parted! Type 'help' to view a list of commands.                                                                                                                                (parted)                                                                                                                                                                                      (parted) print                                                                                                                                                                                Warning: Not all of the space available to /dev/sda appears to be used, you can fix the GPT to use all of the space (an extra 23068672 blocks) or continue with the current setting?          Fix/Ignore? f                                                                                                                                                                                 Model: QEMU QEMU HARDDISK (scsi)                                                                                                                                                              Disk /dev/sda: 22.5GB                                                                                                                                                                         Sector size (logical/physical): 512B/512B                                                                                                                                                     Partition Table: gpt                                                                                                                                                                          Disk Flags:                                                                                                                                                                                                                                                                                                                                                                                 Number  Start   End     Size    File system  Name  Flags                                                                                                                                       1      17.4kB  1049kB  1031kB                     bios_grub                                                                                                                                   2      1049kB  538MB   537MB   fat32              boot, esp                                                                                                                                   3      538MB   10.7GB  10.2GB                     lvm                                                                                                                                        Step 2: Verify Disk SizeAfter fixing the partition table, print the partition information again:(parted) resizepart 3 100%                                                 (parted) print                                                             Model: QEMU QEMU HARDDISK (scsi)Disk /dev/sda: 22.5GBSector size (logical/physical): 512B/512BPartition Table: gptDisk Flags: Number  Start   End     Size    File system  Name  Flags 1      17.4kB  1049kB  1031kB                     bios_grub 2      1049kB  538MB   537MB   fat32              boot, esp 3      538MB   22.5GB  22.0GB                     lvmStep 3: Resize Physical VolumeNext, resize the physical volume associated with your logical volume manager (LVM):root@pbs:~# pvresize /dev/sda3  Physical volume \"/dev/sda3\" changed  1 physical volume(s) resized or updated / 0 physical volume(s) not resizedroot@pbs:~# lsblkNAME         MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTSsda            8:0    0   21G  0 disk ├─sda1         8:1    0 1007K  0 part ├─sda2         8:2    0  512M  0 part └─sda3         8:3    0 20.5G  0 part   ├─pbs-swap 252:0    0    1G  0 lvm  [SWAP]  └─pbs-root 252:1    0  8.5G  0 lvm  /sr0           11:0    1 1024M  0 rom  root@pbs:~# df -hFilesystem                             Size  Used Avail Use% Mounted onudev                                   953M     0  953M   0% /devtmpfs                                  198M  772K  197M   1% /run/dev/mapper/pbs-root                   8.3G  7.7G  125M  99% /tmpfs                                  986M  200K  986M   1% /dev/shmtmpfs                                  5.0M     0  5.0M   0% /run/lock20.20.20.210:/mnt/nasp/proxmox-backup  4.3T  167G  4.1T   4% /mnt/nasptmpfs                                  198M     0  198M   0% /run/user/0Step 4: Extend Logical VolumeNow that the physical volume is resized, extend your logical volume using:root@pbs:~# lvextend -r -l +100%FREE /dev/mapper/pbs-root                                         Size of logical volume pbs/root changed from 8.49 GiB (2174 extents) to &lt;19.50 GiB (4991 extents).  Logical volume pbs/root successfully resized. resize2fs 1.47.0 (5-Feb-2023)Filesystem at /dev/mapper/pbs-root is mounted on /; on-line resizing requiredold_desc_blocks = 2, new_desc_blocks = 3The filesystem on /dev/mapper/pbs-root is now 5110784 (4k) blocks long.Step 5: Verify Filesystem SizeFinally, check if the filesystem reflects the new size:root@pbs:~# df -hFilesystem                             Size  Used Avail Use% Mounted onudev                                   953M     0  953M   0% /devtmpfs                                  198M  792K  197M   1% /run/dev/mapper/pbs-root                    20G  7.8G   11G  43% /tmpfs                                  986M  200K  986M   1% /dev/shmtmpfs                                  5.0M     0  5.0M   0% /run/lock20.20.20.210:/mnt/nasp/proxmox-backup  4.3T  167G  4.1T   4% /mnt/nasptmpfs                                  198M     0  198M   0% /run/user/0"
  },
  
  {
    "title": "Jenkins LXC on proxmox",
    "url": "/posts/jenkins/",
    "categories": "Virtualization",
    "tags": "",
    "date": "2024-10-04 00:00:00 +0530",
    





    
    "snippet": "In this blog post, we will go through the steps to install Jenkins in Proxmox using the TurnKey image. This method is quite simple and efficient for setting up Jenkins in a containerized environmen...",
    "content": "In this blog post, we will go through the steps to install Jenkins in Proxmox using the TurnKey image. This method is quite simple and efficient for setting up Jenkins in a containerized environment.Download Jenkins TurnKey ImageFirst, we need to download the Jenkins TurnKey image for Proxmox. Open your terminal and run the following command:❯ pveam download $storage debian-12-turnkey-jenkins_18.0-1_amd64.tar.gzThis command will download the image from the TurnKey Linux repository. You will see output indicating the progress of the download, and once it’s finished, you will have the image ready for use.❯ pveam download nasp debian-12-turnkey-jenkins_18.0-1_amd64.tar.gzdownloading http://mirror.turnkeylinux.org/turnkeylinux/images/proxmox/debian-12-turnkey-jenkins_18.0-1_amd64.tar.gz to /mnt/pve/nasp/template/cache/debian-12-turnkey-jenkins_18.0-1_amd64.tar.gz--2024-10-04 18:50:44--  http://mirror.turnkeylinux.org/turnkeylinux/images/proxmox/debian-12-turnkey-jenkins_18.0-1_amd64.tar.gzResolving mirror.turnkeylinux.org (mirror.turnkeylinux.org)... 137.226.34.46, 131.188.12.211, 2801:82:80ff:8000::eConnecting to mirror.turnkeylinux.org (mirror.turnkeylinux.org)|137.226.34.46|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 775492051 (740M) [application/octet-stream]Saving to: '/mnt/pve/nasp/template/cache/debian-12-turnkey-jenkins_18.0-1_amd64.tar.gz.tmp_dwnl.3535802'     0K ........ ........ ........ ........  4% 2.21M 5m20s 98304K ........ ........ ........ ........ 17% 3.15M 3m45s131072K ........ ........ ........ ........ 21% 3.85M 3m20s262144K ........ ........ ........ ........ 38% 3.02M 2m32s360448K ........ ........ ........ ........ 51% 3.53M 1m54s\\491520K ........ ........ ........ ........ 69% 3.90M 72s524288K ........ ........ ........ ........ 73% 3.72M 61s555360K ........ ........ ........ ........ 90% 3.43M 22s720896K ........ ........ ........ ........ 99% 4.07M 1s753664K ...                                100% 4.79M=3m58s2024-10-04 18:54:42 (3.11 MB/s) - '/mnt/pve/nasp/template/cache/debian-12-turnkey-jenkins_18.0-1_amd64.tar.gz.tmp_dwnl.3535802' saved [775492051/775492051]calculating checksum...OK, checksum verifieddownload of 'http://mirror.turnkeylinux.org/turnkeylinux/images/proxmox/debian-12-turnkey-jenkins_18.0-1_amd64.tar.gz' to 'template/cache/debian-12-turnkey-jenkins_18.0-1_amd64.tar.gz' finishedCreate a Container (CT)Now that we have the image, we can create a container using it. Here’s how you can do it:  Open Proxmox Web Interface: Go to your Proxmox web interface.  Create CT: Click on “Create CT” and follow the prompts to set up your container.  Select Template: When prompted, select the downloaded Jenkins TurnKey image as your template.Starting the Jenkins VMOnce the container is created, boot it up. When you try to access the Jenkins UI for the first time, you will see a message like this:  Welcome to TurnKey! You need to initialize this system first before you can use it. To do that you’ll need to log into the root account via SSH. The turnkey-init initialization program should start automatically After initialization try reloading this page. This message should disappear and you’ll be able to access all services on this system normally.Login into the container by using the ssh service and reset the credentialsPerform Security UpdatesFor security reasons, it is recommended to perform updates after installation. Run the following command in your terminal:Once the security patches are installed and configured reboot the containerAccess Jenkins Web UIAfter rebooting, access the Jenkins web UI again by navigating to “https://&lt;your-container-ip&gt;”. You should see the Jenkins control panel.Manage Jenkins PluginsOnce logged in, go to Manage Jenkins &gt; Manage Plugins &gt; Updates and install all latest updates for plugins.Install Dark Theme (Optional)If you prefer a dark theme, you can install it from the plugin manager as well.Final StepsAfter updating all plugins and installing any desired themes, restart your Jenkins container one last time.And that’s it! You have successfully installed Jenkins on Proxmox using a TurnKey image. Enjoy automating your builds and deployments with Jenkins!For more robust information on using Jenkins, you can refer to the official Jenkins documentation available at Jenkins Documentation"
  },
  
  {
    "title": "Wake on LAN for Proxmox Node",
    "url": "/posts/wol/",
    "categories": "Virtualization",
    "tags": "",
    "date": "2024-09-26 00:00:00 +0530",
    





    
    "snippet": "This guide explains how to set up your Proxmox node to respond to Wake-on-LAN (WoL) messages, allowing you to start your server remotely using a “magic packet.”installing toolsFirst, you’ll need to...",
    "content": "This guide explains how to set up your Proxmox node to respond to Wake-on-LAN (WoL) messages, allowing you to start your server remotely using a “magic packet.”installing toolsFirst, you’ll need to install the required tool ethtool on your Proxmox node. Open a terminal or go to your node’s shell and run:❯ sudo apt install ethtoolGet your interface name❯ ip a sThe output will list all your network interfaces. In this case, we’re looking for an interface that’s currently in use. For example:eno1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master vmbr0 state UP group default qlen 1000    link/ether XXXXXXXXXXX brd XXXXXXXXXXX    altname enp1s0f0Here, eno1 is the name of the network interface.Check If Wake-on-LAN is EnabledBefore enabling Wake-on-LAN, it’s a good idea to check if it is already active on your interface. Run:❯ ethtool eno1 | grep Wake-onThe output will look like this:        Supports Wake-on: pumbg        Wake-on: dIn this example, Wake-on-LAN is disabled (d stands for “disabled”). We will enable it.Enable wake-on-lanTo enable the wake on lan we use the ethool and specify the insterface on which we want to enable it❯ ethtool -s eno1 wol gUpdating the Network Interfaces FileTo make sure that Wake-on-LAN stays enabled after a reboot, we need to add this configuration to the network interfaces file. Open the file using:❯ nano /etc/network/interfacesLook for your interface (in our case eno1) and add the following line below the iface configuration:post-up /usr/sbin/ethtool -s enp6s0 wol gThe section for eno1 should now look something like this:auto eno1  iface eno1 inet manual  post-up /usr/sbin/ethtool -s eno1 wol gThis ensures that WoL will remain enabled after reboots.Reload Proxmox Network SettingsTo apply the changes, reload the network configuration from the Proxmox GUI. Go to your Proxmox node, then navigate to System &gt; Network. Find your WoL-enabled interface (eno1), click “Edit,” and add the “Autostart” flag. Save the changes.Testing Wake-on-LANNow that the configuration is set, you can test it. Turn off your Proxmox node and use a WoL tool to send a magic packet to the node’s network interface. I’m using the built-in WoL feature in ipfire to send the magic packet.If your Proxmox node powers up, the Wake-on-LAN setup is complete and working correctly."
  },
  
  {
    "title": "IPFire Firewall on Proxmox",
    "url": "/posts/ipfire/",
    "categories": "Virtualization",
    "tags": "",
    "date": "2024-09-22 00:00:00 +0530",
    





    
    "snippet": "Installing IPFire Firewall on ProxmoxIPFire is a powerful open-source firewall that can be installed on both 32-bit and 64-bit systems. If you have an old PC lying around, you can repurpose it as a...",
    "content": "Installing IPFire Firewall on ProxmoxIPFire is a powerful open-source firewall that can be installed on both 32-bit and 64-bit systems. If you have an old PC lying around, you can repurpose it as a firewall. IPFire is known for its flexibility, and it’s great for securing networks in both home and small business environments.  Open-source: It’s free and constantly updated by the community.  Security: Offers strong protection for your network with features like intrusion detection, VPN, and more.  Customizable: It can be tailored to your specific network needs.Download the ipfire ISOTo get started, you’ll need to download the IPFire ISO. Run the following command to download the ISO directly to your Proxmox storage:wget -O /mnt/pve/$storage/template/iso/ipfire-2.29-core188-x86_64.iso https://downloads.ipfire.org/releases/ipfire-2.x/2.29-core188/ipfire-2.29-core188-x86_64.isocreating the vmTo create a virtual machine (VM) for IPFire in Proxmox, follow these steps:  Click on Create VM.  Go to the General tab and set the preferred VM ID and name.  In the OS section, choose the downloaded IPFire ISO from $storage.  Set up the disk as per your requirements in the Disk section.  Configure CPU and Memory depending on your system needs.Once the VM is created, add two network interfaces: one for WAN and one for LAN.System Requirements            Component      Minimum Requirements      Recommended Requirements                  Processor (CPU)      1 GHz single-core      Multi-core (2+ cores) with AES-NI support              Memory (RAM)      1 GB      2-4 GB or more, especially for IDS/IPS, VPN              Storage      4 GB (HDD/SSD)      20 GB SSD for better performance and longevity              Network Interface      At least 2 NICs (10/100 Mbps)      2 NICs (Gigabit) Intel or Broadcom NICs for reliability        If you prefer to install IPFire on bare metal, you can create bootable media. Visit the official IPFire Download page and grab the ISO image according to your system architecture.Installing the distributionOnce the vm is created and the two networking nics are attached start the vm.Next, choose the Language as respective to your region.At this step, you can see that, if you do not wish to continue the setup you can Cancel the setup and reboot the machine.Accept the license by pressing the Space bar to choose, and press OK to continue.The system will format your disk to install the IPFire system. Please note that all data on the disk will be erased.Next, choose the file system as EXT4 and continue to the future steps.Once, you select the filesystem type, the installation begins and disk will be formatted and system files will be installed.Once installation completes, press OK to reboot to finalize the installation and continue with the further installation to configure ISDN, network cards, and system passwords.After the system reboot, it will prompt you IPFire boot menu option, select the default option by pressing the enter key.choose the keyboard layout from the drop-down list as shown below. and choose the time zone you preffered                               Choose a hostname for our IPFirewall machine. By default, it will be ipfire. I’m not going to make any changes in these steps. and Give a valid domain name, if you have a local DNS server or we can define it later. Here, I am using “secsys.pro” as my local DNS server domain name.                               Enter a password for the root and admin user, This will be used for command-line access and provide a password for the admin user for the IPFire GUI web interface. The password must be different from the command line access credentials for security reasons.                               IPFire Network Configuration SettingsDuring IPFire installation, the network is configured into various segments. This segmented security scheme indicates that there is a suitable place for each system in the network and it can be enabled separately as per our requirements.Each segment acts as a group of machines that share a common security level, which is described in four different colors of zones i.e. Green, Red, Blue, and Orange.  Green  – This represents that we are in a safe area. Clients in the Green area will be without any restrictions and connected internally/locally.  Red  – This indicates that we are in danger or disconnected from the outside world; nothing will be allowed through the firewall unless specifically configured by the admins.  Blue  – This represents the ‘wireless‘ network, which is used for the local area network.  Orange  – This refers to the ‘DMZ‘ (demilitarized zone). Any servers that are accessible publicly are separated from the rest of the network to minimize security breaches.                               Once the Network configuration type is selected then network cards to each adaptor RED and Green zones                               Assign IP Addresses for Network InterfacesHere we have only 2 interfaces and we need to assign IP addresses in different sub-nets one for the RED zone and another for Green zoneIf we use 192.168.0.100 for the RED interface, we must use different IP and network for other interface. For the RED interface we going to use static IP address for both RED and green zones                               If you want to enable the DHCP for the GREEN interface for the local/internal interfaces you can can configure the DHCP pool range as i am using the tp-link (TL-S3210) for DHCP service will skip the DHCP.We have almost completed our setup, Choose OK to complete the IPFire setup.Accessing Web interfaceTo access the web interface connect to the green network and enter https://$I:444/ with the username as “admin” and the password that we’ve set at above.conclusionIPFire is a versatile firewall solution suitable for various environments. Whether you are setting it up for home or small business use, its security features, flexibility, and community-driven updates make it a great choice for protecting your network. for more settings and customization please reffer the official IPfire documentation"
  },
  
  {
    "title": "Installing Parrot OS on Proxmox VM",
    "url": "/posts/parrot/",
    "categories": "Virtualization",
    "tags": "",
    "date": "2024-09-03 00:00:00 +0530",
    





    
    "snippet": "Installing Parrot OS on ProxmoxIn this guide, we’ll walk through the process of installing Parrot OS on a Proxmox virtual machine. Parrot OS is a popular security-focused operating system, and runn...",
    "content": "Installing Parrot OS on ProxmoxIn this guide, we’ll walk through the process of installing Parrot OS on a Proxmox virtual machine. Parrot OS is a popular security-focused operating system, and running it on Proxmox allows you to experiment within a virtualized environment.Step 1: Download the Parrot OS ISOFirst, download the Parrot OS ISO from the official website. Below is the command for downloading the ISO directly to your Proxmox server:wget -O /mnt/pve/$storage/template/iso/Parrot-security-6.1_amd64.iso https://deb.parrot.sh/parrot/iso/6.1/Parrot-security-6.1_amd64.isoThis will save the ISO file to your Proxmox storage.Step 2: Create a Virtual Machine on ProxmoxOnce the ISO is downloaded, go to the Proxmox web interface and click on the “Create VM” button to start creating a new virtual machine.  Name your VM: In the General tab, give your VM a name, for example, “Parrot OS.”  Select ISO image: Under the OS tab, select the Parrot OS ISO that you just downloaded. Keep the system settings as default.  Configure Disk: Parrot OS requires a minimum of 16 GB of disk space, but for this setup, we’ll allocate 100 GB. Choose the storage location, format it to QCOW2, and configure the disk settings  Allocate CPU and Memory: Assign the number of cores and memory for the virtual machine according to your requirements. For a smooth experience, you can allocate 2 cores and 4 GB of RAM.                                 Review and Confirm: Review the VM configuration and check the box that says “Start after created” to boot the VM automatically after creation.Step 3: Install Parrot OSOnce the VM is created and booted, open the console from the Proxmox interface.  Boot into the Installer: When the VM starts, you’ll see the Parrot OS boot menu. Select the “Try/Install” option to begin the installation process.                                 Select Preferences: Choose your preferred language, location, and keyboard layout.                                              Disk Partitioning: Erase the disk and select “Swap to a file” for swap management. Create a user account and verify the settings.                                 Complete Installation: Once you confirm the settings, the installation will begin. After the installation is complete, the VM will reboot, and you’ll be ready to use Parrot OS on Proxmox.                               Step 4: Post InstallationOnce the installation is completed open the terminal and perform the update to update the security patchessudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo apt dist-upgrade -y &amp;&amp; sudo apt autoremove -y &amp;&amp; sudo apt autoclean -y &amp;&amp; sudo apt clean -yFor advanced configurations, such as customizing the installation further or troubleshooting, refer to the official Parrot OS installation guide."
  },
  
  {
    "title": "Post Proxmox",
    "url": "/posts/proxmox/",
    "categories": "Virtualization",
    "tags": "",
    "date": "2024-08-28 00:00:00 +0530",
    





    
    "snippet": "IntroductionAfter installing Proxmox VE (Virtual Environment), there are several essential post-installation steps to ensure that your system is configured correctly and optimized for performance. ...",
    "content": "IntroductionAfter installing Proxmox VE (Virtual Environment), there are several essential post-installation steps to ensure that your system is configured correctly and optimized for performance. This guide walks you through managing Proxmox VE repositories, updating the system, handling processor microcode updates, and setting up automatic LXC updates.1. Managing Proxmox VE RepositoriesProxmox VE repositories need to be configured correctly to ensure you receive updates and access the necessary packages. The following script helps manage these repositories by:  Disabling the Enterprise Repo.  Adding or correcting Proxmox VE sources.  Enabling the No-Subscription Repo.  Adding the Test Repo.  Disabling the subscription nag.Running the Repository Management ScriptTo streamline the configuration process, run the following command in the Proxmox VE Shell:bash -c \"$(wget -qLO - https://github.com/tteck/Proxmox/raw/main/misc/post-pve-install.sh)\"Note: It is recommended to answer “yes” (y) to all options presented during the process. This will ensure that all necessary configurations and updates are applied correctly.Script OutputThe script performs several tasks, including:  Correcting Proxmox VE sources.  Disabling the ‘pve-enterprise’ repository and enabling the ‘pve-no-subscription’ repository.  Correcting Ceph package repositories and adding the ‘pvetest’ repository.  Disabling the subscription nag (you might need to delete your browser cache).  Updating Proxmox VE. ____ _    ________   ____             __     ____           __        ____/ __ \\ |  / / ____/  / __ \\____  _____/ /_   /  _/___  _____/ /_____ _/ / // /_/ / | / / __/    / /_/ / __ \\/ ___/ __/   / // __ \\/ ___/ __/ __ `/ / // ____/| |/ / /___   / ____/ /_/ (__  ) /_   _/ // / / (__  ) /_/ /_/ / / //_/     |___/_____/  /_/    \\____/____/\\__/  /___/_/ /_/____/\\__/\\__,_/_/_/ ✓ Corrected Proxmox VE Sources ✓ Disabled 'pve-enterprise' repository ✓ Enabled 'pve-no-subscription' repository ✓ Corrected 'ceph package repositories' ✓ Added 'pvetest' repository ✓ Disabled subscription nag (Delete browser cache) ✓ Disabled high availability ✓ Updated Proxmox VE ✗ Selected no to Rebooting Proxmox VE (Reboot recommended) ✓ Completed Post Install Routines ✗ Selected no to Rebooting Proxmox VE (Reboot recommended) ✓ Completed Post Install Routines2. Updating Processor MicrocodeProcessor microcode updates are crucial for fixing hardware bugs and improving performance. This section covers how to update microcode for Intel or AMD processors.Running the Microcode Update ScriptExecute the following command in the Proxmox VE Shell to handle microcode updates:bash -c \"$(wget -qLO - https://github.com/tteck/Proxmox/raw/main/misc/microcode.sh)\"Checking Microcode UpdatesAfter running the script and rebooting the system, verify that microcode updates are applied by executing:journalctl -k | grep -E \"microcode\" | head -n 1 ____                                               __  ____                                __/ __ \\_________  ________  ______________  _____   /  |/  (_)_____________  _________  ____/ /__/ /_/ / ___/ __ \\/ ___/ _ \\/ ___/ ___/ __ \\/ ___/  / /|_/ / / ___/ ___/ __ \\/ ___/ __ \\/ __  / _ \\/ ____/ /  / /_/ / /__/  __(__  |__  ) /_/ / /     / /  / / / /__/ /  / /_/ / /__/ /_/ / /_/ /  __//_/   /_/   \\____/\\___/\\___/____/____/\\____/_/     /_/  /_/_/\\___/_/   \\____/\\___/\\____/\\__,_/\\___/ ✓ GenuineIntel was detected ✓ Installed iucode-tool ✓ Downloaded the Intel Processor Microcode Package intel-microcode_3.20240514.1~deb12u1_amd64.deb ✓ Installed intel-microcode_3.20240514.1~deb12u1_amd64.deb ✓ CleanedIn order to apply the changes, a system reboot will be necessary.3. Setting Up Automatic LXC UpdatesTo keep your LXCs (Linux Containers) up-to-date, you can schedule a cron job that updates all LXCs every Sunday at midnight. This helps in maintaining the security and performance of your containers.Running the LXC Updater ScriptTo set up the cron job, run the following command in the Proxmox VE Shell:bash -c \"$(wget -qLO - https://github.com/tteck/Proxmox/raw/main/misc/cron-update-lxcs.sh)\"Excluding Specific LXCsIf you need to exclude specific LXCs from updating, edit the crontab (use crontab -e) and add the CTID(s) as shown in the example:0 0 * * 0 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin /bin/bash -c \"$(wget -qLO - https://github.com/tteck/Proxmox/raw/main/misc/update-lxcs-cron.sh)\" -s 103 111 &gt;&gt;/var/log/update-lxcs-cron.log 2&gt;/dev/nullConclusionProperly configuring and maintaining your Proxmox VE installation is crucial for ensuring optimal performance and security. By following these post-installation steps, including managing repositories, updating microcode, and setting up automatic LXC updates, you can ensure that your Proxmox VE environment is optimized and running smoothly. If you encounter any issues or have questions, consult the Proxmox documentation or community forums for additional support."
  },
  
  {
    "title": "Medicat",
    "url": "/posts/medicat/",
    "categories": "Windows",
    "tags": "",
    "date": "2024-08-21 00:00:00 +0530",
    





    
    "snippet": "Installing MediCatIn this guide, we’ll walk through the steps to install MediCat on a USB drive. MediCat is a powerful tool for diagnostics, troubleshooting, and recovery.PrerequisitesBefore we beg...",
    "content": "Installing MediCatIn this guide, we’ll walk through the steps to install MediCat on a USB drive. MediCat is a powerful tool for diagnostics, troubleshooting, and recovery.PrerequisitesBefore we begin, make sure you have the following:  A USB drive with at least 32GB of space (Recomended 64GB)  A USB drive with Ventoy already installed (Recomended)  A downloaded archive of MediCat (~22GB)  Windows Defender and other antivirus tools temporarily disabled (since they may interfere with the installation process)  Please select the USB drive carefully during the installation process, as the disk will be completely wiped.Step 1: Preparing the USB DriveTo start, you’ll need to install Ventoy on your USB drive. Ventoy is a tool that allows you to create a bootable USB drive that can host multiple ISO files without reformatting the drive.Begin by downloading the latest version of Ventoy from the official website. Once downloaded, extract the Ventoy archive to a folder on your computer.In the extracted folder, locate and run the Ventoy2Disk.exe (or the corresponding executable for your operating system).Next, select the USB drive you want to use with Ventoy. Be cautious when choosing the drive, as this process will format it. After confirming that you’ve selected the correct drive, click the “Install” button. The tool will prompt you to confirm the installation, warning that all data on the USB drive will be erased. Confirm and proceed.Once the installation is complete, you will see a success message. Your USB drive is now ready with Ventoy installed.Since we’ve manually installed Ventoy, there’s no need to format the USB drive again. Additionally, it’s important to temporarily disable Windows Defender and any other antivirus tools because they may interfere with the installation process.After disabling Windows Defender, you can proceed by launching the MediCat installer and accepting the license agreement.                               Step 2: Installing MediCatThe installer will prompt you to select the disk where MediCat will be installed. Double-check that you have selected the correct drive to avoid any data loss. During the installation of MediCat, you have the option to disable drive formatting. This step is crucial if you have already installed Ventoy, as formatting the disk would remove Ventoy from the USB drive. Make sure to disable formatting before proceeding.                               Step 3: Handling the MediCat ArchiveIf you already have the MediCat archive downloaded (approximately 22GB), select “No” when asked to download it during the installation process. Then, manually select the downloaded file. This saves time, as downloading it again would take a while.If you haven’t downloaded the archive yet, you can opt to download it during this step, though it will extend the installation time.Step 4: Finalizing the InstallationOnce you’ve selected the MediCat archive file (or downloaded it), the installer will begin the installation process. If you’ve chosen the “Download Now” option, the file will be downloaded first, and then the installation will proceed.  As medicat needs to install/configure the 22GB archive it’s took me about 2 hour 40 minutes to complete the installation/checks.Congratulations! You’ve successfully installed MediCat on your USB drive."
  },
  
  {
    "title": "Grafana",
    "url": "/posts/grafana/",
    "categories": "Linux",
    "tags": "",
    "date": "2024-07-24 00:00:00 +0530",
    





    
    "snippet": "Grafana is a powerful open-source analytics and monitoring tool that allows you to visualize data in a highly customizable way. This guide will walk you through the steps to install Grafana from th...",
    "content": "Grafana is a powerful open-source analytics and monitoring tool that allows you to visualize data in a highly customizable way. This guide will walk you through the steps to install Grafana from the APT repository on a Debian system.PrerequisitesBefore you start, make sure your system is up-to-date. Run the following commands to update your package list and upgrade all installed packagessudo apt-get updatesudo apt-get upgrade -yAdditionally, ensure that you have sudo privileges to execute administrative commands.Step 1: Install Prerequisite PackagesGrafana requires some prerequisite packages. Install them by running the following command:sudo apt-get install -y apt-transport-https software-properties-common wget  apt-transport-https: Allows APT to communicate over HTTPS, which is required for secure access to external repositories.  software-properties-common: Provides a utility for managing software repositories.  wget: A utility to download files from the web.Step 2: Import the Grafana GPG KeyTo ensure the integrity of the packages you are about to install, you need to import the Grafana GPG key. This key is used to verify the authenticity of the packages. Run the following commands:sudo mkdir -p /etc/apt/keyrings/wget -q -O - https://apt.grafana.com/gpg.key | gpg --dearmor | sudo tee /etc/apt/keyrings/grafana.gpg &gt; /dev/null  mkdir -p /etc/apt/keyrings/: Creates a directory to store GPG keys.  wget -q -O - https://apt.grafana.com/gpg.key: Downloads the Grafana GPG key.  gpg –dearmor: Converts the GPG key to a format that APT can use.  sudo tee /etc/apt/keyrings/grafana.gpg &gt; /dev/null: Saves the key to the appropriate location.Step 3: Add the Grafana APT RepositoryNext, you need to add the Grafana APT repository to your system’s list of package sources. This will allow you to install Grafana directly from the APT repository. You have two options:Stable ReleasesFor stable releases of Grafana, run the following command:echo \"deb [signed-by=/etc/apt/keyrings/grafana.gpg] https://apt.grafana.com stable main\" | sudo tee -a /etc/apt/sources.list.d/grafana.listBeta ReleasesIf you want to install beta releases of Grafana, use this command instead:echo \"deb [signed-by=/etc/apt/keyrings/grafana.gpg] https://apt.grafana.com beta main\" | sudo tee -a /etc/apt/sources.list.d/grafana.listThis will add the chosen repository to your system’s package sources.Step 4: Update the Package ListAfter adding the repository, you need to update the list of available packages:sudo apt-get updateThis will fetch the latest package information from the newly added Grafana repository.Step 5: Install Grafana OSFinally, to install the latest open-source release of Grafana, run the following command:sudo apt-get install grafanaThis will download and install Grafana and its dependencies.Step 6: Start and Enable GrafanaAfter the installation is complete, you can start Grafana and enable it to start at boot with the following commands:sudo systemctl start grafana-serversudo systemctl enable grafana-server  start grafana-server: Starts the Grafana service immediately.  enable grafana-server: Configures Grafana to start automatically on system boot.Step 7: Access GrafanaGrafana runs on port 3000 by default. You can access the Grafana web interface by opening your web browser and navigating to:http://your-server-ip:3000  Change the /etc/grafana/grafana.ini file according to your needs and restart the grafana serviceReplace your-server-ip with your server’s actual IP address.Default Login CredentialsThe default username and password for Grafana are:  Username: admin  Password: adminUpon the first login, you will be prompted to change the password. Once you logged in add the data source and start configuring grafana to monitor and visualize your data.For more detailed documentation and advanced configuration options, visit the official Grafana documentation"
  },
  
  {
    "title": "Prometheus",
    "url": "/posts/prometheus/",
    "categories": "Linux",
    "tags": "",
    "date": "2024-07-20 00:00:00 +0530",
    





    
    "snippet": "IntroductionPrometheus is a powerful open-source monitoring system and time series database. This guide walks you through installing and configuring Prometheus on a Linux system. By the end of this...",
    "content": "IntroductionPrometheus is a powerful open-source monitoring system and time series database. This guide walks you through installing and configuring Prometheus on a Linux system. By the end of this tutorial, you’ll have Prometheus up and running, ready to monitor your infrastructure.1. Create Prometheus UserFirst, create a dedicated Prometheus user to enhance security:sudo useradd -M prometheussudo usermod -L prometheus2. Set Up DirectoriesNext, create the necessary directories for Prometheus and set the correct ownership:sudo mkdir /etc/prometheussudo mkdir /var/lib/prometheussudo chown prometheus:prometheus /etc/prometheussudo chown prometheus:prometheus /var/lib/prometheus3. Download and Extract PrometheusDownload the latest Prometheus release from GitHub:cd ~wget https://github.com/prometheus/prometheus/releases/download/prometheus.linux-amd64.tar.gzsha256sum prometheus.linux-amd64.tar.gz # Verify the checksum before extractingtar xvf prometheus.linux-amd64.tar.gz4. Install PrometheusCopy the Prometheus binaries to the appropriate directories:sudo cp prometheus.linux-amd64/prometheus /usr/local/bin/sudo cp prometheus.linux-amd64/promtool /usr/local/bin/sudo chown prometheus:prometheus /usr/local/bin/prometheussudo chown prometheus:prometheus /usr/local/bin/promtoolMove configuration files to /etc/prometheus:sudo cp -r prometheus.linux-amd64/prometheus.yml /etc/prometheus/sudo cp -r prometheus.linux-amd64/consoles /etc/prometheussudo cp -r prometheus.linux-amd64/console_libraries /etc/prometheussudo chown -R prometheus:prometheus /etc/prometheus/prometheus.ymlsudo chown -R prometheus:prometheus /etc/prometheus/consolessudo chown -R prometheus:prometheus /etc/prometheus/console_libraries5. Configure PrometheusEdit the Prometheus configuration file:sudo vi /etc/prometheus/prometheus.yml# my global configglobal:  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.  # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting:  alertmanagers:  - static_configs:    - targets:      # - alertmanager:port# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.rule_files:  # - \"first_rules.yml\"  # - \"second_rules.yml\"# A scrape configuration containing exactly one endpoint to scrape:# Here it's Prometheus itself.scrape_configs:  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.  - job_name: 'prometheus'    # metrics_path defaults to '/metrics'    # scheme defaults to 'http'.    static_configs:    - targets: ['localhost:3004']#################################################################################################  - job_name: '$NAME'    metrics_path: '/api/v1/allmetrics'    params:      # format: prometheus | prometheus_all_hosts      # You can use `prometheus_all_hosts` if you want Prometheus to set the `instance` to your hostname instead of IP      format: [prometheus_all_hosts]      #      # source: as-collected | raw | average | sum | volume      # default is: average      #source: [as-collected]      #      # server name for this prometheus - the default is the client IP      # for Netdata to uniquely identify it      server: ['grove']    honor_labels: true    static_configs:      - targets: ['$IP:PORT']#################################################################################################                                                                                                                                          This setup configures Prometheus to scrape metrics from itself at localhost:3004.6. Create an Init Script for PrometheusCreate the init script to manage the Prometheus service:sudo nano /etc/init.d/prometheusCopy the init script from the provided code, then save and close the file.#!/bin/sh### BEGIN INIT INFO# Provides:          prometheus# Required-Start:    $remote_fs# Required-Stop:     $remote_fs# Should-Start:      $all# Should-Stop:       $all# Default-Start:     2 3 4 5# Default-Stop:      0 1 6# Short-Description: monitoring system and time series database.# Description:       Prometheus is a systems and service monitoring system.#                    It collects metrics from configured targets at given intervals,#                    evaluates rule expressions, displays the results,#                    and can trigger alerts if some condition is observed to be true.### END INIT INFOset -e. /lib/lsb/init-functionsNAME=prometheusDESC=\"Prometheus monitoring system\"DAEMON=/usr/local/bin/prometheusUSER=prometheusCONFIGDIR=/etc/prometheusDATADIR=/var/lib/prometheus/dataPID=\"/var/run/prometheus/$NAME.pid\"LOG=\"/var/log/prometheus/$NAME.log\"GOSU=/usr/sbin/gosuALERTMANAGER_OPTS=DAEMON_OPTS=\"$ALERTMANAGER_OPTS\"DAEMON_OPTS=\"$DAEMON_OPTS --config.file=$CONFIGDIR/prometheus.yml --storage.tsdb.path=$DATADIR\"DAEMON_OPTS=\"$DAEMON_OPTS --web.console.templates=$CONFIGDIR/consoles --web.console.libraries=$CONFIGDIR/console_libraries\"# Check if DAEMON binary exist[ -f $DAEMON ] || exit 0[ -f \"/etc/default/$NAME\" ] &amp;&amp; . /etc/default/$NAMEservice_not_configured () {  if [ \"$1\" != \"stop\" ]; then    printf \"\\tPlease configure $NAME and then edit /etc/default/$NAME\\n\"    printf \"\\tand set the \\\"START\\\" variable to \\\"yes\\\" in order to allow\\n\"    printf \"\\t$NAME to start.\\n\"  fi  exit 0}service_checks () {  # Check if START variable is set to \"yes\", if not we exit.  if [ \"$START\" != \"yes\" ]; then    service_not_configured $1  fi  # Prepare directories  mkdir -p \"/var/run/prometheus\" \"/var/log/prometheus\"  chown -R $USER \"/var/run/prometheus\" \"/var/log/prometheus\"  # Check if PID exists  if [ -f \"$PID\" ]; then    PID_NUMBER=`cat $PID`    if [ -z \"`ps axf | grep ${PID_NUMBER} | grep -v grep`\" ]; then      echo \"Service was aborted abnormally; clean the PID file and continue...\"      rm -f \"$PID\"    else      echo \"Service already started; skip...\"      exit 1    fi  fi}start () {  service_checks $1  $GOSU $USER   $DAEMON $DAEMON_OPTS  &gt; $LOG 2&gt;&amp;1  &amp;  RETVAL=$?  echo $! &gt; $PID  if [ $RETVAL ]; then    log_end_msg 0  else    log_end_msg 1  fi}stop () {  if start-stop-daemon --retry TERM/5/KILL/5 --oknodo --stop --quiet --pidfile $PID 2&gt;&amp;1 1&gt;$LOG  then    log_end_msg 0    rm $PID  else    log_end_msg 1  fi}case \"$1\" in  start)    log_daemon_msg \"Starting $DESC -\" \"$NAME\"    start    ;;  stop)    log_daemon_msg \"Stopping $DESC -\" \"$NAME\"    stop    ;;  reload)    log_daemon_msg \"Reloading $DESC configuration -\" \"$NAME\"    if start-stop-daemon --stop --signal HUP --quiet --oknodo --pidfile $PID --startas /bin/bash -- -c \"exec $DAEMON $DAEMON_OPTS &gt; $LOG 2&gt;&amp;1\"    then      log_end_msg 0    else      log_end_msg 1      fi    ;;  restart|force-reload)    log_daemon_msg \"Restarting $DESC -\" \"$NAME\"    stop    start    ;;  syntax)    $DAEMON --help    ;;  status)    status_of_proc -p $PID $DAEMON $NAME    ;;  *)    log_action_msg \"Usage: /etc/init.d/$NAME {start|stop|reload|restart|force-reload|syntax|status}\"    ;;esacexit 07. Start and Enable Prometheus ServiceTo start and restart the Prometheus service, use the following commands:sudo service prometheus startsudo service prometheus enablesudo service prometheus statusFor more details and advanced configurations, refer to the official Prometheus documentation."
  },
  
  {
    "title": "Ntfy Push Notifications",
    "url": "/posts/ntfy/",
    "categories": "Docker",
    "tags": "",
    "date": "2023-07-07 00:00:00 +0530",
    





    
    "snippet": "What is Ntfy Push Notifications?Ntfy is a powerful yet simple server for sending and receiving push notifications. It allows you to publish notifications from any system or service to your devices ...",
    "content": "What is Ntfy Push Notifications?Ntfy is a powerful yet simple server for sending and receiving push notifications. It allows you to publish notifications from any system or service to your devices using an easy-to-use HTTP API. Ntfy is particularly handy for setting up notifications for events like system failures, completed tasks, or any important updates that you want to monitor in real-time.Why Use Ntfy?Ntfy stands out due to its simplicity and flexibility. It’s an open-source project that you can self-host, meaning you retain full control over your data. Ntfy works seamlessly with services like IFTTT, Home Assistant, and others, making it an ideal choice for those who prefer to have notifications sent directly to their devices without relying on third-party services.Key Advantages:  Self-hosted: Complete control over your notifications and data.  Easy Integration: Works with various services and systems via a simple HTTP API.  Customizable: You can tailor notifications to fit your specific needs.Use Cases of Ntfy  System Monitoring: Receive real-time alerts when your servers encounter issues.  Task Automation: Get notified when long-running tasks are completed.  Home Automation: Integrate with smart home systems like Home Assistant to receive alerts for events such as doors opening or devices malfunctioning.  Personal Projects: Keep track of your projects or scripts with instant notifications when something important happens.Prerequisites  Docker Engine and Docker Compose packages are installed and running.  A non-root user with Docker privileges.  if you dont want to use the docker you can check the binaries releases page for binaries and deb/rpm packages.Setting Up the Ntfy ServerCreate the Directory StructureTo keep things organized, create a dedicated directory for Ntfy. Here’s the file structure:mkdir ~/docker/ntfy/{etc/ntfy,var/cache/ntfy}touch ~/docker/ntfy/docker-compose.yml📂 ntfy|--📑 docker-compose.yml|--📂 etc|   `--📂 ntfy|       `--📑 server.yml`--📂 var    `--📂 cache        `--📂 ntfyDocker ConfigurationWith the directory structure in place, let’s edit the docker-compose.yml file using your favorite text editor. Here’s an example configuration:sample docker.yaml:services:  ntfy:    image: binwiederhier/ntfy    container_name: ntfy    command:      - serve    environment:      - TZ=Asia/Kolkata    volumes:      - ./var/cache/ntfy:/var/cache/ntfy      - ./etc/ntfy:/etc/ntfy    ports:      - 80:80      #- 9090:9090 # to enable metrics    restart: unless-stopped    networks:      - proxy#    labels:#      - \"traefik.enable=true\"#      - \"traefik.http.routers.ntfy.entrypoints=http\"#      - \"traefik.http.routers.ntfy.rule=Host(`ntfyalertmaster.secsys.pro`)\"#      - \"traefik.http.middlewares.ntfy-https-redirect.redirectscheme.scheme=https\"#      - \"traefik.http.routers.ntfy.middlewares=ntfy-https-redirect\"#      - \"traefik.http.routers.ntfy-secure.entrypoints=https\"#      - \"traefik.http.routers.ntfy-secure.rule=Host(`ntfyalertmaster.secsys.pro`)\"#      - \"traefik.http.routers.ntfy-secure.tls=true\"#      - \"traefik.http.routers.ntfy-secure.service=ntfy\"#      - \"traefik.http.services.ntfy.loadbalancer.server.port=80\"#      - \"traefik.docker.network=proxy\"#      - \"traefik.http.routers.ntfy-secure.middlewares=authelia@file\"#      - \"com.centurylinklabs.watchtower.enable=true\"networks:  proxy:    external: true      sample server.yml:base-url: \"https://ntfy.sh\"attachment-cache-dir: \"/var/cache/ntfy/attachments\"attachment-total-size-limit: \"5G\"attachment-file-size-limit: \"15M\"attachment-expiry-duration: \"3h\"visitor-attachment-total-size-limit: \"100M\"visitor-attachment-daily-bandwidth-limit: \"500M\"# Monitoring metrics # enable-metrics: true  # metrics-listen-http: \"$IP:9090\"enabling Monitoring metricsyou can scrape the metrics by using Grafana by scrape data using PrometheusIn Prometheus, an example scrape config would look like this:scrape_configs:  - job_name: \"ntfy\"    static_configs:      - targets: [\"$IP:9090\"]Verify and Launch the ContainerTo verify the Docker Compose file has no issues, use the following command:docker compose configOnce you’ve confirmed that the docker-compose.yml file is correct, you can launch the container in the background:docker-compose up -dThis will create the volume, download the image, and start the container in the background.To verify the status of all running containers, run the following command:docker-compose ps# ORdocker ps -aIf something goes wrong, you can check the logs for each container using the docker logs command. For example, to check the log of the Ntfy container, run the following command:docker logs ntfy# ORdocker compose logs -f ntfyAccessing the Ntfy DashboardIf all is well, you can access your Ntfy Server locally by navigating to http://$IP:80 in your web browser.For more advanced configuration options, refer to the Ntfy manual."
  },
  
  {
    "title": "Duplicati",
    "url": "/posts/duplicati/",
    "categories": "Docker",
    "tags": "",
    "date": "2023-06-02 00:00:00 +0530",
    





    
    "snippet": "Duplicati is a powerful backup solution that supports standard protocols like FTP, SSH, WebDAV, and integrates with popular cloud services such as Microsoft OneDrive, Amazon Cloud Drive &amp; S3, G...",
    "content": "Duplicati is a powerful backup solution that supports standard protocols like FTP, SSH, WebDAV, and integrates with popular cloud services such as Microsoft OneDrive, Amazon Cloud Drive &amp; S3, Google Drive, Box.com, Mega, hubiC, and more.What is Duplicati?Duplicati is an open-source backup software designed for secure and efficient backups. It supports strong encryption, incremental backups, and is highly configurable, making it suitable for both personal and professional use.Core Features of Duplicati  Incremental Backups: Save time and storage space by only backing up changes.  Encryption: Protect your data with AES-256 encryption.  Scheduling: Automate your backup tasks.  Cross-Platform: Works on Windows, macOS, and Linux.  Cloud Integration: Seamless integration with major cloud storage providers.Why Use Duplicati?Duplicati is ideal for users who need a flexible and reliable backup solution that can be tailored to different environments. Whether you’re backing up data locally or to the cloud, Duplicati ensures your files are secure and easily recoverable.Prerequisites  Docker Engine and Docker Compose packages are installed and running  A non-root user with Docker privilegesSetting Up the Duplicati ServerCreate the Directory StructureTo keep things organized, create a dedicated directory for Duplicati. Here’s the file structure:mkdir ~/docker/duplicati/datatouch ~/docker/duplicati/docker-compose.yml📂 duplicati|--📂 data|   |--📂 config|   |   |-- CDATXERRIL.backup|   |   |-- Duplicati-server.sqlite|   |   |-- GSZRNYUBKA.sqlite|   |   |-- KYNVCKAOHG.sqlite|   |   `-- control_dir_v2|   |       `-- lock_v2|   `--📂 scripts|       `-- duplicati-after.sh`--📑 docker-compose.ymlDocker ConfigurationWith the directory structure in place, let’s edit the docker-compose.yml file using your favorite text editor. Here’s an example configuration:services:  duplicati-notifications:    image: jameslloyd/duplicati-notifications    container_name: duplicati-notifications    restart: unless-stopped    expose:      - 5000    networks:      routevlan:        ipv4_address: $IP      proxy:  duplicati:    image: lscr.io/linuxserver/duplicati:latest    container_name: duplicati    environment:      - TZ=Asia/Kolkata      # - CLI_ARGS= #optional      - PUID=1000      - PGID=1000          volumes:      - ./data/config:/config      - ./data/scripts:/tmp/scripts      - /root/docker:/docker      - /root/script:/script    ports:      - 8200:8200    restart: unless-stopped    networks:      routevlan:        ipv4_address: $IP       proxy:# Traefik labels can be uncommented and configured if using Traefik as a reverse proxy.#    labels:#      - \"traefik.enable=true\"#      - \"traefik.http.routers.duplicati.entrypoints=http\"#      - \"traefik.http.routers.duplicati.rule=Host(`test.domain.tld`)\"#      - \"traefik.http.middlewares.duplicati-https-redirect.redirectscheme.scheme=https\"#      - \"traefik.http.routers.duplicati.middlewares=duplicati-https-redirect\"#      - \"traefik.http.routers.duplicati-secure.entrypoints=https\"#      - \"traefik.http.routers.duplicati-secure.rule=Host(`test.domain.tld`)\"#      - \"traefik.http.routers.duplicati-secure.tls=true\"#      - \"traefik.http.routers.duplicati-secure.service=duplicati\"#      - \"traefik.http.services.duplicati.loadbalancer.server.port=8200\"#      - \"traefik.docker.network=proxy\"#      - \"traefik.http.routers.duplicati-secure.middlewares=authelia@file\"#      - \"com.centurylinklabs.watchtower.enable=true\"networks:  routevlan:    external: true  proxy:    external: trueVerify and Launch the Containerto verify the docker compose files has no issues use docker compose config Once you have the docker-compose.yml file you can run docker-compose up -d which will create the volume, download the image and start the container in the backgroundTo verify the status of all running containers, run the docker-compose command:docker-compose ps (OR) docker ps -aIf something goes wrong, you can check the logs for each container using the docker logs command and specify the service name. For example, to check the log of the Nginx Proxy Manager container, run the following command:docker logs $container (OR) docker compose logs -fAccessing the Duplicati DashboardIf all is well, you can locally view your duplicati Server by navigating to http://$IP:8200Creating a Backup ConfigurationIn this section, we’ll create a backup for the Docker directory, using SFTP to store the backups on another server.Configure Backup SettingsOpen the Duplicati web interface by navigating to http://$IP:8200. Click on Add backup to start the configuration process.First, give your backup a name that helps you identify it easily, such as “Docker Directory Backup.” If you want to encrypt your backup, select the encryption option and securely store the passphrase.Configure Backup DestinationFor the storage type, select SFTP since you are backing up to another server. Enter the IP address or hostname of the remote server and specify the directory where you want the backups to be stored (e.g., /home/$user/backup). Provide the SFTP username and password or SSH key for the remote server. This setup ensures that your backups are securely transferred to and stored on the remote server.  Make sure you use the full path on the server (e.g., /home/$user/backup).Next, navigate through the file browser in Duplicati to select the data you wish to back up. In this example, select the Docker directory. Ensure you use the full path to the directory (e.g., /home/$user/docker). You can include or exclude specific files and folders based on your needs.Schedule the BackupConfigure the backup schedule to automate the process according to your needs. You might schedule daily or weekly backups, depending on how often your data changes. Additionally, set retention policies to keep only a certain number of backups or delete backups older than a specified time.Enable NotificationsWhile Duplicati doesn’t have built-in support for sending notifications to Discord, you can configure the duplicati-notifications container to send notifications upon backup completion. In Duplicati settings, you can configure email notifications or use scripts to trigger custom notifications. Set up duplicati-notifications to integrate with your Discord server, ensuring that you receive a detailed notification about the backup status in your Discord channel.                               Run the BackupOnce everything is configured, run the backup to ensure it’s working as expected. Monitor the progress in the Duplicati dashboard and view logs for any issues. After completion, a notification will be sent to your Discord server                               Verify and RestorePeriodically, it’s a good practice to verify that your backups are functioning correctly and that you can restore data when needed. Duplicati provides a straightforward restore process, which can be initiated from the dashboard.For more advanced configuration options, refer to the Duplicati manual."
  },
  
  {
    "title": "Traefik",
    "url": "/posts/traefik/",
    "categories": "Linux",
    "tags": "",
    "date": "2023-05-10 00:00:00 +0530",
    





    
    "snippet": "IntroductionIn this post, we’ll walk through the steps to install and configure Traefik as a reverse proxy and load balancer for your Docker environment. Traefik is a powerful tool that dynamically...",
    "content": "IntroductionIn this post, we’ll walk through the steps to install and configure Traefik as a reverse proxy and load balancer for your Docker environment. Traefik is a powerful tool that dynamically discovers backend services and can automatically generate SSL certificates via ACME providers like Let’s Encrypt.This guide assumes you are familiar with docker and have it installed on your system if you didnt you can follow our detailed Docker installation guide.Step 1: Download and Extract TraefikFirst, download the appropriate version of Traefik you would like to use for your system from the official Traefik GitHub releases page.wget https://github.com/traefik/traefik/releases/download/vX.X.X/traefik_linux_amd64.tar.gztar -xzvf traefik_linux_amd64.tar.gzStep 2: Move Traefik to a System PathMake the Traefik binary executable and move it to a directory in your system’s PATH:chmod +x traefiksudo mv traefik /usr/local/bin/traefikTo allow Traefik to bind to privileged ports (like 80 and 443) as a non-root user, execute the following command:sudo setcap 'cap_net_bind_service=+ep' /usr/local/bin/traefikStep 3: Set Up User and DirectoriesNow, we’ll create the necessary user, group, and directories for Traefik. This ensures that Traefik runs with limited permissions, improving security.sudo groupadd -g 321 traefik dockersudo useradd -g traefik --no-user-group --home-dir /var/www --no-create-home --shell /usr/sbin/nologin --system --uid 321 traefikCreate the configuration directories:sudo mkdir -p /etc/traefik/{acme,dynamic} /var/log/traefik/sudo touch /var/log/traefik/{traefik.log,debug.log}sudo chown -R root:root /etc/traefiksudo chown -R traefik:traefik /etc/traefik/acmeSet appropriate ownership and permissions:sudo chown root:root /etc/traefik/traefik.yamlsudo chmod 644 /etc/traefik/traefik.yaml /etc/traefik/dynamic/config.yml /var/log/traefik/{traefik.log,debug.log}Step 4: Create and Configure Traefik FilesYou need to place the traefik.yaml configuration file in /etc/traefik/ and config.yml configuration file in /etc/traefik/dynamic/config.yaml.This file defines global settings, entry points, logging, and providers for Docker and file-based configurations.Update sample traefik.yaml according to your needs:################################################################# Global configuration#################################################################global:#  checkNewVersion = true#  sendAnonymousUsage = true################################################################# API and dashboard configuration################################################################api:  dashboard: true  debug: true################################################################# Traefik logs configuration################################################################log:  level: DEBUG  filePath: /var/log/traefik/debug.log  #format: jsonaccessLog:  filePath: \"/var/log/traefik/traefik.log\"  bufferingSize: 100  #format: json################################################################# EntryPoints configuration################################################################entryPoints:  http:    address: \":80\"    http:      redirections:        entryPoint:          to: https          scheme: https  https:    address: \":443\"serversTransport:  insecureSkipVerify: true################################################################# Docker configuration backend################################################################providers:  docker:    endpoint: \"unix:///var/run/docker.sock\"    exposedByDefault: false    useBindPortIP: true    network: proxy    watch: true  file:    directory: /etc/traefik/dynamic/    watch: true################################################################# Certificate resolvers################################################################certificatesResolvers:  cloudflare:    acme:      email: XXXXXXXXXX      storage: /etc/traefik/acme/acme.json      dnsChallenge:        provider: cloudflare        #disablePropagationCheck: true # uncomment this if you have issues pulling certificates through cloudflare, By setting this flag to true disables the need to wait for the propagation of the TXT record to all authoritative name servers.        resolvers:          - \"1.1.1.1:53\"          - \"1.0.0.1:53\"                                                                                                                                Update sample config.yaml according to your needs:http:  routers:    test:      entryPoints:        - \"https\"      rule: \"Host(`test.example.com`)\"      middlewares:        - secured      tls:        certResolver: cloudflare        domains:          - main: \"example.com\"            sans:              - \"*.example.com\"      service: test  traefik:    entryPoints:      - \"https\"    rule: \"Host(`traefik-detract.example.com`) &amp;&amp; (PathPrefix(`/api`) || PathPrefix(`/dashboard`))\"    middlewares:      - default-headers    tls:      certResolver: cloudflare      domains:        - main: \"example.com\"          sans:            - \"*.example.com\"    service: api@internal############################################################################################### services############################################################################################  services:    test:      loadBalancer:        sticky:          cookie:            secure: true            httpOnly: true        servers:          - url: \"https://$IP:$PORT/\"        passHostHeader: true############################################################################################### middlewares############################################################################################  middlewares:    https-redirectscheme:      redirectScheme:        scheme: https        permanent: true    gzip:      compress: {}    default-headers:      headers:        frameDeny: true        browserXssFilter: true        contentTypeNosniff: true        forceSTSHeader: true        stsIncludeSubdomains: true        stsPreload: true        stsSeconds: 15552000        customFrameOptionsValue: SAMEORIGIN        customRequestHeaders:          X-Forwarded-Proto: https        customResponseHeaders:          Access-Control-Allow-Origin: \"*\"    cors:      headers:        accessControlAllowOriginList: [\"*\"]        accessControlAllowCredentials: true        accessControlAllowHeaders: [\"*\"]        accessControlAllowMethods: [\"*\"]        accessControlMaxAge: 100        addVaryHeader: truetls:  options:    # To use with the label \"traefik.http.routers.myrouter.tls.options=modern@file\"    modern:      minVersion: \"VersionTLS13\"                          # Minimum TLS Version      sniStrict: true                                     # Strict SNI Checking    # To use with the label \"traefik.http.routers.myrouter.tls.options=intermediate@file\"    intermediate:      cipherSuites:        - \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\"        - \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\"        - \"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\"        - \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\"        - \"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\"        - \"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305\"      minVersion: \"VersionTLS12\"                          # Minimum TLS Version      sniStrict: true                                     # Strict SNI Checking    # To use with the label \"traefik.http.routers.myrouter.tls.options=old@file\"    old:      cipherSuites:        - \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\"        - \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\"        - \"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\"        - \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\"        - \"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\"        - \"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305\"        - \"TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256\"        - \"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\"        - \"TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA\"        - \"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\"        - \"TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA\"        - \"TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA\"        - \"TLS_RSA_WITH_AES_128_GCM_SHA256\"        - \"TLS_RSA_WITH_AES_256_GCM_SHA384\"        - \"TLS_RSA_WITH_AES_128_CBC_SHA256\"        - \"TLS_RSA_WITH_AES_128_CBC_SHA\"        - \"TLS_RSA_WITH_AES_256_CBC_SHA\"        - \"TLS_RSA_WITH_3DES_EDE_CBC_SHA\"      minVersion: \"TLSv1\"                                 # Minimum TLS Version      sniStrict: true                                     # Strict SNI CheckingStep 5: Create Systemd ServiceCreate a systemd service file for Traefik to manage its lifecycle.sudo nano /etc/systemd/system/traefik.servicesudo chown root:root /etc/systemd/system/traefik.servicesudo chmod 644 /etc/systemd/system/traefik.serviceUpdate sample traefik.service according to your needs:[Unit]Description=TraefikDocumentation=https://doc.traefik.io/traefik/After=network-online.targetWants=network-online.target systemd-networkd-wait-online.service[Service]Environment=\"CF_API_EMAIL=XXXXXXXXXX\"Environment=\"CF_DNS_API_TOKEN=XXXXXXXXXX\"Environment=\"CF_API_KEY=XXXXXXXXXX\"Restart=alwaysRestartSec=3; User and group the process will run as.User=traefikGroup=traefik; Always set \"-root\" to something safe in case it gets forgotten in the traefikfile.ExecStart=/usr/local/bin/traefik --configfile=/etc/traefik/traefik.toml --api=true --api.insecure=true --api.dashboard=true --api.debug=true; Limit the number of file descriptors; see `man systemd.exec` for more limit settings.LimitNOFILE=1048576; Use private /tmp and /var/tmp, which are discarded after traefik stops.PrivateTmp=true; Use a minimal /dev (May bring additional security if switched to 'true', but it may not work on Raspberry Pis or other devices, so it has been disabled in this dist.)PrivateDevices=false; Hide /home, /root, and /run/user. Nobody will steal your SSH-keys.ProtectHome=true; Make /usr, /boot, /etc and possibly some more folders read-only.ProtectSystem=full; … except /etc/ssl/traefik, because we want Letsencrypt-certificates there.;   This merely retains r/w access rights, it does not add any new. Must still be writable on the host!ReadWriteDirectories=/etc/traefik/acme; The following additional security directives only work with systemd v229 or later.; They further restrict privileges that can be gained by traefik. Uncomment if you like.; Note that you may have to add capabilities required by any plugins in use.CapabilityBoundingSet=CAP_NET_BIND_SERVICEAmbientCapabilities=CAP_NET_BIND_SERVICENoNewPrivileges=true[Install]WantedBy=multi-user.targetReload systemd and start the Traefik service:sudo systemctl daemon-reloadsudo systemctl start traefik.serviceTo enable Traefik to start automatically at boot:sudo systemctl enable traefik.serviceConclusionYou now have Traefik up and running, configured to work with Docker and your Cloudflare DNS for SSL certificates. For more advanced configurations, such as setting up middlewares or further customizing TLS options, refer to the official Traefik documentation."
  },
  
  {
    "title": "Portainer",
    "url": "/posts/portainer/",
    "categories": "Docker",
    "tags": "",
    "date": "2023-04-07 00:00:00 +0530",
    





    
    "snippet": "In the world of modern application development, containers have become the go-to solution for deploying, scaling, and managing software. However, managing these containers can be complex, especiall...",
    "content": "In the world of modern application development, containers have become the go-to solution for deploying, scaling, and managing software. However, managing these containers can be complex, especially as environments grow in size and complexity. This is where Portainer comes in—a powerful and user-friendly container management platform designed to streamline containerized application management across various environments, including cloud, datacenters, and Industrial IoT.What is Portainer? Portainer is a container management tool that offers a web-based interface for managing Docker environments and Kubernetes clusters. Its simplicity makes it accessible to both beginners and experienced users, providing all the features necessary to deploy, troubleshoot, and secure applications effortlessly.Core Features of PortainerPortainer’s primary goal is to simplify the management of containerized environments. It achieves this through a range of features tailored to different use cases.Container Management Deploying containers with Portainer is a breeze, whether you use pre-configured templates or custom configurations. You can manage Docker images, networks, volumes, and services, all from a central dashboard. For multi-container applications, Portainer allows easy management using Docker Compose or Kubernetes manifests, and it provides real-time visibility into the performance of your containers.Prerequisites  Docker Engine and Docker Compose packages are installed and running  A non-root user with Docker privilegesSetting up the portainer ServerCreate the directory structureTo keep the docker directory clean i have created a portainer directory to store the configuration file. My file structure is as follows:mkdir ~/docker/portainer/datatouch ~/docker/portainer/docker-compose.yml📂portainer|--📂 data|   |--📂 backups|   |   |--📂 common|   |   |   |-- portainer.db.2.19.1.20231204141757|   |   |   `-- portainer.db.2.19.4.20240423030116|   |   `-- portainer.db.bak|   |--📂 bin|   |--📂 certs|   |   |-- cert.pem|   |   `-- key.pem|   |--📂 chisel|   |   `-- private-key.pem|   |--📂 compose|   |--📂 docker_config|   |   `-- config.json|   |-- portainer.db|   |-- portainer.key|   |-- portainer.pub|   `--📂 tls`--📑 docker-compose.ymlDocker ConfigurationWith the directory structure in place, let’s edit the docker-compose.yml file using your favorite text editor. Here’s an example configuration:services:  portainer:    image: portainer/portainer-ce:latest    container_name: portainer    restart: unless-stopped    security_opt:      - no-new-privileges:true    networks:      - proxy    volumes:      - /etc/localtime:/etc/localtime:ro      - /var/run/docker.sock:/var/run/docker.sock:ro      - ./data:/data    ports:      - 9443:9443      - 9000:9000networks:  proxy:    external: trueVerify and launch the containerto verify the docker compose files has no issues use docker compose config Once you have the docker-compose.yml file you can run docker-compose up -d which will create the volume, download the image and start the container in the backgroundTo verify the status of all running containers, run the docker-compose command:docker-compose ps (OR) docker ps -aIf something goes wrong, you can check the logs for each container using the docker logs command and specify the service name. For example, to check the log of the Nginx Proxy Manager container, run the following command:docker logs $container (OR) docker compose logs -fAccessing the Portainer dashboardIf all is well, you can locally view your portainer Server by navigating to http://localhost:9443. You should see something that looks like the following. The first time you access Portainer, the system asks to create a password for the admin user. Type the password twice and select the Create user button. once you login you can able to access the portainerFor a more detailed guide, you can check the official documentation at Portainer Documentation.Launching the nginx containerAfter setting up Portainer, you can verify that everything is working as expected by deploying a simple Nginx container.To get started, access the Portainer dashboard by navigating to http://localhost:9443 in your web browser. Log in using the admin credentials you set up earlier.In the dashboard, go to the Containers section and click Add Container. Name the container nginx and set the image to nginx:latest, which will pull the latest Nginx image from Docker Hub. For port configuration, map port 80 in the container to port 8080 on your host by entering 8080:80.After configuring the settings, deploy the container. Once deployed, open a new browser tab and visit http://localhost:8080 or http://serverIp:8080. If the deployment is successful, the default Nginx welcome page will appear.This simple test confirms that Portainer is correctly managing your Docker containers. If issues arise, check the container logs within the Portainer interface by selecting the container and navigating to the Logs tab.After following the steps in this tutorial, you should now have a fully functional Portainer setup to manage Docker containers on your Linux system. The tutorial guided you through installing the Portainer Server, configuring it, and testing the setup by deploying an Nginx container.With Portainer, managing your Docker environments becomes more straightforward, allowing you to add and manage containers, monitor performance, and troubleshoot issues with ease. Whether you’re working with a single environment or managing multiple Docker instances, Portainer provides a unified platform to simplify your container management tasks."
  },
  
  {
    "title": "Adguard",
    "url": "/posts/aadguard/",
    "categories": "Docker",
    "tags": "",
    "date": "2023-03-29 00:00:00 +0530",
    





    
    "snippet": "AdGuard Home functions as a DNS sinkhole, blocking connection requests to domains or hosts identified as ad servers and trackers, while also providing built-in support for DNS over HTTPS (DoH) to e...",
    "content": "AdGuard Home functions as a DNS sinkhole, blocking connection requests to domains or hosts identified as ad servers and trackers, while also providing built-in support for DNS over HTTPS (DoH) to enhance privacy. Unlike Pi-Hole, which requires extra setup, AdGuard Home simplifies this process. With its DoH capabilities, it enables convenient use on mobile devices without necessitating a VPN, and it offers the flexibility to create whitelists in addition to blocklists.Prerequisites  Docker Engine and Docker Compose packages are installed and running  A non-root user with Docker privilegesSetting up the adguard ServerCreate the directory structureTo keep the docker directory clean i have created a adguard directory to store the configuration file. My file structure is as follows:mkdir ~/docker/adguard/{conf,work}touch ~/docker/adguard/docker-compose.yml📂adguard|-- 📂conf|   `-- 📑 AdGuardHome.yaml|-- 📂work|   `--📂data        |-- filters        |   `-- 1.txt        |-- querylog.json        |-- sessions.db        `-- stats.db|-- 📑docker-compose.yml  Docker ConfigurationAs we have created the docker compose file lets edit the files with your favorite editors of choice---version: \"3\"services:  adguardhome:    image: adguard/adguardhome    container_name: adguardhome    environment:      - PUID=1000      - PGID=1000    volumes:      - ./conf:/opt/adguardhome/conf      - ./work:/opt/adguardhome/work    ports:        # DNS        - 53:53          # # DHCP server          # - 67:67/udp          # - 68:68/tcp          # - 68:68/udp          # # HTTPS/DNS-over-HTTPS          # - 443:443/tcp          # # DNS-over-TLS          # - 853:853/tcp          # # DNS-over-QUIC          # - 784:784/udp          # # DNSCrypt          # - 5443:5443/tcp          # - 5443:5443/udp          # # WebUI        - 3000:3000/tcp    restart: unless-stopped    networks:      routevlan:        ipv4_address: 192.168.1.100      proxy:    networks:  routevlan:    external: true  proxy:    external: true#routevlan:#  name: routevlan#  driver: macvlan#  driver_opts:#    parent: eth1 # using ifconfig#  ipam:#    config:#      - subnet: \"192.168.1.0/24\"#        ip_range: \"192.168.1.100/32\"#        gateway: \"192.168.1.1\"As the Adguard service is relays on the network to block the adds on the network i recommend you to use the docker macvlan. AdGuard Home Docker service would act as a separate machine thereby eliminating any port conflicts.Verify and launch the containerto verify the docker compose files has no issues use docker compose config Once you have the docker-compose.yml file you can run docker-compose up -d which will create the volume, download the image and start the container in the backgroundTo verify the status of all running containers, run the docker-compose command:docker-compose ps (OR) docker ps -aIf something goes wrong, you can check the logs for each container using the docker logs command and specify the service name. For example, to check the log of the Nginx Proxy Manager container, run the following command:docker logs $container (OR) docker compose logs -fAccessing the Adguard dashboardIf all is well, you can locally view your adguard Server by navigating to http://localhost:PORT. Or from another machine by using your ip (macvlan ip) address.You should see something that looks like the following.Setup DNS service on routerTo use AdGuard Home, replace the default DNS server IPs with your AdGuard Home IP address (macvlan ip) in your router or other devices. If you haven’t altered DNS settings before, typically there won’t be any IP address configured in your settings. DNS configuration depends on the routers you use please google them according.If you like you can use the adguard default DHCP service for assigning the IP address in your network instead of the default router DHCP."
  },
  
  {
    "title": "Bitwarden",
    "url": "/posts/bitwarden/",
    "categories": "Docker",
    "tags": "",
    "date": "2023-02-18 00:00:00 +0530",
    





    
    "snippet": "Bitwarden is a free and open-source password management service that stores sensitive information such as website credentials in an encrypted vault. The Bitwarden platform offers a variety of clien...",
    "content": "Bitwarden is a free and open-source password management service that stores sensitive information such as website credentials in an encrypted vault. The Bitwarden platform offers a variety of client applications including a web interface, desktop applications, browser extensions, mobile apps, and a CLI.I use Bitwarden as my main password vault. It stores my card details for automating the filling out of payment forms. Saves me from having to find or remember my card details. I also use Bitwarden for storing all of my passwords.Having Bitwarden as a public endpoint means that I can connect to my password vault using the Bitwarden app on Android, specifying my self hosted instance.Prerequisites  Docker Engine and Docker Compose packages are installed and running  A non-root user with Docker privilegesSetting up the Bitwarden ServerCreate the directory structureTo keep the docker directory clean i have created a bitwarden directory to store the configuration file. My file structure is as follows:mkdir ~/docker/bitwarden/datatouch ~/docker/bitwarden/docker-compose.yml📂bitwarden|-- 📂data|-- 📑docker-compose.yml  Docker ConfigurationAs we have created the docker compose file lets edit the files with your favorite editors of choiceversion: '3.8'services:  bitwarden:    image: vaultwarden/server:latest    container_name: bitwarden    volumes:      - ./data:/data    restart: unless-stopped    environment:      - PUID=1000      - PGID=1000      - WEBSOCKET_ENABLED=true      - SIGNUPS_ALLOWED=true      - DOMAIN=https://subdomain.yourdomain.com      - LOGIN_RATELIMIT_MAX_BURST=10       - LOGIN_RATELIMIT_SECONDS=60      - ADMIN_RATELIMIT_MAX_BURST=10      - ADMIN_RATELIMIT_SECONDS=60      - ADMIN_TOKEN=YourReallyStrongAdminTokenHere      - SENDS_ALLOWED=true      - EMERGENCY_ACCESS_ALLOWED=true      - WEB_VAULT_ENABLED=true      - SIGNUPS_ALLOWED=false      - SIGNUPS_VERIFY=true      - SIGNUPS_VERIFY_RESEND_TIME=3600      - SIGNUPS_VERIFY_RESEND_LIMIT=5      - SIGNUPS_DOMAINS_WHITELIST=yourdomainhere.com,anotherdomain.com      - SMTP_HOST=smtp.youremaildomain.com      - SMTP_FROM=vaultwarden@youremaildomain.com      - SMTP_FROM_NAME=Vaultwarden      - SMTP_SECURITY=SECURITYMETHOD      - SMTP_PORT=XXXX      - SMTP_USERNAME=vaultwarden@youremaildomain.com      - SMTP_PASSWORD=YourReallyStrongPasswordHere      - SMTP_AUTH_MECHANISM=\"Mechanism\"    ports:      - \"8080:80\"#    networks:#      - proxy##networks:#  proxy:#    external: trueEnvironment variablesSystem Settings  Domain: This is the domain you wish to associate with your Vaultwarden instance.  LOGIN_RATELIMIT_MAX_BURST: This is the maximum number of requests allowed in a burst of login / two-factor attempts while maintaining the average specified in LOGIN_RATELIMIT_SECONDS.  LOGIN_RATELIMIT_SECONDS: This is the average number of seconds between login requests from the same IP before Vaultwarden rate limits logins.  ADMIN_RATELIMIT_MAX_BURST: This is the same as LOGIN_RATELIMIT_MAX_BURST, only for the admin panel.  ADMIN_RATELIMIT_SECONDS: This is the same as LOGIN_RATELIMIT_SECONDS, only for the admin panel.  ADMIN_TOKEN: This value is the token (a type of password) for the Vaultwarden admin panel. For security, this should be a long random string of characters. The admin panel is disabled if this value is not set.  SENDS_ALLOWED: This setting determines whether users are allowed to create Bitwarden Sends – a form of credential sharing.  EMERGENCY_ACCESS_ALLOWED: This setting controls whether users can enable emergency access to their accounts. This is useful, for example, so a spouse can access a password vault in the event of death so they can gain access to account credentials. Possible values: true / false.  WEB_VAULT_ENABLED: This setting determines whether or not the web vault is accessible. Stopping your container then switching this value to false and restarting Vaultwarden could be useful once you’ve configured your accounts and clients to prevent unauthorized access. Possible values: true/false.Signup Settings  SIGNUPS_ALLOWED: This setting controls whether or not new users can register for accounts without an invitation. Possible values: true / false.  SIGNUPS_VERIFY: This setting determines whether or not new accounts must verify their email address before being able to login to Vaultwarden. Possible values: true / false.  SIGNUPS_VERIFY_RESEND_TIME: If SIGNUPS_VERIFY is set to true, this value specifies how many seconds a user must wait before another verification email can be sent.  SIGNUPS_VERIFY_RESEND_LIMIT: If SIGNUPS_VERIFY is set to true, this value specifies the maximum number of times an email verification may be re-sent.  SIGNUPS_DOMAINS_WHITELIST: This setting is a comma separated list of domains that can register for Vaultwarden accounts, even if SIGNUPS_ALLOWED is set to false. This is useful for when your Vaultwarden accounts are to be used specifically by email addresses whose domains you control.SMTP Settings  SMTP_HOST: This is your SMTP mailserver.  SMTP_FROM: This is the email address messages will be sent from.  SMTP_FROM_NAME: The name you wish to appear as the email account name on sent messages  SMTP_SECURITY: The security method used by your SMTP server. Possible values: “starttls” / “force_tls” / “off”.  SMTP_PORT: This is the SMTP port used by your mail server. Possible values: 587 / 465.  SMTP_USERNAME: This is the login for your SMTP mail server.  SMTP_PASSWORD: This is the password for your SMTP credentials.  SMTP_AUTH_MECHANISM: This is the SMTP authentication mechanism of your mail server. Possible values: “Plain” / “Login” / “Xoauth2”.I’m using Vaultwarden which is an opensource project. It is not owned by Bitwarden. They’re an unofficial bitwarden compatible server written in Rust.Save your docker-compose.yml file and exit back to your bitwarden directory.Verify and launch the containerto verify the docker compose files has no issues use docker compose config Once you have the docker-compose.yml file you can run docker-compose up -d which will create the volume, download the image and start the container in the backgroundTo verify the status of all running containers, run the docker-compose command:docker-compose ps (OR) docker ps -aIf something goes wrong, you can check the logs for each container using the docker logs command and specify the service name. For example, to check the log of the Nginx Proxy Manager container, run the following command:docker logs $container (OR) docker compose logs -fAccessing the bitwarden dashboardIf all is well, you can locally view your Bitwarden Server by navigating to http://localhost:PORT. Or from another machine by using your ip address instead of localhostYou should see something that looks like the following.Finally, you’ll just need to register for an account on your new hosted instance.Click the Create Account buttonThen fill out your details. If you have an existing Bitwarden account, you’ll still have to create a new account on this instance. You can then Export and Import your credentials.                               Browsers like chrome, firefox … also has the inbuilt vaults to store the credentials you can also export the credentials in a csv/json format and exportYou can use the Nginx proxy manager to create the reverse proxy and configure the lets-encrypt SSL certificate to remove the SSL warningsyou can use the bitwarden clients applications/browser extensions to access the stored credentials Application firefox extension chrome extension  For More details please check the github issues discussions"
  },
  
  {
    "title": "Homer",
    "url": "/posts/homer/",
    "categories": "Docker",
    "tags": "",
    "date": "2023-02-05 00:00:00 +0530",
    





    
    "snippet": "Recently I have decided to get my home network in order, One of the things I realized was that I spend a lot of time trying to remember the IP addresses or URLs for services within my home, especia...",
    "content": "Recently I have decided to get my home network in order, One of the things I realized was that I spend a lot of time trying to remember the IP addresses or URLs for services within my home, especially ones that I access infrequently.After searching for some open source projects i came across the homer. A simple to use Docker container that stores all my service url which can be accessible easily. Homer is a full static html/js dashboard, based on a simple yaml configuration filePrerequisites  Docker Engine and Docker Compose packages are installed and running  A non-root user with Docker privilegesSetting up the homerCreate the directory structureTo keep the docker directory clean i have created a homer directory to store the configuration file and any other assets such as images. My file structure is as follows:mkdir ~/docker/homer/assetstouch ~/docker/homer/assets/config.yml ~/docker/homer/docker-compose.yml📂homer|-- 📂assets|   |-- 📑config.yml|-- 📑docker-compose.yml  Docker ConfigurationAs we have created the docker compose file lets edit the files with your favorite editors of choice---version: \"3\"services:  homer:    image: b4bz/homer    container_name: homer    volumes:      - ./assets/:/www/assets    ports:      - 8080:8080    #networks:    #  - proxy    restart: unless-stopped    user: 1000:1000 # default    environment:      - INIT_ASSETS=1 # default#networks:#  proxy:#    external: trueAs you notice am using the port 8080 if the port 8080 is already used feel free to use the free port. if you want to create a separate network for the homer (eg:- proxy) don’t forget to create the docker network manually docker network create proxyVerify and launch the containerto verify the docker compose files has no issues use docker compose config Once you have the docker-compose.yml file you can run docker-compose up -d which will create the volume, download the image and start the container in the backgroundTo verify the status of all running containers, run the docker-compose command:docker-compose ps (OR) docker ps -aIf something goes wrong, you can check the logs for each container using the docker logs command and specify the service name. For example, to check the log of the Nginx Proxy Manager container, run the following command:docker logs $container (OR) docker compose logs -fAccessing the Homer dashboardNow that the container is up and running you can access it via:http://&lt;docker-host-ip-address&gt;:&lt;port&gt;If everything has worked as expected you should see the following demo dashboard:You can use the Nginx proxy manager to create the reverse proxy and configure the lets-encrypt SSL certificate to remove the SSL warningsSample config.yml file# Homelab Dashboard Main Page Configuration# See https://fontawesome.com/v5/search for icons optionstitle: \"Dashboard\"subtitle: \"Homelab\"#logo: \"logo.png\"icon: \"fas fa-house-laptop\" # Optional iconheader: truefooter: '&lt;p&gt; Home lab Dashboard &lt;/p&gt;'columns: \"4\" # Options \"auto\" or must be a factor of 12: 1,2,3,4,6,12connectivityCheck: true#proxy:#  useCredentials: truedefaults:  layout: list  colorTheme: darkstylesheet:- \"assets/custom.css\"# Optional theme customizationtheme: default # 'default' or one of the themes available in 'src/assets/themes'colors:  light:    highlight-primary: \"#fff5f2\"    highlight-secondary: \"#fff5f2\"    highlight-hover: \"#bebebe\"    background: \"#12152B\"    card-background: \"rgba(255, 245, 242, 0.8)\"    text: \"#ffffff\"    text-header: \"#fafafa\"    text-title: \"#000000\"    text-subtitle: \"#111111\"    card-shadow: rgba(0, 0, 0, 0.5)    link: \"#3273dc\"    link-hover: \"#2e4053\"    background-image: \"../assets/wallpaper-light.jpeg\" # Change wallpaper.jpeg to the name of your own custom wallpaper!  dark:    highlight-primary: \"#181C3A\"    highlight-secondary: \"#181C3A\"    highlight-hover: \"#1F2347\"    background: \"#12152B\"    card-background: \"rgba(24, 28, 58, 0.8)\"    text: \"#eaeaea\"    text-header: \"#7C71DD\"    text-title: \"#fafafa\"    text-subtitle: \"#8B8D9C\"    card-shadow: rgba(0, 0, 0, 0.5)    link: \"#c1c1c1\"    link-hover: \"#fafafa\"    background-image: \"../assets/wallpaper.jpeg\"# Optional navbarlinks:  - name: \"Wordpress\"    icon: \"fab fa-wordpress\"    url: \"https://www.myblog.io/\"    target: \"_blank\"      - name: \"Linkedin\"    icon: \"fab fa-linkedin\"    url: \"https://www.linkedin.com/feed/\"    target: \"_blank\"  - name: \"Github\"    icon: \"fab fa-github\"    url: \"https://github.com/\"    target: \"_blank\" # optional html a tag target attribute  - name: \"Reddit\"    icon: \"fab fa-reddit\"    url: \"https://www.reddit.com/user/\"    target: \"_blank\"  - name: \"Twitter\"    icon: \"fab fa-twitter\"    url: \"https://twitter.com/home\"    target: \"_blank\"  - name: \"Facebook\"    icon: \"fab fa-facebook\"    url: \"https://www.facebook.com\"    target: \"_blank\"  - name: \"Work Dashboard (local)\"    icon: \"fas fa-tools\"    url: \"https://wordpress.com\"    target: \"_blank\"# Servicesservices:    # Section 1  - name: \"Hardware\"    icon: \"fas fa-server\"    items:      - name: \"Proxmox\"        logo: \"https://raw.githubusercontent.com/walkxcode/dashboard-icons/master/png/proxmox.png\"        subtitle: \"Virtual Environment\"        tag: \"server\"        tagstyle: \"is-link\" # options is-primary,is-link,is-info,is-success,is-warning,is-danger,is-small,is-medium,is-large,is-outlined,is-loading,[disabled]        url: \"https://proxmox.com\"        target: \"_blank\"      - name: \"servers\"        subtitle: \"Power edge Dell R720\"        logo: \"https://www.freepnglogos.com/uploads/server-png/home-server-icon-icons-and-png-backgrounds-30.png\"        tag: \"Server\"        tagstyle: \"is-link\"        url: \"https://www.dell.com/\"        target: \"_blank\"  # Section 2  - name: \"Container Management ARM\"    icon: \"fab fa-docker\"    items:      - name: \"Portainer\"        logo: \"https://raw.githubusercontent.com/walkxcode/dashboard-icons/master/png/portainer.png\"        url: \"https://www.portainer.io/\"        type: \"Portainer\"        target: \"_blank\"  # Section 3  - name: \"Applications\"    icon: \"fas fa-gears\"    items:            - name: \"Nginx Proxy Manager\"        subtitle: \"Reverse Proxy Service\"        logo: \"https://raw.githubusercontent.com/walkxcode/dashboard-icons/master/png/nginx-proxy-manager.png\"        tag: \"proxy\"        tagstyle: \"is-link\"        url: \"https://nginxproxymanager.com/\"        target: \"_blank\"      - name: \"Bitwarden\"        subtitle: \"Password Management\"        logo: \"https://raw.githubusercontent.com/walkxcode/dashboard-icons/master/png/bitwarden.png\"        tag: \"app\"        tagstyle: \"is-link\"        url: \"https://bitwarden.com/\"        target: \"_blank\"Custom ThemesYou can add custom CSS to homer in order to have a personal look similar to the one I have used from Walkxcode called homer-theme  For More details please check the manual"
  },
  
  {
    "title": "Npm",
    "url": "/posts/npm/",
    "categories": "Docker",
    "tags": "",
    "date": "2023-01-19 00:00:00 +0530",
    





    
    "snippet": "The Nginx Proxy Manager (NPM) is an open-source reverse proxy management system that runs as a Docker container. It is easy to set up and requires no expertise to work with Nginx servers or SSL cer...",
    "content": "The Nginx Proxy Manager (NPM) is an open-source reverse proxy management system that runs as a Docker container. It is easy to set up and requires no expertise to work with Nginx servers or SSL certificates. All you need to do is install a Docker and Docker Compose on each server.Prerequisites  Docker Engine and Docker Compose packages are installed and running  A non-root user with Docker privileges  A valid domain name for Nginx Proxy Manager (example.your-domain.tld in this tutorial).Setting up the Nginx proxy managerCreate the directory structuremkdir -p ~/docker/npm/{data,letsencrypt,data/snippet}touch ~/docker/npm/data/snippet/_hsts.confNext, let’s create a docker-compose.yml file to define different services to deploy Nginx Proxy Manager you can use the yaml files depending upon your requirements.nano docker-compose.ymlversion: \"3\"services:  app:    image: 'jc21/nginx-proxy-manager:latest'    restart: unless-stopped    ports:      - '80:80'      - '443:443'      - '81:81'    environment:      DB_MYSQL_HOST: \"db\"      DB_MYSQL_PORT: 3306      DB_MYSQL_USER: \"$secure-user\"      DB_MYSQL_PASSWORD: \"$secure-password\"      DB_MYSQL_NAME: \"$database\"    volumes:      - ./data:/data      - ./letsencrypt:/etc/letsencrypt      - ./data/snippet/_hsts.conf:/app/templates/_hsts.conf## Optional#    depends_on:#      - db##  db:#    image: 'jc21/mariadb-aria:latest'#    restart: unless-stopped#    environment:#      MYSQL_ROOT_PASSWORD: '$secure-password'#      MYSQL_DATABASE: '$database'#      MYSQL_USER: '$secure-user'#      MYSQL_PASSWORD: '$secure-password'#    volumes:#      - ./data/mysql:/var/lib/mysqlStarting the containerDocker Compose allows us to start all the services we specified in the docker-compose.yml file with just one command: docker-compose up.Let’s start all the containers and make them run in the background using the -d flag:docker-compose up -dTo verify the status of all running containers, run the docker-compose ps command:docker-compose ps (OR) docker ps -aIf something goes wrong, you can check the logs for each container using the docker logs command and specify the service name. For example, to check the log of the Nginx Proxy Manager container, run the following command:docker logs $container (OR) docker compose logs -fConfiguring the headersFeel free to update the headers according to your specific needs and include them in the nginx proxy vhost while configuringnano ./data/snippet/_hsts.conf{% if certificate and certificate_id &gt; 0 -%}{% if ssl_forced == 1 or ssl_forced == true %}{% if hsts_enabled == 1 or hsts_enabled == true %}  # HSTS (ngx_http_headers_module is required) (63072000 seconds = 2 years)  add_header Strict-Transport-Security \"max-age=63072000;{% if hsts_subdomains == 1 or hsts_subdomains == true -%} includeSubDomains;{% endif %} preload\" always;  add_header Referrer-Policy strict-origin-when-cross-origin;  add_header X-Content-Type-Options nosniff;  add_header X-XSS-Protection \"1; mode=block\";  add_header X-Frame-Options SAMEORIGIN;  add_header \"Access-Control-Allow-Origin *;\";  add_header Content-Security-Policy upgrade-insecure-requests;  add_header Permissions-Policy interest-cohort=();  add_header Expect-CT 'enforce; max-age=604800';  more_set_headers 'Server: Proxy';  more_clear_headers 'X-Powered-By';{% endif %}{% endif %}{% endif %}  For More details please check the manual"
  },
  
  {
    "title": "Docker",
    "url": "/posts/docker/",
    "categories": "Docker",
    "tags": "",
    "date": "2023-01-15 00:00:00 +0530",
    





    
    "snippet": "  Docker installation various depends upon the distribution you use check docker manualInstalling Docker Engine on Debian 11To install Docker on Debian 11, follow these steps:Update a Package ListM...",
    "content": "  Docker installation various depends upon the distribution you use check docker manualInstalling Docker Engine on Debian 11To install Docker on Debian 11, follow these steps:Update a Package ListMake sure your package list is up to date by opening a terminal and running the following command.sudo apt updateInstall Required PackagesInstall the required packages to enable apt to use HTTPS repositories and support other package types.sudo apt install apt-transport-https ca-certificates curl software-properties-commonAdd Docker RepositoryYou can add Docker’s official GPG key and repository to your system by following these steps.curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpgecho \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/nullUpdate Package List AgainPlease run the update command again to ensure that the Docker repository is included.sudo apt updateInstall Docker EngineInstall Docker Engine and its dependencies.  If you want to keep all the libraries in home directory create the symlink as follows mkdir /home/.docker/ &amp;&amp; ln -s /home/.docker/ /var/lib/dockersudo apt install docker-ce docker-ce-cli containerd.io  If you would like to use the docker compose plugin please add docker-compose-pluginStart and Enable DockerStart the Docker service and enable it to start on boot. First run the command:sudo systemctl start dockerTo automatically launch alongside the operating system, include it in the startup configuration with this command.sudo systemctl enable dockerVerify InstallationTo ensure that Docker is up and running, try running a basic container with a “Hello World” command.sudo docker run hello-worldCongratulations! You have successfully installed Docker on Debian 11. Get ready to efficiently manage and run containers for your applications.Enable TCP port 2375 for external connection to DockerUpdate the systemd configurationLocate the docker daemon service file in my case /lib/systemd/system/docker.service❯ systemctl status docker● docker.service - Docker Application Container Engine     Loaded: loaded (/lib/systemd/system/docker.service; enabled; preset: enabled)    Drop-In: /usr/lib/systemd/system/docker.service.d             └─dietpi-simple.conf     Active: active (running) since Wed 2024-03-27 23:10:33 IST; 1 week 2 days agoTriggeredBy: ● docker.socket       Docs: https://docs.docker.com   Main PID: 1451697 (dockerd)      Tasks: 67     Memory: 48.3M        CPU: 1h 13min 47.364s     CGroup: /system.slice/docker.servicelocate the ExecStart and update the tcp lisen port as followsExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock -H=fd:// -H=tcp://0.0.0.0:2375Reload the systemd daemon:systemctl daemon-reloadRestart docker: systemctl restart docker.serviceVerify TCP portYou can verify if the docker is lisening on the port 2375❯ ss -lntp | grep 2375LISTEN 0      4096               *:2375             *:*    users:((\"dockerd\",pid=1451697,fd=3))Manage Docker as a non-root user  The docker group grants root-level privileges to the user. For details on how this impacts security in your system, see Docker Daemon Attack Surface.Create the docker group and add your user:sudo groupadd dockerAdd your user to the docker group.sudo usermod -aG docker $USERLog out and log back in so that your group membership is re-evaluated.  If you’re running Linux in a virtual machine, it may be necessary to restart the virtual machine for changes to take effect.You can also run the following command to activate the changes to groups:newgrp dockerVerify that you can run docker commands without sudo.docker run hello-world"
  },
  
  {
    "title": "Forge",
    "url": "/posts/forge/",
    "categories": "CTF, HTB",
    "tags": "ftp, ssh",
    "date": "2023-01-12 00:00:00 +0530",
    





    
    "snippet": "IntroductionToday we will learn about the server-side request forgery attack. While enumerating, we discovered the FTP credentials through which we gain access to the server via ssh and the root us...",
    "content": "IntroductionToday we will learn about the server-side request forgery attack. While enumerating, we discovered the FTP credentials through which we gain access to the server via ssh and the root using the Python library. With that stated, let’s get started.As is customary, we will begin by scanning the server for open ports using the nmap.We only have two open ports: SSH (OpenSSH) and Apache (for the webpage). Let’s examine the web server.We have a standard website with an image upload option. We can upload images in two ways: one from a local computer and the other from a URL.To receive the request from the server to our local system, I had setup the Python server from scratch. So we can be sure the server is sending us the request.EnumerationWhile the web server is operating, let’s use the sec list wordlist to run the gobuster for directory listing.However, we just have the uploads directory, which redirects /uploads to /upload, and I didn’t find much in the directory, so I began the DNS enumeration using the wordlist present in seclist itself.I discovered another subdomain admin, however it was only accessible via localhost.However, we can send the request from the server to our localhost and then redirect the web server to the admin page.Let me describe the situation. We use the upload function to send the request to our localhost, from which we redirect the web server to the restricted admin panel. Let us try…We received the request from the remote server, which was fulfilled by my scratch python web server, and we received success and an address to view our files.Let’s open this URL in a browser, however I didn’t get a response from the server when I tried to curl the same address, but I can see the content of the admin section.Ftp accessI located the FTP credentials.I am unable to access the FTP port since it has been filtered.Considering that we have URL redirection, let us redirect the web server to the FTP server to review the content.As we can access the files, let us copy the user’s private ssh keys in order to login via SSH, as the ssh port was open.We have the ssh user’s private key, which we store to a file and use to login to the server through ssh.Escalating PrivilegesLet’s look at the user’s permissions. We can execute /opt/remote-manage.py. Because this script expected numbers as input, let’s cause an error by pressing the random key, which takes us to the Python debugger ( PDB ). We can acquire root access via the PDB."
  },
  
  {
    "title": "All in one",
    "url": "/posts/allinone/",
    "categories": "CTF, THM",
    "tags": "cyberchef, find, socat, sudi binary, wpscan",
    "date": "2022-05-18 00:00:00 +0530",
    





    
    "snippet": "This is a fun box where we can find many ways where we can exploit let’s see how we can get the root into it let’s beginReconnaissanceThis phase involves gathering information about the target syst...",
    "content": "This is a fun box where we can find many ways where we can exploit let’s see how we can get the root into it let’s beginReconnaissanceThis phase involves gathering information about the target system to understand its vulnerabilities and potential entry points. Tools like Nmap are used to scan for open ports and services running on those ports. Gobuster is utilized to search for web directories, while Nikto helps in identifying common vulnerabilities in web servers. This step helps the attacker understand the target’s attack surface and plan their approach accordingly.As we always do let’s start scanning the box for the open ports with Nmap gobuster for web directories and Nikto for common vulnerabilityBy looking at the results we there are three ports open FTP SSH and APACHE web server. We can also see that anonymous ftp logins are allowed let’s hope into FTP to find what’s present in itIt seems to be the FTP is currently empty let’s check for the APACHE web server we ALL IN ONE page of WordPress it was also identified by the gobusterLets check the front end of the WordPress to figure out which themes/modules it usedENUMERATINGLet’s start WordPress scanner and ENUMERATE users to find the usernames for login into the WordPress to gain the accessWe got the username and need to escalate the password for login this we can do by using a vulnerability plugin installed in the WordPress which contains The File Inclusion vulnerability ( LFI ) allows an attacker to include a filehttp://ip/wordpress/wp-content/plugins/mail-masta/inc/campaign/count_of_send.php?pl=php://filter/convert.base64-encode/resource=../../../../../wp-config.phpWe are using PHP filters for converting the code into the base64 format and by decrypting it we can able to see the code clearly in plain textWe can use the cyber chef for decrypting it is one of my favorite fools for encryption, encoding, compression, and data analysis.As It was the easy box we have the found credentials for the database user and the same credentials has been used for the WordPress loginWordPressLet’s hope in into the WordPress by using the credentials and get a quick reverse shell from itAm using custom plugin for getting the reverse shell.Navigate into the home directory we have a hint here saying that the password is saved in the box we need to find themFindAm using find followed by the user and type as a file, am going to scan the root directory (/) by sending an error message to nullRoot AccessWe got the credentials let’s escalate the user to root by knowing what permissions have been given to the user. We can use socat It can be used to break out from restricted environments by spawning an interactive system shell. Let’s break the restricted shell and BOOM we are ROOT"
  },
  
  {
    "title": "Chill Hack",
    "url": "/posts/chillhack/",
    "categories": "CTF, THM",
    "tags": "Pwncat, zip, john",
    "date": "2021-12-15 00:00:00 +0530",
    





    
    "snippet": "ReconnaissanceLet’s do  a quick reconnaissance to find the information about the box by doing a nmap scan to find the open portsAs we have ftp port open by seeing at the namp results we have anonym...",
    "content": "ReconnaissanceLet’s do  a quick reconnaissance to find the information about the box by doing a nmap scan to find the open portsAs we have ftp port open by seeing at the namp results we have anonymous login allowed lets login as anonymousWe have a successful login and we have a note.txt hear download it with get it and cat it out as we have usernames hear and nothing much. Let’s have a look at the port 80we have a static web page. run the gobuster and nikto to find the directories present in it                               By looking at the gobuster and nikto results /secrets seems to be interesting open the /secrets page we have commands execution hear run the ls to check the files present in it                               we have an error when we executed the ls command we have to bypass the filter to get the command execution. send the request to the brup and so that we can easily modify the requestfilter’s bypassLets fire up the burp suite and catch the request so that we can play around itwe have a 200 response that means we have successfully bypassed the filters lets cat the index file to see the filters present in it so that we can get the reverse shellthe filter seems to block the tools like python bash php perl rm cat head tail python3 more less sh and ls commands that’s why when we executed the ls command we get an error. Since we cannot execute the shell scripts we can by pass this filters by creating an exploit in our local meacine start the simple http server and curl the exploit and send it to the bash to get the reverse shell. bash is also get blocked due to filter we can bypass it by send bash and the it wont get stopped by filerGetting an shelllets create the exploit to get the reverse shell connection                               stat the listener service when the code get executed it should give us a no response seems to be interesting lets look at the listener service we have a reverse shell as www-datalets check what permission we have as a www-data user and we can run the helpline script and we logged in as another  user we can conform it by seeing at the id outputSSH Keyslets create a ssh key and upload it in the user authorized_keys so that we can ssh into the boxWe have a successful login to ssh and by doing some basic enumeration we came to know that another port is listen 9001. forward the port locally and locate it at https://localhost:9001bypassing loginWe don’t have the credentials to login. lets save the request to a file and run the sqlmap to find the bypass payload to login we have bypassed it with this payload and we have a login                               Steganographywe have only some text on the page look in the dark! you will find your answer.  and we have a image lets wget it to download it and use steghide to see any information is hidden in itJohnwe have find that some information is hidden in it we need to have the credentials to extract it from the jpg file am using joh tool to extract the information from it. first we have to use zip to john tool against the backup.zip file ehich was extracted from the jgp file and save the output hash to a file and run the john tool against the hash to get the passpharse to extract the information from the backup.zipPrivilege escalationrun the id command by looking at it we came to know that we are in a DOCKER container and If the binary is allowed to run as superuser by sudo, it does not drop the elevated privileges and may be used to access the file system, escalate or maintain privileged access.That’s it for today hope you like the box subscribe for more updated content fell free to drop a comment  and HAVE A GREAT DAY !"
  },
  
  {
    "title": "Brute Force",
    "url": "/posts/bruteforce/",
    "categories": "CTF, THM",
    "tags": "hydra, john",
    "date": "2021-11-17 00:00:00 +0530",
    





    
    "snippet": "Reconnaissancelet’s do the quick scanning to find the open ports on the boxnmap -sc -sV -oN file_name ipnmap -SC for default cripts aand -sV for enumerate version -oN for simple nmap format and the...",
    "content": "Reconnaissancelet’s do the quick scanning to find the open ports on the boxnmap -sc -sV -oN file_name ipnmap -SC for default cripts aand -sV for enumerate version -oN for simple nmap format and the ip address you want to scan there are two ports open one is being SSH and the other is web server HTTP, ssh is running on version openssh 7.6 and the apache 2.4.29. As the web server is open scan for the directories present in it with the gobustergobuster dir -u url -w wordlist -o outputgobuster dir for specifying the gobuster in directory mode and -u for the URL you want to scan and the -o for output the scanning resultswe have got the results /admin login  page lets check the pageHydrahydra – Very fast network logon crackerHydra is a parallelized login cracker which supports numerous protocols to attack. It is very fast and flexible, and new modules are easy to add. This tool makes it possible for researchers and security consultants to show how easy it would be to gain unauthorized access to a system remotely.Hydra is a tool to guess/crack valid login/password pairs – usage only allowed for legal purposes.we are going to brute force the admin panel to gain the access into the boxwe found the admin credentials by brute forcing the admin panel with the rockyou wordlist, login with the credentials into the admin paneljohnfirst  run the ssh2john python script for the rsa private key and save the output format to a file. then run the run the john on the output file to start the john you can use custom wordlist if you want or the you can use the custom wordlist which came along with the john the ripplewe got the password of the rsa private key and we have the ssh port open lets login into the SSH. we got logged in into the box and we have successfully logged inEnumerationif we check the permission of the user we can run the cat as sudo, If the binary is allowed to run as superuser by sudo, it does not drop the elevated privileges and may be used to access the file system, escalate or maintain privileged access. sudo cat /etc/shadowwe got the hash of the root user, let’s crack it with the john. first save the hash to a file and sun the john tool in this case am not using any wordlist just providing the ash to the john to crack itwe got the root credentialsthat’s all for today hope you like the box, have a GREAT DAY."
  },
  
  {
    "title": "Root Me",
    "url": "/posts/rootme/",
    "categories": "CTF, THM",
    "tags": "Lfi, pwncat",
    "date": "2021-11-10 00:00:00 +0530",
    





    
    "snippet": "This is a easy box we start with file upload vulnerabilityReconnaissanceLet’s do  a quick reconnaissance to find the information about the box by doing a nmap scan to find the open portswe have two...",
    "content": "This is a easy box we start with file upload vulnerabilityReconnaissanceLet’s do  a quick reconnaissance to find the information about the box by doing a nmap scan to find the open portswe have two ports open one is SSH running on openssh 7.6  version and the other is the web server running on the Apache httpd 2.4. As the web server is open lets run the gobuster and the nikto to find the directories and the vulnerability present in the web server                               by looking at both the results gobuster seems to be interesting it had found the /upload and /panel directory.Enumerationlet’s upload the reverse shell and start the listener server on the local host to gain access into the server as the www-data userEnumerationby running some enumeration on the setuid binaries we may able to know that we can gain the root access by using the python"
  },
  
  {
    "title": "Jeff",
    "url": "/posts/jeff/",
    "categories": "CTF, THM",
    "tags": "fcrackzip, wpscan",
    "date": "2021-10-30 00:00:00 +0530",
    





    
    "snippet": "scanningLet’s scan the host to find the open ports with nmap, nikto and gobuster                               Looking at the results we have two ports open one is being SSH and HTTP. the gobuster ...",
    "content": "scanningLet’s scan the host to find the open ports with nmap, nikto and gobuster                               Looking at the results we have two ports open one is being SSH and HTTP. the gobuster and nikto both find the /admin pannel to login and the gobuster had find the /backup which seems to be interesting, let’s start another gobuster to scan the backup directory to find content present in the backup with the extensions as zip, txt, gzipit looks like we have backup.zip file in the backup directory. lets download it with the wgetwgetWget is a free utility for non-interactive download of files from the Web. It supports HTTP, HTTPS, and FTP protocols, as well asretrieval through HTTP proxies.unzip it but we failed. it was protected by a password. we don’t have any password to unzip it let’s try to crack it with the fcrackzipfcrackzipWe frequently use zipped files due to its small size and encryption algorithm. These zipped files come with a facility of password protection which maintains the security of the files. When u have lost the password, and the problem arises of how to crack it, fcrackzip comes to the rescue to save and provide you with the way out in order to protect your documents. Simple way to crack a protected zip file with the help of fcrackzip which is available under Linux. fcrackzip is a free/fast zip password cracker                               we got the password lets unzip it and we have the password to login but we don’t have any valid user name to loginwp scanLets scan the WordPress site with the WordPress scanners to find the informationwith the wpscan we can able to find the username. let’s login with that credentials to WordPressexploitlet’s create an WordPress plugin to give us a reverse shell, upload the plugin and install it                               we got the shell as the www-data we have nothing to do with that low level user privileges.but a little bit of enumeration we found that there is a ftp_backup.phpwe have ftp backup user credentials. but we cannot switch the user to backup but we can upload a reverse shell and run it to gain access to the backup user                               We can use netcat to start the listening service but am using the pwncat to listen and i got the shell as a backup user. and run the find command to run the files owned by the user find ~/ -type d -exec ls.1 -d {} \\; 2&gt;/dev/nullit looks like we have systools to run let’s run it and restore the password of the user for valid credentialssshIn the nmap results we had seen that there is an SSH port is open, lets ssh into the box with this credentialswe got into the box but we are in a restricted shell but we need to break out from restricted environments by spawning an interactive system shell to get the commands executedThat’s all for today guys hope you like the box. HAVE A GREAT DAY"
  },
  
  {
    "title": "Shell Shock",
    "url": "/posts/shellshock/",
    "categories": "CTF, THM",
    "tags": "john, metasploit",
    "date": "2021-10-15 00:00:00 +0530",
    





    
    "snippet": "what is shell shock vulnerabilityShellshock is a security bug causing Bash to execute commands from environment variables unintentionally. In other words if exploited the vulnerability allows the a...",
    "content": "what is shell shock vulnerabilityShellshock is a security bug causing Bash to execute commands from environment variables unintentionally. In other words if exploited the vulnerability allows the attacker to remotely issue commands on the server, also known as remote code execution. Even though Bash is not an internet-facing service, many internet and network services such as web servers use environment variables to communicate with the server’s operating system.Since the environment variables are not sanitized properly by Bash before being executed, the attacker can send commands to the server through HTTP requests and get them executed by the web server operating system.ScanningLet’s scan with the nmap to find the open services on the boxnmap -sC default scripts -sV enumerate version -oN output out format ipWe have two services open one is being SSH and the other is HTTP. As the web server is open let’s fire up the gobuster to find the directories and the nikto to find vulnerabilities and mis-configurations on the server.gobuster dir -u url -w wordlist -x extensions -o output nikto -h url                               looking at the gobuster results it seems to be /backup is interesting. Let’s check what is present in the backup directoryJohn The RipperJohn the Ripper is a fast password cracker Its primary purpose is to detect weak Unix passwords we can crack the private key with the john for this we need to run the script to change the private key for john compatibilityssh2john.py ssh/privatekey &gt; output.txtRun the john against the private key and we have a password of an private key. let’s login with the private key but we couldn’t login because we didn’t have the user passwordLet’s see what we can do with it, looking at the nikto results /cgi-bin/test.cgi seems to be vulnerability with the shell shock vulnerability. It’s time to start the metasploit frameworkMetasploit FrameworkWe are using the metasploit framework use the exploitmsf &gt; use exploit/multi/http/apache_mod_cgi_bash_env_execmsf &gt; exploit(apache_mod_cgi_bash_env_exec) &gt; set TARGET &lt; target-id &gt;msf &gt; exploit(apache_mod_cgi_bash_env_exec) &gt; show options    ...show and set options...msf exploit(apache_mod_cgi_bash_env_exec) &gt; run0day_8Ones we run it we have a meterpreter shell opened as a low level userPrivilege EscalationLets upload the linpeas to escalate the privilegesby looking at the results we can say that the Linux version is too old we can exploit it using the CVE-2015-1328CVE-2015-1328Description The overlayfs implementation in the Linux (aka Linux kernel) package before 3.19.0-21.21 in Ubuntu through 15.04 does not properly check permissions for file creation in the upper filesystem directory, which allows local users to obtain root access by leveraging a configuration in which overlayfs is permitted in an arbitrary mount namespace.upload the exploit and run itWe are root now ! HOPE YOU LIKE THE BOX HAVE A GOOD DAY"
  },
  
  {
    "title": "Wordpress",
    "url": "/posts/wordpress/",
    "categories": "CTF, THM",
    "tags": "streghide, wpscan, metasploit, find",
    "date": "2021-09-16 00:00:00 +0530",
    





    
    "snippet": "scanningLet’s scan the host to find the open portsWe have three services open one is being an SSH HTTP &amp; SMB  let’s check the web servernothing much interesting hear. we have a billy as the use...",
    "content": "scanningLet’s scan the host to find the open portsWe have three services open one is being an SSH HTTP &amp; SMB  let’s check the web servernothing much interesting hear. we have a billy as the user let’s check the samba share with this user nameAs the share contains only three two jpg files and one mp4. Download it locally and check whether the jpg files contains any data inside it with streghidestreghideSteghide is a steganography program that is able to hide data in various kinds of image- and audio-files. The color- respectivly sample-frequencies are not changed thus making the embedding resistant against first-order statistical tests.Steganography literally means covered writing. Its goal is to hide the fact that communication is taking place. This is often achieved by using a (rather large) cover file and embedding the (rather short) secret message into this file. The result is a innocuous looking file (the stego file) that contains the secret message.wpscanWPScan is an open source WordPress security scanner. You can use it to scan your WordPress website for known vulnerabilities within the WordPress core, as well as popular WordPress plugins and themes.Since it is a WordPress black box scanner, it mimics a real attacker. This means it does not rely on any sort of access to your WordPress dashboard or source code to conduct the tests. In other words, if WPScan can find a vulnerability in your WordPress website, so can an attacker.WPScan uses the vulnerability database called wpvulndb.com to check the target for known vulnerabilities. The team which develops WPScan maintains this database. It has an ever-growing list of WordPress core, plugins and themes vulnerabilities.With the help of the wpscan we can got the users names and we dont have a valid password to login let’s bruteforce to find the valid passwordBrute ForceWe have a valid username and password login with that creds into the wordpressWordPress Crop-image Shell UploadAs the above WordPress version is 5.0 we have WordPress Crop-image Shell Upload vulnerability present in this version. This exploits a path traversal and a local file inclusion vulnerability on WordPress versions 5.0.0 and &lt;= 4.9.8.The crop-image function allows a user, with at least author privileges, to resize an image and perform a path traversal by changing the _wp_attached_file reference during the upload.The second part of the exploit will include this image in the current theme by changing the _wp_page_template attribute when creating a post. This exploit module only works for Unix-based systems currently.metasploitLet’s fire up the metasploit framework and run the crop image exploit by setting the WordPress login credentials. if you don’t have metasploit framework you can install it by using this single command in your terminalcurl https://raw.githubusercontent.com/rapid7/metasploit-omnibus/master/config/templates/metasploit-framework-wrappers/msfupdate.erb &gt; msfinstall &amp;&amp; chmod 755 msfinstall &amp;&amp; ./msfinstallone’s you set all the requirements run it and we have a meterpreter shell opened as a low level userEnumerationwith the help of find command lets check the enumerate of all binaries having SUID permission. find / -perm -u=s -type f 2&gt;/dev/nullThe /usr/sbin/checker file seems to be interesting and all the  other files are quite common. let’s trace the checker with ltrace and we export admin = 1 and we have a nice root shell hearThat’s all for to day hope you like the box. HAVE A GREAT DAY"
  },
  
  {
    "title": "Peak",
    "url": "/posts/peakhill/",
    "categories": "CTF, THM",
    "tags": "cyberchef, pyc",
    "date": "2021-09-07 00:00:00 +0530",
    





    
    "snippet": "As usual we begin with the scanning the box with the nmap, rust to find the open ports and we found the anonymous ftp login through winch we get the credentials in a binary format by using the cybe...",
    "content": "As usual we begin with the scanning the box with the nmap, rust to find the open ports and we found the anonymous ftp login through winch we get the credentials in a binary format by using the cyber chef get the login credentials though which we connect via ssh with all being said let jump into the boxscanningLet’s scan the box to find the open port’s                               As we have two service’s open one is being SSH  and the other is FTP. As the nmap scan results ANONYMOUS login’s are allowed let’s check the ftpAs we have .creds and the test.txt files get this files and exit from ftp. As we have a binary code in .creds let’s decode it with the cyberchefAfter decoding it you will find a SSH user name and password login with that user &amp; passwordWe have a successful login and we are into the box with low level user and we have nothing to do from hear except we have a cmd_service.pyc file. Let’s download the file with the Secure copy protocol (scp) followed by the user name IP and location of file to copyscp user@ip:location/of/file.pyc.pyc files are created by the Python interpreter when a .py file is imported. They contain the “compiled bytecode” of the imported module/program so that the “translation” from source code to bytecode (which only needs to be done once) can be skipped on subsequent imports if the .pyc is newer than the corresponding .py file, thus speeding startup a little. But it’s still interpreted.Once the *.pyc file is generated, there is no need of *.py file, unless you edit it. But we can’t able to read the bytecode we have convert the bytecode into the .py script. For this we need a tool called uncompile6 install it and run it you will get the python codeuncompyle6 cmd_service.pyc &gt; code.pyWe can also find that the code is starting the server on local at port 7321 and credentials to login. Let’s connect it with the NETCAT. Copy the private key of the userSave it to a file and change the permission of the id_rsa and SSH with that with the private keyif we check the permissions of the user we may only run the peak_hil_farm as a ROOT user with out the passwordlet’s create a pickle exploit to give us the setuid binarySet User ID is a type of permission that allows users to execute a file with the permissions of a specified user. Those files which have suid permissions run with higher privileges.  Assume we are accessing the target system as a non-root user and we found suid bit enabled binaries, then those file/program/command can run with root privileges.Hope you like the box, HAVE A GREAT DAY"
  },
  
  {
    "title": "Peak",
    "url": "/posts/jboss/",
    "categories": "CTF, THM",
    "tags": "gtfo bin, jexboss",
    "date": "2021-08-08 00:00:00 +0530",
    





    
    "snippet": "This is a fun and easy box with a lot of open ports to explore in many ways. will start with the nmap as usual check the site explore the vulnerability get the reverse shell with netcat and escalat...",
    "content": "This is a fun and easy box with a lot of open ports to explore in many ways. will start with the nmap as usual check the site explore the vulnerability get the reverse shell with netcat and escalate the privileges with the pingsys suid binary with all being said let jump into the box.let scan the box to find the open ports with the nmapBy looking at the nmap results we have a bunch of open ports. Open ssh with version 7.4 Apache listening on two ports 80. the jboss on port 8080.Lets explore the jobssAs this application is build with jboss we have a tool called Jexboss which is used for testing and exploiting vulnerabilities in JBoss Application Server and others Java PlatformsUpon running the jexboss scan we have found the vulnerability and and got the shell as Jacob userAs we have the shell let get the reverse shell connection via netcat to escalate the privilegeswe are in the box as the Jacob user and run the following command, you can list all binaries with SUID permission.the pingsys command to perform a presence check (ping) on the targeted systems. When the presence check is completed, the results are reflected in the OperatingState and CommunicationState attributes.Through the pingsys command we can get the ROOT user here,  it will ping the localhost (127.0.0.1). then it executes the /bin/sh through winch we got the root shell"
  },
  
  {
    "title": "Hacker",
    "url": "/posts/hacker/",
    "categories": "CTF, THM",
    "tags": "crackmap, suid",
    "date": "2021-07-23 00:00:00 +0530",
    





    
    "snippet": "As usual we will start with the nmap scan and we notice ftp anonymous login allows and we found two files there will get these files and use the crack map exec to find the credentials once we are i...",
    "content": "As usual we will start with the nmap scan and we notice ftp anonymous login allows and we found two files there will get these files and use the crack map exec to find the credentials once we are into the box by using the tar suid we will escalate the root privileges with all being said let beginLets scan the box with the nmap nmap -sC -sV -oN ipWe some open port’s FTP ( 21 ), SSH ( 22 ), HTTP ( 80 ) we can see that the anonymous ftp login’s are allowed let’s login as anonymousWe got the user name from tasks.txt let’s see whats on port HTTP 80 nothing find’s interesting. let’s start the gobuster to discover the directories gobuster dir -u (url) -w (wordlist) -o (outputfile)As we have a valid user name let’s try to validate credentials with crack mapcrackmap exec ssh ip -u user -p passlistAs we have a valid credentials let’s ssh into the boxwe have a successful login. let’s check what permissions we have sudo -l the root user may run the /bin/tarlets escalate to root user by using the tar sudo tar -cf /dev/null /dev/null --checkpoint=1 --checkpoint-action=exec=/bin/shThat’s all for today hope you like the box! HAVE A GREAT  DAY"
  },
  
  {
    "title": "Active Directory",
    "url": "/posts/activedirectry/",
    "categories": "CTF, THM",
    "tags": "impacket, kerbrute, psexec",
    "date": "2021-07-09 00:00:00 +0530",
    





    
    "snippet": "Most of the Corporate networks run off of AD. We are going to exploit a DOMAIN CONTROLLER. With the kerbrute and impacket tools to enumerate the box .  Let’s scan for the open ports with the nmap  ...",
    "content": "Most of the Corporate networks run off of AD. We are going to exploit a DOMAIN CONTROLLER. With the kerbrute and impacket tools to enumerate the box .  Let’s scan for the open ports with the nmap                               ImpacketInstalling Impacket:First, you need to clone the repo with: git clone https://github.com/SecureAuthCorp/impacket.git /opt/impacket This will clone Impacket to /opt/impacket/ once the repo is cloned, you will notice several install related files, requirements.txt, and setup.py. Setup.py is commonly skipped during the installation. It’s key that you DO NOT miss it. So let’s install the requirements: pip3 install -r /opt/impacket/requirements.txtOnce all the python modules are installed, we can then run the python setup install script cd /opt/impacket/ &amp;&amp; python3 ./setup.py installAfter that, Impacket should be correctly installed and then download the kerbrute https://github.com/ropnop/kerbrutewe have got the users account and the svc-admin user names of the spookysec.local. Let’s run a tool from IMPACKET to collect the hash of a svc-admin accountwe got the hash save it to a file and crack the hash with the hashcat                               We have a valid user name and the password let’s check the shares with the SMBCLIENTAs the backup account password is stored in base64 format  let’s decrypt it by echo \" hash \" | base64 -dAs we have the backup account le’s try to dump the HASHES by dumping the NTFS.DIT by using the impacket toolWe got the Administrator hash. We are going to PASS THE HASH attack to authenticate with out the password as a AdministratorThat’s all for to day hope you like the box. HAVE A GOOD DAY"
  },
  
  {
    "title": "tartarus",
    "url": "/posts/tartarus/",
    "categories": "CTF, THM",
    "tags": "Rust scan",
    "date": "2021-06-25 00:00:00 +0530",
    





    
    "snippet": "We are going to solve an CTF challenge called the tartarus. This is a beginner box based on simple enumeration of services and basic privilege escalation techniques. We have 3 port open FTP , SSH &...",
    "content": "We are going to solve an CTF challenge called the tartarus. This is a beginner box based on simple enumeration of services and basic privilege escalation techniques. We have 3 port open FTP , SSH &amp; HTTP. We do brute force with hydra to login as www-data and upload a reverse shell to connect into the boxWe will start with rust scan to find the open ports                               As the port 80 is open let’s fire the nikto and save the output to a file nikto.lognikto had find the robots.txt lets open it and we have a secret admin directory. and we found users id and credentials let’s download it with wgetAs the FTP SERVER allows anonymous login let’s login as anonymouswe found the super secret directory. let’s run the gobuster to find more directorieswe found the login page at /super-secret and we have users id and credentials let’s run the HYDRA  to get the valid user and passwordhydra IP -L userid -P credentials.txt http-post-form \"/sUp3r-s3cr3t/authenticate.php/:username=^USER^&amp;password=^PASS^&amp;Login=Login:F=Incorrect*\"                               we are having a upload page. Let’s upload a php-reverse-shell to get the shell. We found it in /super secret /images/uploadslet’s open the port and wait for the connection. Am using pwncat tool for listening you can find the pwncat in GitHubwe are into the box as the www-data user let’s check if we have the sudo permission to the user sudo -lour user may run the gdb as thirtytwo user with out the password.sudo -u thirtytwo /var/www/gdb -nx -ex '!sh' -ex quitwe are the thirtytwo user now and let’s check what sudo permissions we have darckh user who may run the git with out root passwordsudo -u d4rckh /usr/bin/git help configA little bit of enumeration we had find that we can write the cleanup.py script we edit the script to give us the setuid binary to the darckh user so that we can have root permissionchmod +s /bin/bashThat’s all for to day hope you like the box see you i next tutorial until then good bye have  a great day"
  },
  
  {
    "title": "overpass",
    "url": "/posts/overpass/",
    "categories": "CTF, THM",
    "tags": "linpeas",
    "date": "2021-06-09 00:00:00 +0530",
    





    
    "snippet": "we are going to solve an online challenge called the overpass. As we find the two open ports one is being ssh on port 22 and the other HTTP on port 80. we find the private ssh key and we will crack...",
    "content": "we are going to solve an online challenge called the overpass. As we find the two open ports one is being ssh on port 22 and the other HTTP on port 80. we find the private ssh key and we will crack the password with john tool through which we will ssh into the box and we will do some basic enumeration in the box to get the root user with all being said lets get startedFirst we will start with the nmap to find the open portsnmap -sC -sV -oN nmap/initial_scan $IPLet’s take a look at the port 80. we have Overpass page saying that a secure password manager with support for windows, Linux, macos and moreLet’s fire up the gobuster to find the directories and the pages present in it and save the output yo gobuster.loggobuster dir -u http://10.10.44.66/ -w /usr/share/wordlist/dirbuster/directorylist-2.3-medium.txt -o gobuster.log Lets open the /admin page we find a simple login page nothing interesting there. let’s open the page source we will find the login.js java script there open the java script file as we send the request sessiontoken = something.  we will get to /adminLet send the request through the curl by adding the cookie sessiontoken=anythingcurl \"http://10.10.44.66/admin/\" --cookie \"SessionToken=anything\"Here we got an private ssh key. copy the private ssh key to a file and change the permission of the private key and lets try to ssh it with the keyAs the private key is asking for password lets try to crack it with the john too. For this john has a ssh2john,py script first we have to execute a script on private key and save it to a filessh2john.py id_rsa &gt; forjohn.txtNow run the john tool we will get the password as james13As we have the private ssh key and the password lets ssh into itwe are now in as a low level user let’s do some enumeration to get the high level user. Let’s start the HTTP server and upload the linpeas.sh and run itpython3 -m http.server &amp;&amp; chmod +x linpeas.sh &amp;&amp; ./linpeas.shA little bit of enumeration our user can run curl as root only on overpass.thm domainLet’s modify the domain address to our ip address so that we can get the root usermake the request is going through the downloads/src/buildscript.sh let’s make the directories for it and make the bildscript.sh to run in bash and start the http servermkdir -p downloads/src &amp;&amp; cd $_ &amp;&amp; vi buildscript.sh &amp;&amp;sudo python3 -m http.server                        These script will  gives us the accessible root privileges through bash binary setuidThat’s all for to day hope you like the box see you i next tutorial until then good bye have  a great day"
  }
  
]

